---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.16
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-11**|**TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering**|基于点的辐射场渲染在新颖的视图合成中表现出了令人印象深刻的结果，提供了令人信服的渲染质量和计算效率的结合。然而，这一领域的最新方法也并非没有缺点。3D高斯飞溅[Kerbl和Kopanas等人2023]在渲染高度详细的场景时，由于模糊和模糊的伪影，会遇到困难。另一方面ADOP[R\“uckert et al.2022]可以容纳更清晰的图像，但神经重建网络会降低性能，它会处理时间不稳定性，并且无法有效地解决点云中的大间隙。在本文中，我们提出了TRIPS（Trilinear point Splatting），一种结合了Gaussian Splatting和ADOP思想的方法。我们的新技术背后的基本概念包括将点光栅化为屏幕空间图像金字塔，金字塔层的选择由投影点的大小决定。这种方法允许使用单个三线性写入来渲染任意大的点。然后使用轻量级神经网络来重建包括超出飞溅分辨率的细节的无孔图像。重要的是，我们的渲染管道是完全可微分的，允许点大小和位置的自动优化。我们的评估表明，TRIPS在渲染质量方面超过了现有的最先进的方法，同时在现成的硬件上保持了每秒60帧的实时帧速率。这种性能扩展到具有挑战性的场景，例如具有复杂几何形状、广阔景观和自动曝光镜头的场景。 et.al.|[2401.06003](http://arxiv.org/abs/2401.06003)|null|
|**2024-01-10**|**Diffusion Priors for Dynamic View Synthesis from Monocular Videos**|动态新颖视图合成旨在捕捉视频中视觉内容的时间演变。现有的方法很难区分运动和结构，特别是在相机姿态与对象运动相比未知或受约束的情况下。此外，仅使用来自参考图像的信息，对在给定视频中被遮挡或部分观察到的看不见的区域产生幻觉是极具挑战性的。为了解决这些问题，我们首先使用定制技术在视频帧上微调预训练的RGB-D扩散模型。随后，我们将微调模型中的知识提取为包括动态和静态神经辐射场（NeRF）分量的4D表示。所提出的流水线在保持场景同一性的同时实现了几何一致性。我们进行了深入的实验，以定性和定量地评估所提出方法的有效性。我们的结果证明了我们的方法在具有挑战性的情况下的稳健性和实用性，进一步推进了动态新视图合成。 et.al.|[2401.05583](http://arxiv.org/abs/2401.05583)|null|
|**2024-01-09**|**Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation**|生成扩散模型的最新进展已经实现了从单个输入图像或文本提示生成3D资产的先前不可行的能力。在这项工作中，我们的目标是提高这些模型的质量和功能，以完成创建可控、照片真实感的人类化身的任务。我们通过将3D可变形模型集成到最先进的多视角一致扩散方法中来实现这一点。我们证明了生成管道在关节式3D模型上的精确调节增强了基线模型在从单个图像合成新视图任务中的性能。更重要的是，这种集成有助于将面部表情和身体姿势控制无缝准确地结合到生成过程中。据我们所知，我们提出的框架是第一个扩散模型，能够从看不见的物体的单个图像中创建完全3D一致、可动画化和照片真实感的人类化身；大量的定量和定性评估证明了我们的方法在新视角和新表情合成任务上优于现有的最先进的化身创建模型。 et.al.|[2401.04728](http://arxiv.org/abs/2401.04728)|null|
|**2024-01-07**|**See360: Novel Panoramic View Interpolation**|我们介绍了See360，它是一种使用潜在空间视点估计进行360全景插值的通用且高效的框架。大多数现有的视图渲染方法只关注室内或合成三维环境，并渲染小对象的新视图。相反，我们建议将以相机为中心的视图合成作为2D仿射变换来处理，而不使用点云或深度图，这使得能够实现有效的360？全景场景探索。给定一对参考图像，See360模型通过提出的新颖的多尺度仿射变换器（MSAT）来学习渲染新颖的视图，从而实现从粗到细的特征渲染。我们还提出了一种条件潜在空间自动编码器（C-LAE）来实现任意角度的视图插值。为了展示我们方法的多功能性，我们引入了四个训练数据集，即UrbanCity360、Archinterior360、HungHom360和Lab360，它们是从室内和室外环境中收集的，用于真实和合成渲染。实验结果表明，该方法具有足够的通用性，可以实现四个数据集任意视图的实时绘制。此外，我们的See360模型可以应用于野外的视图合成：只需很短的额外训练时间（约10分钟），并且能够渲染未知的真实世界场景。See360的卓越性能为以相机为中心的视图渲染和360全景视图插值开辟了一个很有前途的方向。 et.al.|[2401.03431](http://arxiv.org/abs/2401.03431)|**[link](https://github.com/Holmes-Alan/See360)**|
|**2024-01-06**|**RustNeRF: Robust Neural Radiance Field with Low-Quality Images**|最近在神经辐射场（NeRF）方面的工作利用了多视图三维一致性，在三维场景建模和高保真新颖视图合成方面取得了令人印象深刻的结果。然而，也有局限性。首先，现有方法假设有足够的高质量图像可用于训练NeRF模型，忽略了真实世界的图像退化。其次，由于不同视图之间未建模的不一致性，以前的方法在训练集中难以解决模糊性问题。在这项工作中，我们为真实世界的高质量NeRF提供了RustNeRF。为了提高NeRF在真实世界输入下的鲁棒性，我们训练了一个包含真实世界退化建模的3D感知预处理网络。我们提出了一种新的隐式多视图引导来解决图像退化和恢复过程中的信息丢失问题。大量实验证明了RustNeRF在实际退化情况下优于现有方法。代码将被发布。 et.al.|[2401.03257](http://arxiv.org/abs/2401.03257)|null|
|**2024-01-02**|**Street Gaussians for Modeling Dynamic Urban Scenes**|本文旨在解决从单目视频中建模动态城市街道场景的问题。最近的方法扩展了NeRF，将跟踪车辆姿态纳入车辆动画，实现了动态城市街道场景的照片逼真视图合成。然而，它们的显著局限性在于训练和渲染速度慢，再加上履带车辆姿态对高精度的迫切需求。我们介绍了Street Gaussians，一种新的明确的场景表示，它解决了所有这些限制。具体地说，动态城市街道被表示为一组点云，这些点云配备有语义logits和3D Gaussians，每一个都与前景车辆或背景相关联。为了对前景对象车辆的动力学进行建模，使用可优化的跟踪姿态以及动态外观的动态球面谐波模型对每个对象点云进行优化。显式表示允许容易地合成目标车辆和背景，这反过来又允许在半小时的训练内以133 FPS（1066 $\times$ 1600分辨率）进行场景编辑操作和渲染。所提出的方法在多个具有挑战性的基准上进行了评估，包括KITTI和Waymo Open数据集。实验表明，在所有数据集上，所提出的方法始终优于最先进的方法。此外，尽管仅依赖于现成跟踪器的姿态，但所提出的表示提供的性能与使用精确的地面实况姿态所实现的性能不相上下。代码位于https://zju3dv.github.io/street_gaussians/. et.al.|[2401.01339](http://arxiv.org/abs/2401.01339)|null|
|**2024-01-01**|**Deblurring 3D Gaussian Splatting**|最近对辐射场的研究为具有照片级真实感渲染质量的新颖视图合成铺平了坚实的道路。然而，它们通常使用神经网络和体积绘制，这两种方法的训练成本很高，并且由于绘制时间长，阻碍了它们在各种实时应用中的广泛使用。最近，人们提出了一种基于3D高斯散射的方法来对3D场景进行建模，并在实时渲染图像的同时实现了显著的视觉质量。然而，如果训练图像模糊，则渲染质量会严重下降。模糊通常是由于镜头散焦、物体运动和相机抖动而产生的，它不可避免地会干扰干净图像的获取。先前的几项研究试图使用神经场从模糊的输入图像中渲染干净清晰的图像。然而，这些工作中的大多数仅设计用于基于体积渲染的神经辐射场，并不直接适用于基于光栅化的3D高斯散射方法。因此，我们提出了一种新的实时去模糊框架，即去模糊3D高斯散点，使用小型多层感知器（MLP）来操纵每个3D高斯的协方差来对场景模糊度进行建模。虽然去模糊的3D高斯飞溅仍然可以享受实时渲染，但它可以从模糊的图像中重建精细和清晰的细节。在基准上进行了各种实验，结果表明了我们的去模糊方法的有效性。定性结果可在https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/ et.al.|[2401.00834](http://arxiv.org/abs/2401.00834)|null|
|**2024-01-01**|**Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using Sharpness Prior**|神经辐射场（NeRF）在基于神经渲染的新视图合成中表现出了显著的性能。然而，当输入图像是在不完美的条件下拍摄的时，NeRF会遭受严重的视觉质量下降，例如照明不良、散焦模糊和透镜像差。特别是，当通常使用相机拍摄图像时，散焦模糊在图像中非常常见。尽管最近很少有研究提出渲染相当高质量的清晰图像，但它们仍然面临许多关键挑战。特别地，这些方法采用了基于多层感知器（MLP）的NeRF，这需要大量的计算时间。为了克服这些缺点，本文提出了一种新的技术Sharp-NeRF——一种基于网格的NeRF，它可以在半小时的训练内从输入的模糊图像中渲染干净清晰的图像。为此，我们使用了几个基于网格的内核来准确地对场景的清晰度/模糊度进行建模。计算像素的清晰度水平以学习空间变化的模糊核。我们在由模糊图像组成的基准上进行了实验，并评估了完全参考和非参考指标。定性和定量的结果表明，我们的方法以生动的色彩和精细的细节呈现出尖锐的新颖观点，并且它比以前的作品具有更快的训练时间。我们的项目页面位于https://benhenryl.github.io/SharpNeRF/ et.al.|[2401.00825](http://arxiv.org/abs/2401.00825)|**[link](https://github.com/benhenryl/sharpnerf)**|
|**2024-01-02**|**GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields**|在本文中，我们专注于单镜头新颖视图合成（O-NVS）任务，该任务的目标是在每个场景只有一个参考图像的情况下合成照片逼真的新颖视图。先前的一次性可泛化神经辐射场（OG-NeRF）方法以无推理时间微调的方式解决了这一任务，但由于仅编码器的架构高度依赖于有限的参考图像，因此存在模糊问题。另一方面，最近的基于扩散的图像到3d方法通过将预先训练的2D扩散模型提取到3d表示中显示出生动可信的结果，但需要繁琐的逐场景优化。针对这些问题，我们提出了GD $^2$-NeRF，这是一个通过GAN和Diffusion的生成细节补偿框架，既不需要推理时间微调，又具有生动可信的细节。详细地说，遵循从粗到细的策略，GD$^2$-NeRF主要由一级并行流水线（OPP）和3D一致细节增强器（Diff3DE）组成。在粗略阶段，OPP首先将GAN模型有效地插入到现有的OG-NeRF管道中，以主要缓解从训练数据集中捕获的分布内先验的模糊问题，实现清晰度（LPIPS、FID）和保真度（PSNR、SSIM）之间的良好平衡。然后，在精细阶段，Diff3DE进一步利用预先训练的图像扩散模型来补充丰富的分布细节，同时保持良好的3D一致性。在合成数据集和真实世界数据集上进行的大量实验表明，GD$^2$ -NeRF在没有每场景微调的情况下显著改善了细节。 et.al.|[2401.00616](http://arxiv.org/abs/2401.00616)|null|
|**2023-12-28**|**iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views**|我们提出了iFusion，这是一种新颖的3D对象重建框架，只需要两个具有未知相机姿态的视图。虽然单视图重建会产生视觉上吸引人的结果，但它可能会与实际对象有很大的偏差，尤其是在看不见的一侧。附加视图提高了重建保真度，但需要已知的摄影机姿势。然而，假设姿态的可用性可能是不现实的，并且现有的姿态估计器在稀疏视图场景中失败。为了解决这一问题，我们利用了一个预先训练的新颖视图合成扩散模型，该模型嵌入了关于不同对象的几何形状和外观的隐含知识。我们的策略分为三个步骤：（1）我们反转用于相机姿态估计的扩散模型，而不是合成新的视图。（2） 使用提供的视图和估计的姿态对扩散模型进行微调，使其成为为目标对象量身定制的新型视图合成器。（3） 利用配准的视图和微调的扩散模型，我们重建了3D对象。实验表明，在姿态估计和新视图合成方面都有很强的性能。此外，iFusion与各种重建方法无缝集成，并对其进行了增强。 et.al.|[2312.17250](http://arxiv.org/abs/2312.17250)|**[link](https://github.com/chinhsuanwu/ifusion)**|

## MLLM

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-11**|**HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis**|单程序多数据并行（SPMD）最近被用于训练大型深度神经网络（DNN）。很少有研究探讨其在异构集群中的适用性，以充分利用可用资源进行大型模型学习。本文介绍了OurSystem，这是一个旨在加快异构集群上SPMD DNN训练的自动化系统。\OurSystem联合优化了张量分片策略、异构设备的分片率以及张量交换的通信方法，以实现SPMD并行性的优化分布式训练。我们新颖地将模型划分公式化为程序合成问题，在该问题中，我们在语义上类似于为单个设备设计的程序的分布式指令集上从头开始生成分布式程序，并使用基于a*的搜索算法系统地探索解空间。我们通过将其公式化为线性规划问题来推导最优张量分片率。此外，\OurSystem探索了异构集群中的张量通信优化，并将其集成为节目合成过程的一部分，用于自动选择最佳集体通信原语并应用充分因子广播技术。在具有代表性的工作负载上进行的大量实验表明，\OurSystem在异构集群上实现了高达2.41x的加速。 et.al.|[2401.05965](http://arxiv.org/abs/2401.05965)|null|
|**2024-01-11**|**MatSAM: Efficient Materials Microstructure Extraction via Visual Large Model**|准确有效地提取材料微观图像中的微观结构在探索结构-性能关系和优化工艺参数方面发挥着关键作用。基于深度学习的图像分割技术依赖于手动注释，耗时耗力，难以满足模型可移植性和泛化的要求。Segment Anything Model（SAM）是一种具有强大的深度特征表示和零样本泛化能力的大型视觉模型，为图像分割提供了新的解决方案。然而，在没有人为注释的情况下直接应用SAM来分割材料微观图像中的微观结构并不能达到预期的结果，因为难以使其原生的即时工程适应材料微观图像的关键微观结构的密集和分散特征。在本文中，我们提出了一种基于SAM的通用高效的微观结构提取解决方案MatSAM。根据材料微观结构的分布和形状，设计了一种新的基于点的提示生成策略。它为不同的微观图像生成提示，融合感兴趣区域（ROI）关键点和网格关键点的提示，并集成后处理方法对材料微观结构进行定量表征。对于包括晶界和相在内的常见微观结构，MatSAM实现了优于传统方法的分割性能，甚至优于对光学显微镜（OM）和扫描电子显微镜（SEM）成像的18种材料微观结构进行评估的监督学习方法。我们相信，MatSAM可以显著降低材料微观结构定量表征的成本，并加快新材料的设计。 et.al.|[2401.05638](http://arxiv.org/abs/2401.05638)|null|
|**2024-01-10**|**Large Model based Sequential Keyframe Extraction for Video Summarization**|关键帧提取旨在用视频的最小帧数来总结视频的语义。本文提出了一种基于大模型的视频摘要序列关键帧提取方法，称为LMSKE，它包括以下三个阶段。首先，我们使用大模型“TransNetV21”将视频剪切成连续的镜头，并使用大模型”CLIP2“生成每个镜头中每帧的视觉特征；其次，我们开发了一种自适应聚类算法，为每个镜头生成候选关键帧，每个候选关键帧位于离聚类中心最近的位置；第三，我们通过在每个镜头内消除冗余来进一步减少上述候选关键帧，并最终根据镜头的顺序将它们连接起来，作为最终的顺序关键帧。为了评估LMSKE，我们策划了一个基准数据集并进行了丰富的实验，实验结果表明，LMSKE的性能比相当多的SOTA竞争对手要好得多，平均F1为0.5311，平均保真度为0.8141，平均压缩比为0.9922。 et.al.|[2401.04962](http://arxiv.org/abs/2401.04962)|null|
|**2024-01-09**|**Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs**|在本文中，我们探索了一种新的用户定位方式，即非专业营销人员可以以自然语言的形式仅在给定需求的情况下选择目标用户。这一问题的关键是如何将自然语言转化为实用的结构化逻辑语言，即对营销人员需求的结构化理解。考虑到大型语言模型（LLM）令人印象深刻的自然语言处理能力，我们试图利用LLM来解决这个问题。以往的研究表明，通过思维链提示可以有效地提高LLM的推理能力。但现有的方法仍有一些局限性：（1）以前的方法要么使用简单的“让我们一步一步地思考”咒语，要么在演示中提供固定的例子，而不考虑提示和问题之间的兼容性，这使得LLM在一些复杂的推理任务（如结构化语言转换）中无效。（2） 以前的方法往往在闭源模型或过大的模型中实现，不适合在工业实际场景中实现。在此基础上，我们提出了ARALLM（即类比推理增强的大型语言模型），它由两个模块组成：基于类比推理的提示和推理增强的多任务模型提取。 et.al.|[2401.04319](http://arxiv.org/abs/2401.04319)|null|
|**2024-01-09**|**Advancing Deep Active Learning & Data Subset Selection: Unifying Principles with Information-Theory Intuitions**|本文的核心是通过提高深度学习模型的标签和训练效率来增强深度学习的实用性。为此，我们研究了基于信息论原理的数据子集选择技术，特别是主动学习和主动采样。主动学习提高了标签效率，而主动采样提高了训练效率。有监督的深度学习模型通常需要使用标记数据进行广泛的训练。标签获取可能既昂贵又耗时，而且训练大型模型是资源密集型的，阻碍了学术研究和“大技术”之外的采用。深度学习中现有的数据子集选择方法往往依赖于启发式方法，或者缺乏原则性的信息论基础。相比之下，本文研究了数据子集选择的几个目标及其在深度学习中的应用，力求在信息理论的启发下找到一种更有原则的方法。我们首先将单前向传递深度神经网络中的认知不确定性和任意不确定性解开，这为不同形式的不确定性及其与数据子集选择的相关性提供了有益的直觉和见解。然后，我们提出并研究了在（贝叶斯）深度学习中进行主动学习和数据子集选择的各种方法。最后，我们将各种现有的和提出的方法与权重或预测空间中的信息量的近似联系起来。这项工作的基础是对包括随机变量和观测结果的信息论量的原则性和实用性的表示法。本文展示了从统一的角度工作的好处，并强调了我们对深度学习实际应用的贡献的潜在影响。 et.al.|[2401.04305](http://arxiv.org/abs/2401.04305)|null|
|**2024-01-08**|**Evaluating Brain-Inspired Modular Training in Automated Circuit Discovery for Mechanistic Interpretability**|大型语言模型（LLM）在人工智能领域迅速崛起，凭借其先进的功能改变了广泛的应用程序。随着这些模型越来越成为决策的组成部分，对彻底可解释性的需求从未像现在这样重要。机械解释性通过识别和分析这些复杂系统中的特定子网络或“电路”，提供了一条理解的途径。这种方法的一个关键方面是自动电路发现，它有助于以可行的方式研究GPT4或LLAMA等大型模型。在这种情况下，我们的研究评估了最近的一种方法，即大脑启发模块训练（BIMT），旨在增强神经网络的可解释性。我们展示了BIMT如何显著提高自动电路发现的效率和质量，克服手动方法的局限性。我们的比较分析进一步表明，BIMT在电路质量、发现时间和稀疏性方面优于现有模型。此外，我们还对BIMT进行了全面的计算分析，包括训练持续时间、内存分配要求和推理速度等方面。这项研究推进了创建可信和透明的人工智能系统的更大目标，此外还展示了BIMT在使神经网络更容易理解方面的工作效果。 et.al.|[2401.03646](http://arxiv.org/abs/2401.03646)|null|
|**2024-01-06**|**CaMML: Context-Aware Multimodal Learner for Large Models**|在这项工作中，我们介绍了上下文感知多模式学习器（CaMML），用于调整大型多模式模型（LMM）。CaMML是一个轻量级模块，旨在将多模式上下文样本无缝集成到大型模型中，从而使模型能够从类似的、特定领域的最新信息中获得知识，并做出有根据的推断。重要的是，CaMML具有高度的可扩展性，并且由于其分层设计，可以有效地处理冗长的多模式上下文示例。基于CaMML，我们开发了两个多模式模型，即CaMML-7B和CaMML-13B，它们在多模式任务的基准数据集阵列中表现出了卓越的性能。值得注意的是，在不集成任何外部资源的情况下，CaMML-13B在十多个广泛认可的多模式基准数据集上实现了最先进的性能，以显著的优势超过了LLaVA-1.5（13B）。此外，我们进行了广泛的消融研究，以检查CaMML的内部工作原理，并进行了定性分析，以展示其在处理现实世界中具有挑战性的案例方面的有效性。 et.al.|[2401.03149](http://arxiv.org/abs/2401.03149)|null|
|**2024-01-09**|**Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM**|在对话式人工智能研究中，有一个明显的趋势，即开发具有更多参数的模型，例如ChatGPT等模型。虽然这些扩展的模型往往会产生越来越好的聊天响应，但它们需要大量的计算资源和内存。这项研究探讨了一个相关的问题：相对于单一的大型模型，小型模型的组合能否协同实现可比或增强的性能？我们介绍了一种称为“混合”的方法，这是一种简单而有效的集成多个聊天AI的方法。我们的经验证据表明，当特定的较小模型协同混合时，它们可能会优于或匹配更大模型的能力。例如，仅集成三个中等大小的模型（6B/13B参数）就可以与ChatGPT（175B+参数）等大得多的模型的性能指标相媲美，甚至超过它们。在Chai研究平台上使用A/B测试方法对这一假设进行了为期30天的严格测试，该方法拥有大量用户。这些发现强调了“混合”策略作为一种可行的方法的潜力，可以在不增加相应计算需求的情况下提高聊天人工智能的效率。 et.al.|[2401.02994](http://arxiv.org/abs/2401.02994)|null|
|**2024-01-05**|**LMaaS: Exploring Pricing Strategy of Large Model as a Service for Communication**|下一代通信被设想为智能通信，它可以取代传统的符号通信，在符号通信中，考虑到源和信道的高度浓缩的语义信息将被高效提取和传输。最近流行的GPT4等大型模型和助推学习技术为智能通信奠定了坚实的基础，并促使其在不久的将来得到实际部署。鉴于这些多模式大型语言模型“一次性培训并广泛使用”的特点，我们认为现收现付服务模式将适合这种情况，称为大型服务模型（LMaaS）。然而，交易和定价问题非常复杂，具有异构和动态的客户环境，这使得定价优化问题在寻求现有解决方案方面具有挑战性。在本文中，我们旨在填补这一空白，并将LMaaS市场交易公式化为两步的Stackelberg对策。在第一步中，我们优化了卖家的定价决策，并提出了一种迭代模型定价（IMP）算法，该算法通过推理客户未来的租赁决策来迭代优化大型模型的价格，从而能够实现接近最优的定价解决方案。在第二步中，我们通过设计一个稳健的选择和租赁（RSR）算法来优化客户的选择决策，该算法在严格的理论证明下保证是最优的。大量实验证实了我们算法的有效性和稳健性。 et.al.|[2401.02675](http://arxiv.org/abs/2401.02675)|null|
|**2024-01-05**|**CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs**|在探索通用人工智能（AGI）的发展时，这些模型的关键任务包括解释和处理来自多个图像输入的信息。然而，大型多模式模型（LMM）在这种情况下遇到了两个问题：（1）缺乏细粒度感知，以及（2）倾向于在多个图像中混合信息。我们首先广泛研究了LMM在处理多个输入图像时感知细粒度视觉细节的能力。研究主要集中在两个方面：第一，图像到图像匹配（评估LMM是否能够有效地推理和配对相关图像），第二，多图像到文本匹配（评估LM是否能够准确地捕捉和总结详细的图像信息）。我们对一系列开源和闭源大型模型进行了评估，包括GPT-4V、Gemini、OpenFlamingo和MMICL。为了提高模型性能，我们进一步开发了一种基于多输入多模式模型的对比思维链（CoCoT）提示方法。该方法要求LMM比较多个图像输入之间的相似性和差异性，然后根据识别出的相似性与差异性引导模型回答有关多个图像输出的详细问题。我们的实验结果展示了CoCoT在增强大型多模式模型的多图像理解能力方面的熟练程度。 et.al.|[2401.02582](http://arxiv.org/abs/2401.02582)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-12**|**360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model**|360度全景视频最近吸引了人们对研究和应用的更多兴趣，因为它们带来了更高的沉浸式体验。由于捕捉360度全景视频的昂贵成本，迫切需要通过给定提示生成所需的全景视频。最近，新兴的文本到视频（T2V）扩散方法在标准视频生成中表现出显著的有效性。然而，由于全景视频和标准视频之间在内容和运动模式方面存在显著差距，这些方法在产生令人满意的360度全景视频方面遇到了挑战。在本文中，我们提出了一种可控的全景视频生成管道，称为360度视频扩散模型（360DVD），用于根据给定的提示和运动条件生成全景视频。具体来说，我们介绍了一个名为360适配器的轻量级模块和辅助360增强技术，以转换预训练的T2V模型，用于360度视频生成。我们进一步提出了一个名为WEB360的新全景数据集，该数据集由360度视频-文本对组成，用于训练360DVD，解决了缺乏字幕全景视频数据集的问题。大量实验证明了360DVD在全景视频生成中的优越性和有效性。代码和数据集将很快发布。 et.al.|[2401.06578](http://arxiv.org/abs/2401.06578)|null|
|**2024-01-12**|**Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation**|最近的研究强调了在文本到图像（T2I）模型世代中对不同身份群体的人的刻板描述问题。然而，这些现有方法有几个关键的局限性，包括在评估中明显缺乏对全球身份群体的覆盖，以及与之相关的刻板印象的范围。此外，他们往往在固有的视觉刻板印象（如“体重不足”或“宽边帽”）和文化依赖性刻板印象（例如“有吸引力”或“恐怖分子”）之间缺乏关键区别。在这项工作中，我们通过一种多方面的方法来解决这些局限性，该方法利用现有的文本资源，为我们对T2I模型生成的图像中的地理文化刻板印象的评估奠定基础。我们采用现有的刻板印象基准来识别和评估全球范围内的视觉刻板印象，涵盖135个基于国籍的身份群体。我们证明，与其他属性相比，刻板印象属性出现在这些身份的图像中的可能性是其他属性的三倍。我们进一步调查了对生成图像的描述对不同民族的冒犯程度。最后，通过一个详细的案例研究，我们揭示了所有身份群体的“默认”表征是如何具有刻板印象的。此外，对于Global South，不同属性的图像在视觉上是相似的，即使在明确提示的情况下也是如此。内容警告：某些示例可能包含冒犯性的刻板印象。 et.al.|[2401.06310](http://arxiv.org/abs/2401.06310)|null|
|**2024-01-11**|**Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications**|我们介绍了可变形卷积v4（DCNv4），这是一种高效有效的算子，专为广泛的视觉应用而设计。DCNv4通过两个关键增强解决了其前身DCNv3的局限性：1。去除空间聚合中的softmax归一化以增强其动态特性和表达能力。优化内存访问以最大限度地减少冗余操作以加快速度。与DCNv3相比，这些改进显著加快了收敛速度，并显著提高了处理速度，其中DCNv4实现了三倍以上的正向速度。DCNv4在各种任务中表现出卓越的性能，包括图像分类、实例和语义分割，尤其是图像生成。当集成到潜在扩散模型中的U-Net等生成模型中时，DCNv4的性能优于其基线，突出了其增强生成模型的可能性。在实际应用中，将InternetImage模型中的DCNv3替换为DCNv4以创建FlashInternetImage，可以在不进行进一步修改的情况下提高高达80%的速度和进一步的性能。DCNv4在速度和效率方面的进步，加上其在不同视觉任务中的强大性能，显示出其作为未来视觉模型基础构建块的潜力。 et.al.|[2401.06197](http://arxiv.org/abs/2401.06197)|null|
|**2024-01-10**|**AI Art is Theft: Labour, Extraction, and Exploitation, Or, On the Dangers of Stochastic Pollocks**|自从DALL-E、Midtravel和Stable Diffusion等应用程序推出以来，生成人工智能作为一种创作艺术品的工具一直备受争议。尽管一些人对这些技术提出了最长期的担忧，认为它们预示着未来的完全自动化，但更紧迫的是生成性人工智能对当前创造性劳动的影响。商业领袖们已经开始用人工智能生成的图像取代人类的艺术劳动。作为回应，艺术界发起了一场抗议运动，认为人工智能图像生成是一种盗窃。本文分析、证实和批评了这些论点，得出的结论是，人工智能图像生成器涉及一种不道德的劳动力盗窃。如果正确的话，许多其他人工智能应用程序也依赖于盗窃。 et.al.|[2401.06178](http://arxiv.org/abs/2401.06178)|null|
|**2024-01-11**|**RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks**|我们提出了一种新的无条件视频生成模型，旨在解决长期的空间和时间依赖性。为了捕捉这些依赖性，我们的方法结合了一种混合显式-隐式三平面表示，其灵感来自为三维对象表示开发的3D感知生成框架，并使用奇异潜在代码对整个视频序列进行建模。然后，从中间三平面表示合成单独的视频帧，该中间三平面表达本身是从主要潜在代码导出的。这种新策略将计算复杂度降低了以FLOP为单位的2美元。因此，我们的方法有助于视频的高效和时间连贯生成。此外，与自回归方法相比，我们的联合框架建模方法减少了视觉伪影的产生。我们通过在基于生成对抗性网络（GAN）的生成器架构中集成基于光流的模块，进一步增强了模型的能力，从而补偿了较小生成器尺寸带来的限制。因此，我们的模型能够合成分辨率为256美元×256美元像素的高保真视频片段，持续时间以30帧/秒的帧速率扩展到5美元秒以上。我们的方法的有效性和多功能性通过三个不同数据集（包括合成和真实视频剪辑）的定性和定量评估得到了实证验证。 et.al.|[2401.06035](http://arxiv.org/abs/2401.06035)|null|
|**2024-01-11**|**EraseDiff: Erasing Data Influence in Diffusion Models**|为了响应数据保护法规和“被遗忘权”，在这项工作中，我们引入了一种扩散模型的遗忘算法。我们的算法为扩散模型配备了一种机制，以减轻与数据记忆相关的问题。为了实现这一点，我们将遗忘问题公式化为双层优化问题，其中外部目标是保持扩散模型对剩余数据的效用。内部目标旨在通过偏离基本事实去噪程序的可学习生成过程来清除与遗忘数据相关的信息。为了解决由此产生的双层问题，我们采用了一阶方法，具有优越的实际性能，同时对扩散过程保持警惕，并解决了其中的双层问题。从经验上讲，我们证明了我们的算法可以保持模型的效用、有效性和效率，同时在两个广泛使用的扩散模型中以及在有条件和无条件的图像生成场景中进行去除。在我们的实验中，我们展示了从人脸和对象数据集（如UTKFace、CelebA、CelebA-HQ和CIFAR10）中忘记类、属性，甚至种族。 et.al.|[2401.05779](http://arxiv.org/abs/2401.05779)|null|
|**2024-01-11**|**Learn From Zoom: Decoupled Supervised Contrastive Learning For WCE Image Classification**|无线胶囊内窥镜（WCE）图像中的准确病变分类对于胃肠道（GI）癌的早期诊断和治疗至关重要。然而，这项任务面临着诸如微小病变和背景干扰等挑战。此外，WCE图像表现出更高的类内方差和类间相似性，增加了复杂性。为了应对这些挑战，我们提出了用于WCE图像分类的解耦监督对比学习，从由显著性增强器生成的放大WCE图像中学习鲁棒表示。具体来说，我们使用均匀下采样的WCE图像作为锚点，使用来自同一类的WCE图片，尤其是它们的放大图片作为阳性。这种方法使特征提取器能够从同一图像的不同视图中捕获丰富的表示，这得益于解耦监督对比学习。在10个时期内对这些表示进行线性分类器训练，产生了令人印象深刻的92.01%的总体准确率，在两个可公开访问的WCE数据集的混合上超过了先前最先进的（SOTA）0.72%。代码位于：https://github.com/Qiukunpeng/DSCL. et.al.|[2401.05771](http://arxiv.org/abs/2401.05771)|null|
|**2024-01-11**|**Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for Text-to-Image Generation**|最近的工作表明，在文本到图像（T2I）生成中，使用具有质量奖励的强化学习（RL）可以提高生成图像的质量。然而，多个奖励的简单聚合可能会导致某些指标的过度优化和其他指标的退化，手动找到最佳权重是一项挑战。联合优化RL中用于T2I生成的多个奖励的有效策略是非常可取的。本文介绍了一种新的用于T2I生成的多奖励RL框架Parrot。通过使用逐批Pareto最优选择，Parrot在T2I生成的RL优化过程中自动识别不同奖励之间的最优权衡。此外，Parrot对T2I模型和提示扩展网络采用了联合优化方法，促进了质量感知文本提示的生成，从而进一步提高了最终图像质量。为了抵消由于提示扩展而导致的原始用户提示的潜在灾难性遗忘，我们在推理时引入了以原始提示为中心的引导，确保生成的图像忠实于用户输入。广泛的实验和用户研究表明，Parrot在各种质量标准上都优于几种基线方法，包括美学、人类偏好、图像情感和文本图像对齐。 et.al.|[2401.05675](http://arxiv.org/abs/2401.05675)|null|
|**2024-01-10**|**From Pampas to Pixels: Fine-Tuning Diffusion Models for Gaúcho Heritage**|世代人工智能已经在社会中普及，见证了各个领域的重大进步。特别是在文本到图像（TTI）模型领域，潜在扩散模型（LDM）展示了基于文本提示生成视觉内容的卓越能力。本文探讨LDM在代表当地文化概念、历史人物和濒危物种方面的潜力。在本研究中，我们以巴西南里奥格兰德州的文化遗产为例。我们的目标是帮助更广泛地理解生成模型如何有助于捕捉和保存地区的文化和历史特征。论文概述了方法，包括主题选择、数据集创建和微调过程。结果展示了生成的图像，以及每个概念的挑战和可行性。总之，这项工作展示了这些模型在代表和保护不同地区和社区的独特方面的力量。 et.al.|[2401.05520](http://arxiv.org/abs/2401.05520)|null|
|**2024-01-10**|**PIXART-δ: Fast and Controllable Image Generation with Latent Consistency Models**|本技术报告介绍了PIXART-｛\delta｝，这是一种文本到图像合成框架，将潜在一致性模型（LCM）和ControlNet集成到高级PIXART-｛\alpha｝模型中。PIXART-｛\alpha｝因其通过非常有效的训练过程生成1024px分辨率的高质量图像的能力而受到认可。在PIXART-｛\delta｝中集成LCM显著加快了推理速度，仅需2-4步即可生成高质量图像。值得注意的是，PIXART-｛\delta｝在生成1024x1024像素图像方面实现了0.5秒的突破，这标志着比PIXART-｛\alpha｝提高了7倍。此外，PIXART-｛\delta｝被设计为可在一天内在32GB V100 GPU上有效训练。凭借其8位推理能力（von Platen et al.，2023），PIXART-｛\delta｝可以在8GB GPU内存限制内合成1024px的图像，大大提高了其可用性和可访问性。此外，结合类似ControlNet的模块可以实现对文本到图像扩散模型的细粒度控制。我们引入了一种新颖的ControlNet Transformer架构，专门为变压器量身定制，在生成高质量图像的同时实现了明确的可控性。作为一种最先进的开源图像生成模型，PIXART-｛\delta｝为稳定扩散模型家族提供了一种很有前途的替代方案，对文本到图像的合成做出了重大贡献。 et.al.|[2401.05252](http://arxiv.org/abs/2401.05252)|**[link](https://github.com/PixArt-alpha/PixArt-alpha)**|

## avatar

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-09**|**DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation**|我们提出了DiffSHEG，这是一种基于扩散的方法，用于语音驱动的任意长度的整体三维表达和手势生成。虽然之前的工作专注于共同语音手势或单独生成表情，但同步表情和手势的联合生成仍然很少被探索。为了解决这一问题，我们基于扩散的协同语音运动生成转换器实现了从表情到手势的单向信息流，有助于改进联合表情手势分布的匹配。此外，我们还介绍了一种基于外绘的采样策略，用于扩散模型中的任意长序列生成，提供了灵活性和计算效率。我们的方法提供了一种实用的解决方案，可以产生由语音驱动的高质量同步表情和手势生成。在两个公共数据集上进行评估后，我们的方法在数量和质量上都达到了最先进的性能。此外，一项用户研究证实了DiffSHEG优于先前的方法。通过实现表达和同步运动的实时生成，DiffSHEG展示了其在数字人和具体代理开发中的各种应用潜力。 et.al.|[2401.04747](http://arxiv.org/abs/2401.04747)|null|
|**2024-01-11**|**Jump Cut Smoothing for Talking Heads**|跳跃式剪裁给观看体验带来了一种突然的、有时是不必要的变化。我们提出了一个新颖的框架来平滑这些跳跃剪辑，在谈话头部视频的背景下。我们利用视频中其他源帧中受试者的外观，将其与由DensePose关键点和面部标志驱动的中级表示融合。为了实现运动，我们在切割周围的结束帧之间插入关键点和地标。然后，我们使用来自关键点和源帧的图像翻译网络来合成像素。由于关键点可能包含错误，我们提出了一种跨模态注意力方案，以在每个关键点的多个选项中选择最合适的来源。通过利用这种中级表示，我们的方法可以获得比强大的视频插值基线更强的结果。我们在会说话的头部视频中演示了我们的方法，如剪切填充词、停顿，甚至随机剪切。我们的实验表明，即使在有挑战性的情况下，我们也可以实现无缝转换，其中会说话的头部在跳跃切割中旋转或剧烈移动。 et.al.|[2401.04718](http://arxiv.org/abs/2401.04718)|null|
|**2024-01-01**|**Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade**|我们介绍了一个图形感知的自动编码器集成框架，以及相关的形式主义和工具，旨在促进人文学科学术的深度学习。通过组成子体系结构以生成同构于人文领域的模型，我们保持了可解释性，同时为每个子体系结构选择提供功能签名，允许传统和计算研究人员在不干扰既定实践的情况下进行合作。我们展示了我们的方法在美国后大西洋奴隶贸易历史研究中的实际应用，并做出了一些具体的技术贡献：一种新的混合图卷积自动编码器机制，用于常见图拓扑的批处理策略，以及用于特定用例的掩蔽技术。越来越多的20多项研究证明了该框架扩大不同领域参与的有效性，这些研究既有与人文主义者的合作，也有机器学习文献中的既定任务，涵盖了各种领域和数据模式。我们对几种不同的体系结构选择进行了性能比较，最后列出了这项研究即将采取的下一步行动。 et.al.|[2401.00824](http://arxiv.org/abs/2401.00824)|null|
|**2023-12-23**|**Human101: Training 100+FPS Human Gaussians in 100s from 1 View**|从单视角视频中重构人体在虚拟现实领域发挥着关键作用。一种流行的应用场景需要快速重建高保真3D数字人，同时确保实时渲染和交互。现有的方法往往难以满足这两个要求。在本文中，我们介绍了Human101，这是一种新颖的框架，通过在100秒内训练3D高斯人并以100+FPS进行渲染，能够从单视图视频中生成高保真动态3D人体重建。我们的方法利用了3D高斯飞溅的优势，它提供了3D人类的明确而有效的表示。与之前基于NeRF的管道不同，Human101巧妙地应用了以人为中心的前向高斯动画方法来变形3D高斯的参数，从而提高了渲染速度（即，以令人印象深刻的60+FPS渲染1024个分辨率的图像，以100+FPS渲染512个分辨率的图片）。实验结果表明，我们的方法大大超过了当前的方法，每秒帧数激增了10倍，并提供了相当或卓越的渲染质量。代码和演示将在上发布https://github.com/longxiang-ai/Human101. et.al.|[2312.15258](http://arxiv.org/abs/2312.15258)|**[link](https://github.com/longxiang-ai/human101)**|
|**2023-12-23**|**Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models**|扩散模型以文本提示为条件，生成具有复杂细节的逼真图像。但是，当涉及到手、牙齿等人类特征时，大多数预先训练的模型都无法生成准确的图像。我们假设，可以通过注释良好的高质量数据来克服扩散模型的这种无能。在本文中，我们专门研究了使用扩散模型改进手-物体交互图像的生成。我们收集了一个注释良好的手-物交互合成数据集，该数据集使用Prompt Propose-Verify框架进行策划，并在其上微调稳定的扩散模型。我们根据CLIPScore、ImageReward、Fedility和alignment等定性和定量指标对图像-文本数据集进行评估，并显示出比当前最先进的基准测试更好的性能。 et.al.|[2312.15247](http://arxiv.org/abs/2312.15247)|null|
|**2023-12-23**|**TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**|直接语音到语音翻译通过引入自监督学习中获得的离散单元来实现高质量的结果。这种方法避免了与模型级联相关的延迟和级联错误。然而，与音频语音相比，有声头翻译，即将视听语音（即有声头视频）从一种语言转换为另一种语言，仍然面临着几个挑战：（1）现有方法总是依赖于级联，通过音频和文本进行合成，导致延迟和级联错误。（2） 会说话的头翻译有一套有限的参考框架。如果生成的翻译超过了原始语音的长度，则需要通过重复帧来补充视频序列，从而导致不和谐的视频转换。在这项工作中，我们提出了一个有声头翻译模型\textbf{TransFace}，它可以直接将视听语音翻译成其他语言的视听语音。它由一个语音到单元的翻译模型和一个基于单元的视听语音合成器Unit2Lip组成，前者将音频语音转换为离散单元，后者并行地从离散单元重新合成同步的视听语音。此外，我们还引入了一个有界持续时间预测器，确保等轴测头的平移并防止重复的参考帧。实验表明，我们提出的Unit2Lip模型显著提高了同步性（原始和生成的音频语音在LSE-C上分别为1.601和0.982），并将LRS2上的推理速度提高了4.35倍。此外，TransFace在LRS3-T和100%等时翻译上的Es-En和Fr-En的BLEU得分分别为61.93和47.55。 et.al.|[2312.15197](http://arxiv.org/abs/2312.15197)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|
|**2023-12-22**|**Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images**|随着元宇宙慢慢成为现实，以及数字人类的快速发展，对人脸原则风格编辑管道的需求势必会增加。我们通过引入潜在2语义自动编码器（L2SAE）来满足这一需求，这是一种生成式自动编码器模型，有助于对人脸图像中几个感兴趣区域（ROI）的风格属性进行高度本地化编辑。L2SAE学习编码图像的结构和风格信息的单独潜在表示。因此，允许对所选择的ROI进行保留结构的样式编辑。编码的结构表示是具有减少的空间维度的多通道2D张量，其捕获局部和全局结构属性。样式表示是捕获全局样式属性的1D张量。在我们的框架中，我们对结构表示进行切片，以建立与不同ROI的强且不纠缠的对应关系。因此，所选ROI的样式编辑相当于（a）从切片结构表示生成的ROI掩模和（b）具有全局样式变化的解码图像的简单组合，该全局样式变化是从操纵的（使用高斯噪声）全局样式和不变的结构张量生成的。在没有额外人力监督的情况下进行风格编辑是对SOTA风格编辑管道的重大胜利，因为大多数现有作品都需要额外的人力（监督）后期培训，才能将语义归因于风格编辑。我们还取消了基于迭代优化的反演或在训练后确定可控的潜在方向，这需要额外的计算成本高昂的操作。我们在多个应用程序中为相同的应用程序提供定性和定量结果，例如使用从多个数据集采样的测试图像进行选择性风格编辑和交换。 et.al.|[2312.15037](http://arxiv.org/abs/2312.15037)|null|
|**2023-12-21**|**From Past to Future: Digital Methods Towards Artefact Analysis**|在过去的二十年里，数字人文改变了人文和社会科学的格局，实现了对广泛数据集的高级计算分析和解释。值得注意的是，东南亚，特别是新加坡最近的举措侧重于对历史数据进行分类和归档，如艺术品、文学作品，尤其是考古文物。这项研究通过在两个不同的人工制品数据集上应用统计方法，展示了数字人文的巨大潜力。具体而言，我们展示了对公元1千年中期东南亚大陆“旭日”铸币的自动模具研究结果，随后对从新加坡中部殖民前圣安德鲁大教堂遗址挖掘的13-14世纪陶器的2D图像使用了无监督统计方法。这项研究提供了一个比较评估，展示了基于统计的方法对不同考古材料的解释和分析以及数字人文整体的变革影响。 et.al.|[2312.13790](http://arxiv.org/abs/2312.13790)|null|
|**2023-12-18**|**Relightable Neural Actor with Intrinsic Decomposition and Pose Control**|在视觉和图形学中，创建一个可欣赏、可驾驶和逼真的数字人类化身是一个具有挑战性的重要问题。人类是高度立体化的，会产生依赖姿势的外观效果，如自我阴影和皱纹，皮肤和衣服需要复杂且空间变化的BRDF模型。虽然最近的人类重新照明方法可以从多视图视频中恢复看似合理的材料光分解，但它们不能推广到新颖的姿势，并且仍然存在视觉伪影。为了解决这一问题，我们提出了Relightable Neural Actor，这是第一种基于视频的方法，用于学习照片真实感的神经人体模型，该模型可以重新照明，允许外观编辑，并可以由任意骨骼姿势控制。重要的是，为了学习我们的人类化身，我们只需要在已知但静态的照明条件下对人类进行多视图记录。为了实现这一点，我们用可驱动的密度场来表示演员的几何体，该密度场对姿势相关的服装变形进行建模，并提供3D和UV空间之间的映射，其中对法线、可见性和材质进行编码。为了在现实世界场景中评估我们的方法，我们收集了一个新的数据集，其中包括在室内和室外不同光照条件下记录的四个参与者，为人类重新照明提供了第一个此类基准，并展示了最先进的新人类姿势的重新照明结果。 et.al.|[2312.11587](http://arxiv.org/abs/2312.11587)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

