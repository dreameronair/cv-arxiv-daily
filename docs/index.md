---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.02.01
> Usage instructions: [here](./docs/README.md#usage)

## avatar

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**DressCode: Autoregressively Sewing and Generating Garments from Text Guidance**|服装在人类外表中的重要作用突显了服装数字化对数字人类创造的重要性。3D内容创作的最新进展对数字人类创作至关重要。尽管如此，基于文本指导的服装生成仍处于萌芽阶段。我们介绍了一个文本驱动的3D服装生成框架DressCode，旨在为新手民主化设计，并在时尚设计、虚拟试穿和数字人类创作方面提供巨大潜力。对于我们的框架，我们首先介绍了SewingGPT，这是一种基于GPT的架构，将交叉注意力与文本条件嵌入相结合，以生成具有文本指导的缝合模式。我们还为高质量、基于瓦片的PBR纹理生成量身定制了预训练的稳定扩散。通过利用大型语言模型，我们的框架通过自然语言交互生成CG友好型服装。我们的方法还便于图案完成和纹理编辑，通过用户友好的交互简化了设计师的流程。通过与其他最先进的方法进行全面的评估和比较，我们的方法展示了最佳的质量和与输入提示的一致性。用户研究进一步验证了我们的高质量渲染结果，突出了其在生产环境中的实用性和潜力。 et.al.|[2401.16465](http://arxiv.org/abs/2401.16465)|null|
|**2024-01-29**|**Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis**|在生物特征安全成为现代身份验证系统基石的时代，确保这些生物特征样本的真实性至关重要。活体检测，即区分真实和伪造生物特征样本的能力，是这一挑战的前沿。这项研究对活跃度检测模型进行了全面评估，特别关注它们在跨数据库场景中的性能，这是一种因其复杂性和现实世界相关性而臭名昭著的测试范式。我们的研究首先在单个数据集上仔细评估模型，揭示其性能指标的细微差别。深入研究了一半总错误率、错误接受率和错误拒绝率等指标，我们发现了对模型优势和劣势的宝贵见解。至关重要的是，我们对跨数据库测试的探索提供了一个独特的视角，突出了在一个数据集上训练和在另一个数据集中部署之间的鸿沟。与现有方法的比较分析，从卷积网络到更复杂的策略，丰富了我们对当前形势的理解。即使在最先进的模型之间，性能的差异也突显了这一领域的固有挑战。从本质上讲，这篇论文既是研究结果的宝库，也是对生物特征活体检测中更细致、数据多样和适应性更强的方法的明确呼吁。在真实性和欺骗性之间的动态舞蹈中，我们的工作为驾驭生物识别安全的发展节奏提供了蓝图。 et.al.|[2401.16232](http://arxiv.org/abs/2401.16232)|**[link](https://github.com/ZamDimon/liveness-detection)**|
|**2024-01-27**|**AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**|最近的社区在从稀疏的多视图视频构建照片逼真的可动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色呈现逼真的服装动态，因为它们主要依赖裸体模型进行人体建模，同时保留未建模的服装部分。这主要是因为宽松服装产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为4-8）生成宽松衣服中的可动画化人类化身的新方法。为了能够在这种情况下捕捉和学习宽松服装的外观，我们使用了从基于物理的模拟数据中获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕捉和渲染复杂的服装动力学。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动力学的新方法。为了使用粗略估计为看不见的衣服状态建立逼真的渲染，引入了一个以身体和衣服运动为条件的姿势驱动的可变形神经辐射场，提供了对这两个部分的显式控制。在测试时，可以从看不见的情况中捕捉新的服装姿势，这些姿势来自基于物理或神经网络的模拟器，以驱动看不见服装的动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松、动作各异的表演者。实验表明，我们的方法能够呈现出与身体高度偏离的自然服装动力学，并很好地推广到看不见的视图和姿势，超过了现有方法的性能。代码和数据将公开。 et.al.|[2401.15348](http://arxiv.org/abs/2401.15348)|null|
|**2023-11-30**|**DanceMeld: Unraveling Dance Phrases with Hierarchical Latent Codes for Music-to-Dance Synthesis**|在3D数字人类应用领域，音乐到舞蹈是一项具有挑战性的任务。鉴于音乐和舞蹈之间的一对多关系，以前的方法在方法上受到限制，仅依赖于基于音乐节奏的匹配和生成相应的舞蹈动作。在专业编舞领域，一个舞蹈短语由几个舞蹈姿势和舞蹈动作组成。舞蹈姿势由一系列基本的有意义的身体姿势组成，而舞蹈动作可以反映舞蹈的节奏、旋律和风格等动态变化。从这些概念中获得灵感，我们引入了一个名为DanceField的创新舞蹈生成管道，它包括两个阶段，即舞蹈解耦阶段和舞蹈生成阶段。在解耦阶段，使用分层VQ-VAE来分解不同特征空间级别中的舞蹈姿势和舞蹈动作，其中底部代码表示舞蹈姿势，顶部代码表示舞蹈动作。在生成阶段，我们使用扩散模型作为先验来对分布进行建模，并根据音乐特征生成潜在代码。我们已经通过实验证明了顶部代码和底部代码的表示能力，从而实现了舞蹈姿势和舞蹈动作的显式解耦表达。这种解开不仅提供了对动作细节、风格和节奏的控制，还促进了舞蹈风格转换和舞蹈单元编辑等应用。我们的方法在AIST++数据集上进行了定性和定量实验，证明了其优于其他方法。 et.al.|[2401.10242](http://arxiv.org/abs/2401.10242)|null|
|**2024-01-09**|**DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation**|我们提出了DiffSHEG，这是一种基于扩散的方法，用于语音驱动的任意长度的整体三维表达和手势生成。虽然之前的工作专注于共同语音手势或单独生成表情，但同步表情和手势的联合生成仍然很少被探索。为了解决这一问题，我们基于扩散的协同语音运动生成转换器实现了从表情到手势的单向信息流，有助于改进联合表情手势分布的匹配。此外，我们还介绍了一种基于外绘的采样策略，用于扩散模型中的任意长序列生成，提供了灵活性和计算效率。我们的方法提供了一种实用的解决方案，可以产生由语音驱动的高质量同步表情和手势生成。在两个公共数据集上进行评估后，我们的方法在数量和质量上都达到了最先进的性能。此外，一项用户研究证实了DiffSHEG优于先前的方法。通过实现表达和同步运动的实时生成，DiffSHEG展示了其在数字人和具体代理开发中的各种应用潜力。 et.al.|[2401.04747](http://arxiv.org/abs/2401.04747)|null|
|**2024-01-11**|**Jump Cut Smoothing for Talking Heads**|跳跃式剪裁给观看体验带来了一种突然的、有时是不必要的变化。我们提出了一个新颖的框架来平滑这些跳跃剪辑，在谈话头部视频的背景下。我们利用视频中其他源帧中受试者的外观，将其与由DensePose关键点和面部标志驱动的中级表示融合。为了实现运动，我们在切割周围的结束帧之间插入关键点和地标。然后，我们使用来自关键点和源帧的图像翻译网络来合成像素。由于关键点可能包含错误，我们提出了一种跨模态注意力方案，以在每个关键点的多个选项中选择最合适的来源。通过利用这种中级表示，我们的方法可以获得比强大的视频插值基线更强的结果。我们在会说话的头部视频中演示了我们的方法，如剪切填充词、停顿，甚至随机剪切。我们的实验表明，即使在有挑战性的情况下，我们也可以实现无缝转换，其中会说话的头部在跳跃切割中旋转或剧烈移动。 et.al.|[2401.04718](http://arxiv.org/abs/2401.04718)|null|
|**2024-01-01**|**Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade**|我们介绍了一个图形感知的自动编码器集成框架，以及相关的形式主义和工具，旨在促进人文学科学术的深度学习。通过组成子体系结构以生成同构于人文领域的模型，我们保持了可解释性，同时为每个子体系结构选择提供功能签名，允许传统和计算研究人员在不干扰既定实践的情况下进行合作。我们展示了我们的方法在美国后大西洋奴隶贸易历史研究中的实际应用，并做出了一些具体的技术贡献：一种新的混合图卷积自动编码器机制，用于常见图拓扑的批处理策略，以及用于特定用例的掩蔽技术。越来越多的20多项研究证明了该框架扩大不同领域参与的有效性，这些研究既有与人文主义者的合作，也有机器学习文献中的既定任务，涵盖了各种领域和数据模式。我们对几种不同的体系结构选择进行了性能比较，最后列出了这项研究即将采取的下一步行动。 et.al.|[2401.00824](http://arxiv.org/abs/2401.00824)|null|
|**2023-12-23**|**Human101: Training 100+FPS Human Gaussians in 100s from 1 View**|从单视角视频中重构人体在虚拟现实领域发挥着关键作用。一种流行的应用场景需要快速重建高保真3D数字人，同时确保实时渲染和交互。现有的方法往往难以满足这两个要求。在本文中，我们介绍了Human101，这是一种新颖的框架，通过在100秒内训练3D高斯人并以100+FPS进行渲染，能够从单视图视频中生成高保真动态3D人体重建。我们的方法利用了3D高斯飞溅的优势，它提供了3D人类的明确而有效的表示。与之前基于NeRF的管道不同，Human101巧妙地应用了以人为中心的前向高斯动画方法来变形3D高斯的参数，从而提高了渲染速度（即，以令人印象深刻的60+FPS渲染1024个分辨率的图像，以100+FPS渲染512个分辨率的图片）。实验结果表明，我们的方法大大超过了当前的方法，每秒帧数激增了10倍，并提供了相当或卓越的渲染质量。代码和演示将在上发布https://github.com/longxiang-ai/Human101. et.al.|[2312.15258](http://arxiv.org/abs/2312.15258)|**[link](https://github.com/longxiang-ai/human101)**|
|**2023-12-23**|**Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models**|扩散模型以文本提示为条件，生成具有复杂细节的逼真图像。但是，当涉及到手、牙齿等人类特征时，大多数预先训练的模型都无法生成准确的图像。我们假设，可以通过注释良好的高质量数据来克服扩散模型的这种无能。在本文中，我们专门研究了使用扩散模型改进手-物体交互图像的生成。我们收集了一个注释良好的手-物交互合成数据集，该数据集使用Prompt Propose-Verify框架进行策划，并在其上微调稳定的扩散模型。我们根据CLIPScore、ImageReward、Fedility和alignment等定性和定量指标对图像-文本数据集进行评估，并显示出比当前最先进的基准测试更好的性能。 et.al.|[2312.15247](http://arxiv.org/abs/2312.15247)|null|
|**2023-12-23**|**TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**|直接语音到语音翻译通过引入自监督学习中获得的离散单元来实现高质量的结果。这种方法避免了与模型级联相关的延迟和级联错误。然而，与音频语音相比，有声头翻译，即将视听语音（即有声头视频）从一种语言转换为另一种语言，仍然面临着几个挑战：（1）现有方法总是依赖于级联，通过音频和文本进行合成，导致延迟和级联错误。（2） 会说话的头翻译有一套有限的参考框架。如果生成的翻译超过了原始语音的长度，则需要通过重复帧来补充视频序列，从而导致不和谐的视频转换。在这项工作中，我们提出了一个有声头翻译模型\textbf{TransFace}，它可以直接将视听语音翻译成其他语言的视听语音。它由一个语音到单元的翻译模型和一个基于单元的视听语音合成器Unit2Lip组成，前者将音频语音转换为离散单元，后者并行地从离散单元重新合成同步的视听语音。此外，我们还引入了一个有界持续时间预测器，确保等轴测头的平移并防止重复的参考帧。实验表明，我们提出的Unit2Lip模型显著提高了同步性（原始和生成的音频语音在LSE-C上分别为1.601和0.982），并将LRS2上的推理速度提高了4.35倍。此外，TransFace在LRS3-T和100%等时翻译上的Es-En和Fr-En的BLEU得分分别为61.93和47.55。 et.al.|[2312.15197](http://arxiv.org/abs/2312.15197)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|
|**2023-12-22**|**Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images**|随着元宇宙慢慢成为现实，以及数字人类的快速发展，对人脸原则风格编辑管道的需求势必会增加。我们通过引入潜在2语义自动编码器（L2SAE）来满足这一需求，这是一种生成式自动编码器模型，有助于对人脸图像中几个感兴趣区域（ROI）的风格属性进行高度本地化编辑。L2SAE学习编码图像的结构和风格信息的单独潜在表示。因此，允许对所选择的ROI进行保留结构的样式编辑。编码的结构表示是具有减少的空间维度的多通道2D张量，其捕获局部和全局结构属性。样式表示是捕获全局样式属性的1D张量。在我们的框架中，我们对结构表示进行切片，以建立与不同ROI的强且不纠缠的对应关系。因此，所选ROI的样式编辑相当于（a）从切片结构表示生成的ROI掩模和（b）具有全局样式变化的解码图像的简单组合，该全局样式变化是从操纵的（使用高斯噪声）全局样式和不变的结构张量生成的。在没有额外人力监督的情况下进行风格编辑是对SOTA风格编辑管道的重大胜利，因为大多数现有作品都需要额外的人力（监督）后期培训，才能将语义归因于风格编辑。我们还取消了基于迭代优化的反演或在训练后确定可控的潜在方向，这需要额外的计算成本高昂的操作。我们在多个应用程序中为相同的应用程序提供定性和定量结果，例如使用从多个数据集采样的测试图像进行选择性风格编辑和交换。 et.al.|[2312.15037](http://arxiv.org/abs/2312.15037)|null|
|**2023-12-21**|**From Past to Future: Digital Methods Towards Artefact Analysis**|在过去的二十年里，数字人文改变了人文和社会科学的格局，实现了对广泛数据集的高级计算分析和解释。值得注意的是，东南亚，特别是新加坡最近的举措侧重于对历史数据进行分类和归档，如艺术品、文学作品，尤其是考古文物。这项研究通过在两个不同的人工制品数据集上应用统计方法，展示了数字人文的巨大潜力。具体而言，我们展示了对公元1千年中期东南亚大陆“旭日”铸币的自动模具研究结果，随后对从新加坡中部殖民前圣安德鲁大教堂遗址挖掘的13-14世纪陶器的2D图像使用了无监督统计方法。这项研究提供了一个比较评估，展示了基于统计的方法对不同考古材料的解释和分析以及数字人文整体的变革影响。 et.al.|[2312.13790](http://arxiv.org/abs/2312.13790)|null|
|**2023-12-18**|**Relightable Neural Actor with Intrinsic Decomposition and Pose Control**|在视觉和图形学中，创建一个可欣赏、可驾驶和逼真的数字人类化身是一个具有挑战性的重要问题。人类是高度立体化的，会产生依赖姿势的外观效果，如自我阴影和皱纹，皮肤和衣服需要复杂且空间变化的BRDF模型。虽然最近的人类重新照明方法可以从多视图视频中恢复看似合理的材料光分解，但它们不能推广到新颖的姿势，并且仍然存在视觉伪影。为了解决这一问题，我们提出了Relightable Neural Actor，这是第一种基于视频的方法，用于学习照片真实感的神经人体模型，该模型可以重新照明，允许外观编辑，并可以由任意骨骼姿势控制。重要的是，为了学习我们的人类化身，我们只需要在已知但静态的照明条件下对人类进行多视图记录。为了实现这一点，我们用可驱动的密度场来表示演员的几何体，该密度场对姿势相关的服装变形进行建模，并提供3D和UV空间之间的映射，其中对法线、可见性和材质进行编码。为了在现实世界场景中评估我们的方法，我们收集了一个新的数据集，其中包括在室内和室外不同光照条件下记录的四个参与者，为人类重新照明提供了第一个此类基准，并展示了最先进的新人类姿势的重新照明结果。 et.al.|[2312.11587](http://arxiv.org/abs/2312.11587)|null|
|**2023-12-18**|**VectorTalker: SVG Talking Face Generation with Progressive Vectorisation**|高保真度和高效的音频驱动谈话头生成一直是计算机图形学和计算机视觉领域的一个关键研究课题。在这项工作中，我们研究了基于矢量图像的音频驱动的谈话头生成。与现有作品中使用最广泛的光栅图像直接动画相比，矢量图像具有良好的可扩展性，可用于多种应用。基于矢量图像的会说话的头生成面临两个主要挑战：高质量的矢量图像重建（相对于源肖像图像）和生动的动画（相对于音频信号）。为了解决这些问题，我们提出了一种新的可扩展矢量图形重建和动画方法，称为VectorTalker。具体而言，对于高保真度重建，VectorTalker以从粗到细的方式分层重建矢量图像。对于生动的音频驱动的面部动画，我们建议使用面部标志作为中间运动表示，并提出一个有效的标志驱动的矢量图像变形模块。我们的方法可以在一个统一的框架内处理各种风格的肖像图像，包括日本漫画、卡通和照片真实感图像。我们进行了广泛的定量和定性评估，实验结果证明了VectorTalker在矢量图形重建和音频驱动动画方面的优势。 et.al.|[2312.11568](http://arxiv.org/abs/2312.11568)|null|
|**2023-12-18**|**AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis**|音频驱动的会说话的头部合成是一个很有前途的课题，在数字人、电影制作和虚拟现实等领域有着广泛的应用。与之前的研究相比，最近基于NeRF的方法在质量和保真度方面显示出优势。然而，当涉及到少镜头会说话的头部生成时，在一个身份只有几秒钟会说话的视频的实际场景中，出现了两个限制：1）它们要么没有基本模型，作为快速收敛的面部先验，要么在构建先验时忽略了音频的重要性；2） 它们大多忽略了不同面部区域与音频之间的相关性，例如，嘴与音频相关，而耳朵与音频无关。在本文中，我们提出了音频增强神经辐射场（AE NeRF）来解决上述问题，它可以用最少的镜头数据集生成新扬声器的逼真肖像。具体来说，我们在参考方案的特征融合阶段引入了音频感知聚合模块，其中权重由参考图像和目标图像之间音频的相似性决定。然后，提出了一种基于双NeRF框架的音频对齐人脸生成策略，分别对音频相关区域和音频无关区域进行建模。大量实验表明，即使在有限的训练集或训练迭代中，AE NeRF在图像保真度、音频嘴唇同步和泛化能力方面也超过了最先进的技术。 et.al.|[2312.10921](http://arxiv.org/abs/2312.10921)|null|
|**2024-01-31**|**MM-TTS: Multi-modal Prompt based Style Transfer for Expressive Text-to-Speech Synthesis**|文本到语音中的风格转换任务是指将风格信息转换为文本内容，以生成具有特定风格的相应语音的过程。然而，现有的大多数风格转移方法要么基于固定的情感标签，要么基于参考语音片段，无法实现灵活的风格转移。最近，一些方法采用文本描述来引导风格转移。在本文中，我们提出了一个更灵活的多模式和风格可控的TTS框架MM-TTS。它可以在统一的多模态提示空间中使用任何模态作为提示，包括参考语音、情感面部图像和文本描述，以控制系统中生成的语音的风格。建模这种多模态风格可控的TTS的挑战主要在于两个方面：1）将多模态信息对齐到统一的风格空间中，以允许在单个系统中输入任意模态作为风格提示，从而赋予生成提示风格相关语音的能力。为了解决这些问题，我们提出了一种对齐的多模态提示编码器，该编码器将不同的模态嵌入到统一的风格空间中，支持不同模态的风格转换。此外，我们提出了一种新的自适应风格转换方法，称为风格自适应卷积，以实现更好的风格表示。此外，我们还设计了一种基于整流流的细化器，以解决Mel频谱图过于平滑的问题，并生成高保真度的音频。由于没有公共的多模态TTS数据集，我们构建了一个名为MEAD-TTS的数据集，该数据集与表达性谈话头部领域有关。我们在MEAD-TTS数据集和域外数据集上的实验表明，基于多模态提示的MM-TTS可以获得令人满意的结果。 et.al.|[2312.10687](http://arxiv.org/abs/2312.10687)|null|
|**2023-12-15**|**DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models**|扩散模型在各种下游生成任务中取得了显著的成功，但在重要且具有挑战性的表达型会说话的头部生成中仍有待探索。在这项工作中，我们提出了一个DreamTalk框架来填补这一空白，该框架采用了细致的设计来释放扩散模型在生成富有表现力的谈话头方面的潜力。具体来说，DreamTalk由三个关键组成部分组成：去噪网络、风格感知嘴唇专家和风格预测器。基于扩散的去噪网络能够在不同的表情中一致地合成高质量的音频驱动的面部运动。为了提高嘴唇动作的表现力和准确性，我们引入了一位风格敏感的嘴唇专家，他可以在注意说话风格的同时指导嘴唇同步。为了消除对表达参考视频或文本的需要，利用额外的基于扩散的风格预测器来直接从音频预测目标表达。通过这种方式，DreamTalk可以利用强大的扩散模型有效地生成富有表情的人脸，并减少对昂贵风格参考的依赖。实验结果表明，DreamTalk能够生成具有不同说话风格的照片逼真的说话脸，并实现准确的嘴唇运动，超过了现有的最先进的同类产品。 et.al.|[2312.09767](http://arxiv.org/abs/2312.09767)|null|
|**2023-12-15**|**Riveter: Measuring Power and Social Dynamics Between Entities**|Riveer为分析文本语料库中与实体相关的动词含义提供了一个完整的、易于使用的管道。我们在包装中预先填充了情感、权力和代理的内涵框架，这些框架已证明对在广泛的语料库中捕捉社会现象（如性别偏见）是有用的。几十年来，词汇框架一直是计算社会科学、数字人文科学和自然语言处理的基础工具，促进了文本语料库的多方面分析。但是，研究以动词为中心的词汇特别需要自然语言处理技能，这降低了其他研究人员对其的可及性。通过组织语言处理管道，为语料库中的所有实体提供完整的词典评分和可视化，并为用户提供针对特定研究问题的功能，Riveer大大提高了动词词汇的可访问性，并可以促进未来的广泛研究。 et.al.|[2312.09536](http://arxiv.org/abs/2312.09536)|**[link](https://github.com/maartensap/riveter-nlp)**|
|**2023-12-15**|**3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting**|我们介绍了一种使用3D高斯飞溅（3DGS）从单眼视频创建可动画化人类化身的方法。现有的基于神经辐射场（NeRFs）的方法实现了高质量的新视图/新姿态图像合成，但通常需要数天的训练，并且在推理时非常慢。最近，该社区探索了快速网格结构，以有效训练穿着衣服的化身。尽管训练速度极快，但这些方法几乎无法实现约15 FPS的交互式渲染帧速率。在本文中，我们使用3D高斯飞溅并学习非刚性变形网络来重建可动画化的穿着衣服的人类化身，这些化身可以在30分钟内训练并以实时帧速率（50+FPS）渲染。考虑到我们表示的显式性质，我们进一步在高斯均值向量和协方差矩阵上引入尽可能等距的正则化，增强了我们的模型在高度清晰的看不见姿态上的泛化能力。实验结果表明，与最先进的方法相比，我们的方法在从单目输入创建可动画化身方面实现了相当甚至更好的性能，同时在训练和推理方面分别快了400倍和250倍。 et.al.|[2312.09228](http://arxiv.org/abs/2312.09228)|null|
|**2023-12-12**|**On the Potential of an Independent Avatar to Augment Metaverse User Socialization Time**|我们提出了一种计算建模方法，旨在捕捉如何通过在元宇宙中使用独立和自主版本的数字表示来虚拟增加元宇宙用户的可用社交时间容量的细节。我们设想了一个以元宇宙为中心的传统化身概念的扩展：当用户不直接控制化身时，化身也可以被编程为独立操作，从而将其变成基于代理的数字人类表示。这样，用户可以虚拟地委托维持现有联系人所需的化身社交时间，以便最终维持空闲的非化身介导的社交时间，该空闲的社交时间可以潜在地投资于额外的社交活动。我们使用社会科学中选定的概念对环境进行建模，并确定特征变量：自我网络、社会存在和社会线索。然后，我们将用户的非化身中介空闲时间最大化问题公式化为线性优化。最后，我们分析了问题的可行区域，并对化身介导的交互的不同参数值可以实现的空闲时间提出了一些初步见解。 et.al.|[2312.07077](http://arxiv.org/abs/2312.07077)|null|
|**2023-12-11**|**DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers**|我们提出了一种新的谈话头部合成管道，称为“DiT头部”，它基于扩散变换器，并使用音频作为条件来驱动扩散模型的去噪过程。我们的方法是可扩展的，可以推广到多个身份，同时产生高质量的结果。我们训练和评估我们提出的方法，并将其与现有的谈话头部合成方法进行比较。我们证明，我们的模型在视觉质量和唇同步精度方面可以与这些方法竞争。我们的研究结果突出了我们提出的方法在广泛应用中的潜力，包括虚拟助理、娱乐和教育。有关结果的视频演示和我们的用户研究，请参阅我们的补充材料。 et.al.|[2312.06400](http://arxiv.org/abs/2312.06400)|null|
|**2023-12-09**|**R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning**|动态NeRF最近在3D说话肖像合成方面引起了越来越多的关注。尽管在渲染速度和视觉质量方面取得了进步，但在提高效率和有效性方面仍然存在挑战。我们提出了R2 Talker，这是一个高效有效的框架，能够实现逼真的实时谈话头部合成。具体来说，使用多分辨率哈希网格，我们介绍了一种将面部地标编码为条件特征的新方法。该方法通过将任意地标映射到统一的特征空间，将地标结构无损地编码为条件特征、解耦输入分集和条件空间。我们进一步提出了一种在NeRF渲染管道中进行渐进多层条件处理的方案，用于有效的条件特征融合。与现有技术相比，我们的新方法具有以下优势：1）无损输入编码能够获得更精确的特征，产生卓越的视觉质量。输入和条件空间的解耦提高了可推广性。2） 条件特征和MLP输出在每个MLP层的融合增强了条件影响，从而实现更准确的嘴唇合成和更好的视觉质量。3） 它紧凑地构造了条件特征的融合，显著提高了计算效率。 et.al.|[2312.05572](http://arxiv.org/abs/2312.05572)|null|
|**2023-12-05**|**PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features**|语音驱动的三维人脸动画近年来有了很大的进步，而大多数相关工作只利用声学模态，忽略了视觉和文本线索的影响，导致在精度和连贯性方面效果不佳。我们认为视觉和文本线索不是琐碎的信息。因此，我们提出了一种新的框架，即PMMTalk，利用互补的伪多模态特征来提高面部动画的准确性。该框架包含三个模块：PMMTalk编码器、跨模态对准模块和PMMTalx解码器。具体而言，PMMTalk编码器采用现成的谈话头生成架构和语音识别技术，分别从语音中提取视觉信息和文本信息。随后，跨模态对齐模块在时间和语义级别上对齐音频图像文本特征。然后采用PMMTalk解码器对假唱人脸的混合形状系数进行预测。与现有方法相反，PMMTalk只需要额外的随机参考人脸图像，但会产生更准确的结果。此外，它还对艺术家友好，因为它通过引入面部混合形状系数无缝集成到标准动画制作工作流程中。最后，考虑到3D会说话人脸数据集的稀缺性，我们介绍了一个大规模的3D中文视听人脸动画（3D-CAVFA）数据集。大量的实验和用户研究表明，我们的方法优于现有技术。我们建议观看补充视频。 et.al.|[2312.02781](http://arxiv.org/abs/2312.02781)|null|
|**2023-12-07**|**VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior**|近年来，音频驱动的会说话的头部生成引起了人们的广泛关注，并在嘴唇同步、富有表情的面部表情、自然的头部姿势生成和高视频质量方面做出了许多努力。然而，由于音频和运动之间的一对多映射，尚未有任何模型在所有这些指标上领先或捆绑。在本文中，我们提出了VividTalk，这是一个两阶段的通用框架，支持生成具有所有上述属性的高视觉质量的头部谈话视频。具体来说，在第一阶段，我们通过学习两个运动将音频映射到网格，包括非刚性表情运动和刚性头部运动。对于表达式运动，采用混合形状和顶点作为中间表示，以最大限度地提高模型的表示能力。针对自然头部运动，提出了一种新的具有两阶段训练机制的可学习头部姿态码本。在第二阶段，我们提出了一个双分支运动vae和一个生成器来将网格转换为密集运动，并逐帧合成高质量的视频。大量实验表明，所提出的VividTalk可以生成具有唇同步和逼真度的高视觉质量的谈话头部视频，并在客观和主观比较方面优于以往最先进的作品。 et.al.|[2312.01841](http://arxiv.org/abs/2312.01841)|null|
|**2023-11-29**|**AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text**|我们研究仅从文本描述创建高保真度和可动画化的3D化身的问题。现有的文本到化身的方法要么局限于不能被动画化的静态化身，要么难以生成具有良好质量和精确姿势控制的可动画化化身。为了解决这些限制，我们提出了AvatarStudio，这是一个从粗到细的生成模型，用于为可动画化的人类化身生成显式纹理3D网格。具体而言，AvatarStudio从基于低分辨率NeRF的粗略生成表示开始，然后将SMPL引导的关节运动合并到显式网格表示中，以支持化身动画和高分辨率渲染。为了确保生成的化身的视图一致性和姿势可控性，我们引入了一个以DensePose为条件的二维扩散模型，用于分数蒸馏采样监督。通过有效利用铰接网格表示和DensePose条件扩散模型之间的协同作用，AvatarStudio可以从准备好动画的文本中创建高质量的化身，显著优于以前的方法。此外，它适用于许多应用，例如，多模式化身动画和风格引导的化身创建。有关更多结果，请参阅我们的项目页面：http://jeff95.me/projects/avatarstudio.html et.al.|[2311.17917](http://arxiv.org/abs/2311.17917)|null|
|**2023-11-29**|**HUGS: Human Gaussian Splats**|神经渲染的最新进展已经将训练和渲染时间提高了几个数量级。虽然这些方法展示了最先进的质量和速度，但它们是为静态场景的摄影测量而设计的，不能很好地推广到环境中自由移动的人类。在这项工作中，我们介绍了人类高斯飞溅（HUGS），它表示可动画化的人类以及使用3D高斯飞溅（3DGS）的场景。我们的方法只拍摄一个具有少量（50-100）帧的单眼视频，它会自动学习在30分钟内解开静态场景和完全可动画化的人类化身。我们利用SMPL身体模型来初始化人类高斯。为了捕捉SMPL未建模的细节（如布料、头发），我们允许3D高斯与人体模型偏离。为动画人类使用3D高斯带来了新的挑战，包括在表达高斯时产生的人工制品。我们建议联合优化线性混合蒙皮权重，以协调动画过程中各个高斯人的运动。我们的方法实现了人类的新颖姿势合成和人类和场景的新颖视角合成。我们以60 FPS的渲染速度实现了最先进的渲染质量，同时比之前的工作快了约100倍。我们的代码将在此处公布：https://github.com/apple/ml-hugs et.al.|[2311.17910](http://arxiv.org/abs/2311.17910)|null|
|**2023-11-29**|**Gaussian Shell Maps for Efficient 3D Human Generation**|高效生成3D数字人在包括虚拟现实、社交媒体和电影制作在内的几个行业都很重要。3D生成对抗性网络（GANs）已经证明了生成资产的最先进（SOTA）质量和多样性。然而，当前的3D GAN架构通常依赖于体积表示，而体积表示渲染缓慢，从而阻碍了GAN训练并需要多视图不一致的2D上采样器。在这里，我们介绍了高斯壳映射（GSMs）作为一个框架，它使用可连接的基于多壳的支架将SOTA生成器网络架构与新兴的3D高斯渲染基元连接起来。在此设置中，CNN会生成一个3D纹理堆栈，其中包含映射到壳的特征。后者代表了处于标准身体姿势的数字人的模板表面的膨胀和收缩版本。我们不对壳进行直接光栅化，而是对其属性编码在纹理特征中的壳进行3D高斯采样。这些高斯是有效且可微分的。在GAN训练过程中，关节连接外壳的能力很重要，在推理时，将身体变形为任意用户定义的姿势。我们高效的渲染方案绕过了对视图不一致上采样器的需求，以512美元\乘以512美元像素的原生分辨率实现了高质量的多视图一致渲染。我们证明，当在包括SHHQ和DeepFashion在内的单视图数据集上进行训练时，GSMs成功地生成了3D人。 et.al.|[2311.17857](http://arxiv.org/abs/2311.17857)|null|
|**2023-11-29**|**SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis**|在逼真的、语音驱动的谈话头部视频的合成中实现高度同步是一个重大挑战。传统的生成对抗性网络（GAN）难以保持一致的面部身份，而神经辐射场（NeRF）方法虽然可以解决这个问题，但往往会产生不匹配的嘴唇动作、不恰当的面部表情和不稳定的头部姿势。一个栩栩如生的会说话的头需要主体身份、嘴唇动作、面部表情和头部姿势的同步协调。缺乏这些同步是一个根本缺陷，导致了不切实际和人为的结果。为了解决同步这一关键问题，我们引入了SyncTalk，它被认为是创建逼真谈话头的“魔鬼”。这种基于NeRF的方法有效地保持了主体身份，增强了说话头部合成的同步性和真实性。SyncTalk采用面部同步控制器将嘴唇运动与语音对齐，并创新性地使用3D面部混合形状模型来捕捉准确的面部表情。我们的头部同步稳定器可优化头部姿势，实现更自然的头部运动。肖像同步生成器恢复头发细节，并将生成的头部与躯干融合，以获得无缝的视觉体验。大量的实验和用户研究表明，SyncTalk在同步和逼真度方面优于最先进的方法。我们建议观看补充视频：https://ziqiaopeng.github.io/synctalk et.al.|[2311.17590](http://arxiv.org/abs/2311.17590)|**[link](https://github.com/ZiqiaoPeng/SyncTalk)**|
|**2023-11-30**|**Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation**|我们研究了创建一个可以从动画角色的单个图像实时控制的角色模型的问题。这个问题的解决方案将大大降低创建化身、计算机游戏和其他交互式应用程序的成本。Talking Head Anime 3（THA3）是一个开源项目，试图直接解决这个问题。它采用（1）动画角色上身的图像和（2）45维姿势矢量作为输入，并输出采用指定姿势的同一角色的新图像。可能的动作范围对于个人化身和某些类型的游戏角色来说足够有表现力。然而，该系统太慢，无法在普通PC上实时生成动画，并且其图像质量可以提高。在本文中，我们从两个方面改进THA3。首先，我们提出了新的组成网络架构，该架构基于U-Nets旋转角色的头部和身体，并在现代生成模型中广泛使用。新的架构始终产生比THA3基线更好的图像质量。尽管如此，它们也会使整个系统慢得多：生成一帧需要150毫秒。其次，我们提出了一种技术，将系统提取到一个小型网络（小于2MB）中，该网络可以使用消费者游戏GPU实时生成512x512个动画帧（低于30FPS），同时保持接近整个系统的图像质量。这一改进使整个系统在实时应用中具有实用性。 et.al.|[2311.17409](http://arxiv.org/abs/2311.17409)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

