---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.03
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-02**|**Street Gaussians for Modeling Dynamic Urban Scenes**|本文旨在解决从单目视频中建模动态城市街道场景的问题。最近的方法扩展了NeRF，将跟踪车辆姿态纳入车辆动画，实现了动态城市街道场景的照片逼真视图合成。然而，它们的显著局限性在于训练和渲染速度慢，再加上履带车辆姿态对高精度的迫切需求。我们介绍了Street Gaussians，一种新的明确的场景表示，它解决了所有这些限制。具体地说，动态城市街道被表示为一组点云，这些点云配备有语义logits和3D Gaussians，每一个都与前景车辆或背景相关联。为了对前景对象车辆的动力学进行建模，使用可优化的跟踪姿态以及动态外观的动态球面谐波模型对每个对象点云进行优化。显式表示允许容易地合成目标车辆和背景，这反过来又允许在半小时的训练内以133 FPS（1066 $\times$ 1600分辨率）进行场景编辑操作和渲染。所提出的方法在多个具有挑战性的基准上进行了评估，包括KITTI和Waymo Open数据集。实验表明，在所有数据集上，所提出的方法始终优于最先进的方法。此外，尽管仅依赖于现成跟踪器的姿态，但所提出的表示提供的性能与使用精确的地面实况姿态所实现的性能不相上下。代码位于https://zju3dv.github.io/street_gaussians/. et.al.|[2401.01339](http://arxiv.org/abs/2401.01339)|null|
|**2024-01-01**|**Deblurring 3D Gaussian Splatting**|最近对辐射场的研究为具有照片级真实感渲染质量的新颖视图合成铺平了坚实的道路。然而，它们通常使用神经网络和体积绘制，这两种方法的训练成本很高，并且由于绘制时间长，阻碍了它们在各种实时应用中的广泛使用。最近，人们提出了一种基于3D高斯散射的方法来对3D场景进行建模，并在实时渲染图像的同时实现了显著的视觉质量。然而，如果训练图像模糊，则渲染质量会严重下降。模糊通常是由于镜头散焦、物体运动和相机抖动而产生的，它不可避免地会干扰干净图像的获取。先前的几项研究试图使用神经场从模糊的输入图像中渲染干净清晰的图像。然而，这些工作中的大多数仅设计用于基于体积渲染的神经辐射场，并不直接适用于基于光栅化的3D高斯散射方法。因此，我们提出了一种新的实时去模糊框架，即去模糊3D高斯散点，使用小型多层感知器（MLP）来操纵每个3D高斯的协方差来对场景模糊度进行建模。虽然去模糊的3D高斯飞溅仍然可以享受实时渲染，但它可以从模糊的图像中重建精细和清晰的细节。在基准上进行了各种实验，结果表明了我们的去模糊方法的有效性。定性结果可在https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/ et.al.|[2401.00834](http://arxiv.org/abs/2401.00834)|null|
|**2024-01-01**|**Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using Sharpness Prior**|神经辐射场（NeRF）在基于神经渲染的新视图合成中表现出了显著的性能。然而，当输入图像是在不完美的条件下拍摄的时，NeRF会遭受严重的视觉质量下降，例如照明不良、散焦模糊和透镜像差。特别是，当通常使用相机拍摄图像时，散焦模糊在图像中非常常见。尽管最近很少有研究提出渲染相当高质量的清晰图像，但它们仍然面临许多关键挑战。特别地，这些方法采用了基于多层感知器（MLP）的NeRF，这需要大量的计算时间。为了克服这些缺点，本文提出了一种新的技术Sharp-NeRF——一种基于网格的NeRF，它可以在半小时的训练内从输入的模糊图像中渲染干净清晰的图像。为此，我们使用了几个基于网格的内核来准确地对场景的清晰度/模糊度进行建模。计算像素的清晰度水平以学习空间变化的模糊核。我们在由模糊图像组成的基准上进行了实验，并评估了完全参考和非参考指标。定性和定量的结果表明，我们的方法以生动的色彩和精细的细节呈现出尖锐的新颖观点，并且它比以前的作品具有更快的训练时间。我们的项目页面位于https://benhenryl.github.io/SharpNeRF/ et.al.|[2401.00825](http://arxiv.org/abs/2401.00825)|**[link](https://github.com/benhenryl/sharpnerf)**|
|**2024-01-02**|**GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields**|在本文中，我们专注于单镜头新颖视图合成（O-NVS）任务，该任务的目标是在每个场景只有一个参考图像的情况下合成照片逼真的新颖视图。先前的一次性可泛化神经辐射场（OG-NeRF）方法以无推理时间微调的方式解决了这一任务，但由于仅编码器的架构高度依赖于有限的参考图像，因此存在模糊问题。另一方面，最近的基于扩散的图像到3d方法通过将预先训练的2D扩散模型提取到3d表示中显示出生动可信的结果，但需要繁琐的逐场景优化。针对这些问题，我们提出了GD $^2$-NeRF，这是一个通过GAN和Diffusion的生成细节补偿框架，既不需要推理时间微调，又具有生动可信的细节。详细地说，遵循从粗到细的策略，GD$^2$-NeRF主要由一级并行流水线（OPP）和3D一致细节增强器（Diff3DE）组成。在粗略阶段，OPP首先将GAN模型有效地插入到现有的OG-NeRF管道中，以主要缓解从训练数据集中捕获的分布内先验的模糊问题，实现清晰度（LPIPS、FID）和保真度（PSNR、SSIM）之间的良好平衡。然后，在精细阶段，Diff3DE进一步利用预先训练的图像扩散模型来补充丰富的分布细节，同时保持良好的3D一致性。在合成数据集和真实世界数据集上进行的大量实验表明，GD$^2$ -NeRF在没有每场景微调的情况下显著改善了细节。 et.al.|[2401.00616](http://arxiv.org/abs/2401.00616)|null|
|**2023-12-28**|**iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views**|我们提出了iFusion，这是一种新颖的3D对象重建框架，只需要两个具有未知相机姿态的视图。虽然单视图重建会产生视觉上吸引人的结果，但它可能会与实际对象有很大的偏差，尤其是在看不见的一侧。附加视图提高了重建保真度，但需要已知的摄影机姿势。然而，假设姿态的可用性可能是不现实的，并且现有的姿态估计器在稀疏视图场景中失败。为了解决这一问题，我们利用了一个预先训练的新颖视图合成扩散模型，该模型嵌入了关于不同对象的几何形状和外观的隐含知识。我们的策略分为三个步骤：（1）我们反转用于相机姿态估计的扩散模型，而不是合成新的视图。（2） 使用提供的视图和估计的姿态对扩散模型进行微调，使其成为为目标对象量身定制的新型视图合成器。（3） 利用配准的视图和微调的扩散模型，我们重建了3D对象。实验表明，在姿态估计和新视图合成方面都有很强的性能。此外，iFusion与各种重建方法无缝集成，并对其进行了增强。 et.al.|[2312.17250](http://arxiv.org/abs/2312.17250)|**[link](https://github.com/chinhsuanwu/ifusion)**|
|**2023-12-28**|**Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis**|动态场景的新颖视图合成一直是一个有趣但具有挑战性的问题。尽管取得了最新进展，但同时实现高分辨率的真实照片效果、实时渲染和紧凑的存储仍然是一项艰巨的任务。为了应对这些挑战，我们提出了时空高斯特征飞溅作为一种新的动态场景表示，由三个关键组件组成。首先，我们通过增强具有时间不透明度和参数运动/旋转的3D高斯，来形成富有表现力的时空高斯。这使得时空高斯能够捕捉场景中的静态、动态以及瞬态内容。其次，我们介绍了飞溅特征渲染，它用神经特征代替了球面谐波。这些功能有助于在保持小尺寸的同时对视图和与时间相关的外观进行建模。第三，我们利用训练误差和粗略深度的指导，在难以与现有管道融合的区域对新的高斯采样。在几个已建立的真实世界数据集上的实验表明，我们的方法在保持紧凑存储的同时，实现了最先进的渲染质量和速度。在8K分辨率下，我们的lite版本模型可以在Nvidia RTX 4090 GPU上以60 FPS的速度渲染。 et.al.|[2312.16812](http://arxiv.org/abs/2312.16812)|null|
|**2023-12-29**|**DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision**|我们见证了基于深度学习的3D视觉的重大进展，从基于神经辐射场（NeRF）的3D表示学习到在新视图合成（NVS）中的应用。然而，用于基于深度学习的3D视觉的现有场景级数据集，仅限于合成环境或现实世界场景的狭窄选择，是非常不足的。这种不足不仅阻碍了现有方法的全面基准，而且限制了在基于深度学习的3D分析中可以探索的内容。为了解决这一关键差距，我们展示了DL3DV-10K，这是一个大型场景数据集，其特征是从65种类型的兴趣点（POI）位置捕获的10510个视频中的5120万帧，涵盖了有界和无界场景，具有不同的反射、透明度和光照水平。我们在DL3DV-10K上对最近的NVS方法进行了全面的基准测试，为未来的NVS研究提供了宝贵的见解。此外，我们在一项从DL3DV-10K学习可推广NeRF的试点研究中获得了令人鼓舞的结果，这表明了大规模场景级数据集的必要性，以打造学习3D表示的基础模型。我们的DL3DV-10K数据集、基准测试结果和模型将在https://dl3dv-10k.github.io/DL3DV-10K/. et.al.|[2312.16256](http://arxiv.org/abs/2312.16256)|null|
|**2023-12-26**|**fMPI: Fast Novel View Synthesis in the Wild with Layered Scene Representations**|在这项研究中，我们为基于分层场景表示的新视图合成（NVS）方法提出了两种新的输入处理范式，这两种方法在不影响质量的情况下显著提高了它们的运行时间。我们的方法识别并减轻了传统管道最耗时的两个方面：构建和处理所谓的平面扫描体积（PSV），这是输入相机视图的平面重新投影的高维张量。特别是，我们建议在并行组中处理该张量，以提高计算效率，并对相邻输入平面进行超采样，从而生成更密集、更准确的场景表示。所提出的增强提供了显著的灵活性，允许在性能和速度之间取得平衡，从而朝着实时应用迈出了实质性的步伐。此外，它们非常通用，因为任何基于PSV的方法都可以使用它们，包括使用多平面图像、多球体图像和分层深度图像的方法。在一组全面的实验中，我们证明了我们提出的范式能够设计出一种NVS方法，该方法在公共基准上达到最先进的水平，同时比现有的最先进的方法快50倍。它在速度方面也比目前的前辈高出3倍多，同时实现了明显更好的渲染质量。 et.al.|[2312.16109](http://arxiv.org/abs/2312.16109)|null|
|**2023-12-25**|**Sparse-view CT Reconstruction with 3D Gaussian Volumetric Representation**|稀疏视图CT是减少传统CT扫描辐射剂量的一种很有前途的策略，但从不完整和有噪声的数据中重建高质量图像是一项挑战。最近，3D高斯已被应用于复杂自然场景的建模，与隐式神经表示（INRs）相比，它表现出快速收敛和更好的新颖视图渲染。我们从3D高斯在自然场景建模和新视图合成中的成功应用中获得灵感，研究了它们在稀疏视图CT重建中的潜力。我们利用来自滤波后的反投影重建图像的先验信息来初始化高斯；并且通过比较投影空间中的差异来更新它们的参数。自适应密度控制进一步提高了性能。与INRs相比，3D高斯从先验信息中受益更多，可以明确绕过空白空间中的学习，并有效地分配容量，加速收敛。3D高斯还可以有效地学习高频细节。3D高斯以自我监督的方式进行训练，避免了对大规模配对数据的需要。我们在AAPM-Mayo数据集上的实验表明，与基于INR的方法相比，3D高斯可以提供优越的性能。这项工作正在进行中，代码将公开。 et.al.|[2312.15676](http://arxiv.org/abs/2312.15676)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|

## MLLM

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-02**|**Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models**|多模式大语言模型（MLLM）的兴起激发了人们对基于语言的驾驶任务的兴趣。然而，现有的研究通常集中在有限的任务上，往往忽略了关键的多视图和时间信息，这对稳健的自动驾驶至关重要。为了弥补这些差距，我们引入了NuInstruction，这是一个新的数据集，在17个子任务中有91K个多视图视频QA对，其中每个任务都需要整体信息（例如，时间、多视图和空间），大大提高了挑战水平。为了获得NuInstruction，我们提出了一种新的基于SQL的方法来自动生成指令-响应对，这是受人类驱动逻辑进程的启发。我们进一步介绍了BEV InMLLM，这是一种端到端的方法，用于有效地推导感知指令的鸟瞰图（BEV）特征，该特征与大型语言模型的语言对齐。BEV InMLLM集成了多视图、空间感知和时间语义，以增强MLLM在NuInstruction任务上的能力。此外，我们提出的BEV注入模块是现有MLLMs的即插即用方法。我们在NuInstruction上的实验表明，BEV InMLLM显著优于现有的MLLM，例如在各种任务上提高了约9%。我们计划发布我们的NuInstruction，用于未来的研究开发。 et.al.|[2401.00988](http://arxiv.org/abs/2401.00988)|**[link](https://github.com/xmed-lab/nuinstruct)**|
|**2023-12-31**|**Extracting spectra in the shell model Monte Carlo method using imaginary-time correlation matrices**|在非常大的模型空间中，禁止在构型相互作用（CI）壳层模型方法的框架下计算核能级的传统对角化方法。壳层模型蒙特卡罗（SMMC）是一种在非常大的模型空间中计算核的热和基态可观察性的强大技术，但在这种方法中提取核光谱是一项具有挑战性的技术。我们提出了一种新的方法来提取一组好量子数（如自旋和宇称）的给定值的低能级。该方法基于渐近满足广义特征值问题的虚时一体密度相关矩阵。我们在轻核中验证了该方法，该方法允许与CI壳层模型哈密顿量的精确对角化结果进行比较。该方法适用于可以在CI壳模型方法中描述的其他有限尺寸量子多体系统。 et.al.|[2401.00613](http://arxiv.org/abs/2401.00613)|null|
|**2023-12-31**|**Compressing Deep Image Super-resolution Models**|深度学习技术已被应用于图像超分辨率（SR）领域，在重建性能方面取得了显著进步。现有技术通常采用高度复杂的模型结构，这导致大的模型大小和慢的推理速度。这通常导致高能耗，并限制了它们在实际应用中的采用。为了解决这个问题，这项工作采用了一个三阶段的工作流程来压缩深度SR模型，这大大降低了它们的内存需求。通过使用新设计的蒸馏损失进行师生知识蒸馏，保持了恢复性能。我们已经将这种方法应用于两个流行的图像超分辨率网络，SwinIR和EDSR，以证明其有效性。与原始版本相比，由此产生的紧凑型SwinIRmini和EDSRmini在模型大小和浮点运算（FLOP）方面分别减少了89%和96%。与原始模型和其他常用的SR方法相比，它们还保留了具有竞争力的超分辨率性能。这两种轻量级SR方法的源代码和预训练模型发布于https://pikapi22.github.io/CDISM/. et.al.|[2401.00523](http://arxiv.org/abs/2401.00523)|null|
|**2023-12-29**|**Nonasymptotic Regret Analysis of Adaptive Linear Quadratic Control with Model Misspecification**|在不同的数据集上预训练大型模型，然后针对特定应用进行微调的策略在计算机视觉、自然语言处理和机器人控制方面取得了令人印象深刻的结果。这种策略在自适应控制中具有巨大的潜力，在自适应控制下，有必要在有限的数据下快速适应不断变化的条件。为了具体理解预训练对自适应控制的好处，我们研究了自适应线性二次控制问题，在这种情况下，学习者对动力学的基矩阵集合有先验知识。这个基础是错误的，因为它不能完美地表示底层数据生成过程的动态。我们提出了一种使用这种先验知识的算法，并证明了 $T$与系统交互后预期后悔的上界。在$T$较小的情况下，上限由$\texttt｛poly｝（\log T）$或$\sqrt｛T｝$的术语量表支配，这取决于学习者可获得的先验知识。当$T$很大时，后悔主要由一个随着$\delta T$增长的术语决定，其中$\delta$量化了错误指定的程度。这个线性项是由于无法使用错误指定的基来完美地估计潜在的动力学而产生的，因此是不可避免的，除非基矩阵也在线调整。然而，在由于估计基矩阵的权重的误差而产生的次线性项变得可以忽略之后，它只在大的$T$ 中占主导地位。我们提供的模拟验证了我们的分析。我们的模拟还表明，来自一组相关系统的离线数据可以用作预训练阶段的一部分，以估计错误指定的动力学基础，这反过来又被我们的自适应控制器使用。 et.al.|[2401.00073](http://arxiv.org/abs/2401.00073)|null|
|**2024-01-02**|**Structured Packing in LLM Training Improves Long Context Utilization**|长上下文大型语言模型（LCLMs）的最新进展引起了人们的极大兴趣，尤其是在查询科学研究论文等应用中。然而，它们的潜力往往受到上下文利用不足的限制。我们将典型训练数据中缺乏长程语义依赖性确定为主要障碍。为了解决这一问题，我们深入研究了经常将相关文档纳入培训输入的好处。使用代码数据的固有目录结构作为训练示例的来源，我们展示了困惑的改善，即使是对于与编码无关的任务也是如此。在这些发现的基础上，但着眼于更广泛的领域，我们引入了长上下文结构化包装（SPLiCe）。SPLiCe是一种创新的创建训练示例的方法，它使用检索方法将最相关的文档整理到单个训练上下文中。我们的结果表明，\方法｛｝提高了模型性能，可以用于训练大型模型更好地利用长上下文。我们通过训练一个价值30亿美元的大型模型来验证我们的结果，显示出困惑的改善和下游任务上更好的长上下文性能。 et.al.|[2312.17296](http://arxiv.org/abs/2312.17296)|null|
|**2023-12-27**|**PanGu- $π$: Enhancing Language Model Architectures via Nonlinearity Compensation**|大型语言模型（LLM）最近的趋势是增加模型大小（即参数的数量）和数据集的规模，以获得更好的生成能力，这一点已经被著名的GPT和Llama等许多工作所证明。然而，大型模型往往涉及巨大的计算成本，实际应用无法承受如此高的价格。然而，构建LLM的强模型体系结构的方法很少被讨论。我们首先分析了最先进的语言模型体系结构，并观察了特征崩溃问题。基于理论分析，我们提出非线性对于语言模型也非常重要，这通常在卷积神经网络中用于视觉任务。然后引入序列通知激活函数，并进行可以忽略的微小计算，并进一步使用增广快捷方式来增强模型的非线性。然后，我们证明了所提出的方法在通过精心设计的烧蚀来增强模型非线性方面是显著有效的；因此，我们提出了一种新的建立现代有效的模型体系结构，即PanGu-$\pi$。然后使用相同的数据集和训练策略进行实验，将PanGu-$\pi$与最先进的LLM进行比较。结果表明，PanGu-$\pi$-7B可以以大约10%的推理加速实现与基准测试相当的性能，PanGu-\pi$-1B可以在准确性和效率方面实现最先进的性能。此外，我们还在金融和法律的高价值领域部署了PanGu-$\pi$ -7B，开发了一个名为云山的LLM以供实际应用。结果表明，在基准测试中，云山模型可以超越其他尺度相似的模型。 et.al.|[2312.17276](http://arxiv.org/abs/2312.17276)|null|
|**2023-12-28**|**Multi-Prompts Learning with Cross-Modal Alignment for Attribute-based Person Re-Identification**|细粒度的属性描述可以显著地补充人物图像中有价值的语义信息，这对人物重新识别任务的成功至关重要。然而，当前的ReID算法通常无法有效利用可用的丰富上下文信息，主要是因为它们依赖于图像属性的简单和粗略利用。人工智能生成内容的最新进展使自动生成大量细粒度的属性描述并充分利用它们成为可能。因此，本文探索了在现有（大型）模型的ReID任务中使用生成的多人属性作为提示的潜力，以获得更准确的检索结果。为此，我们提出了一个新的框架，称为多提示ReID（MP ReID），基于提示学习和语言模型，以充分挖掘精细属性来帮助ReID任务。具体来说，MP ReID首先学会产生幻觉，产生各种各样的、信息丰富的、可提示的句子来描述查询图像。该程序包括（i）一个人具有哪些属性的明确提示，以及（ii）用于调整/调节用于该人身份匹配的标准的隐含可学习提示。显式提示是通过集合生成模型（如ChatGPT和VQA模型）来获得的。此外，还设计了一个对齐模块，以逐步融合多提示（即显式提示和隐式提示），并减轻跨模态间隙。在现有的涉及属性的ReID数据集，即Market1501和DukeMTMC ReID上进行的大量实验证明了所提出的MP ReID解决方案的有效性和合理性。 et.al.|[2312.16797](http://arxiv.org/abs/2312.16797)|null|
|**2023-12-27**|**Mobility and Cost Aware Inference Accelerating Algorithm for Edge Intelligence**|边缘智能近年来得到了广泛的应用。在设备、边缘服务器和云之间拆分模型可以大大提高EI的性能。先前的工作已经深入研究了没有用户移动性的模型分割。然而，在EI的大多数使用情况下，终端设备是移动的。在这方面只进行了少数工作。这些工作仍然存在许多问题，如忽视移动设备的能耗、网络假设不当、自适应用户移动性的有效性低等。因此，为了解决以往工作中模型分割和资源分配的不足，我们提出了移动和成本感知的模型分割和资源分配算法，以加速边缘推理（MCSA）。具体地，在没有用户移动性的场景中，提供了循环交互梯度下降（Li-GD）算法。当移动用户有较大的模型推理任务需要计算时，它会考虑移动用户的能耗、通信和计算资源租用成本以及推理延迟，以找到最优的模型分割和资源分配策略。在具有用户移动性的场景中，提出了移动感知Li-GD（MLi-GD）算法来计算最优策略。然后，研究了所提出算法的性质，包括收敛性、复杂度和近似率。实验结果验证了所提算法的有效性。 et.al.|[2312.16497](http://arxiv.org/abs/2312.16497)|null|
|**2023-12-30**|**Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding**|近年来，基于视图的三维形状识别方法的结果已经饱和，并且具有优异性能的模型由于其庞大的参数大小而无法部署在内存有限的设备上。为了解决这个问题，我们为该领域引入了一种基于知识蒸馏的压缩方法，该方法在尽可能保持模型性能的同时，大大减少了参数的数量。具体来说，为了增强较小模型的功能，我们设计了一个高性能的大型模型，称为Group Multi-view Vision Transformer（GMViT）。在GMViT中，视图级ViT首先建立视图级特征之间的关系。此外，为了捕捉更深层次的特征，我们使用分组模块将视图级特征增强为组级特征。最后，组级ViT将组级特征聚合为完整的、良好形成的3D形状描述符。值得注意的是，在这两个ViT中，我们都引入了相机坐标的空间编码作为创新的位置嵌入。此外，我们提出了两个基于GMViT的压缩版本，即GMViT simple和GMViT mini。为了提高小模型的训练有效性，我们在整个GMViT过程中引入了一种知识提取方法，其中每个GMViT组件的关键输出作为提取目标。大量实验证明了该方法的有效性。大模型GMViT在基准数据集ModelNet、ShapeNetCore55和MCB上实现了出色的3D分类和检索结果。较小的模型GMViT simple和GMViT mini分别将参数大小减少了8倍和17.6倍，形状识别速度平均提高了1.5倍，同时保持了至少90%的分类和检索性能。 et.al.|[2312.16477](http://arxiv.org/abs/2312.16477)|null|
|**2023-12-26**|**Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation**|参数有效微调（PEFT）方法为使大型视觉语言模型适应特定任务或场景提供了一种有效的方法。通常，他们在白盒公式中为预先训练的模型学习非常小范围的参数，该公式假设模型架构是已知的，参数是可访问的。然而，出于防止滥用或商业因素的考虑，大型模型往往不是开源的，因此对白盒PEFT方法的部署构成了障碍。为了减轻对模型可访问性的依赖，我们引入了协作黑盒调优（CBBT），用于黑盒模型的文本提示优化和输出特征自适应。具体来说，考虑到反向传播梯度被阻塞，我们通过分析具有扰动提示的预测来近似文本提示的梯度。其次，在不可访问模型的输出特性上部署了一个轻量级适配器，进一步方便了模型的自适应过程。有了这些设计，我们的CBBT在11个下游基准上进行了广泛评估，与现有的黑盒VL自适应方法相比，取得了显著的改进。代码发布于https://github.com/guozix/cbbt. et.al.|[2312.15901](http://arxiv.org/abs/2312.15901)|**[link](https://github.com/guozix/cbbt)**|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-02**|**VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM**|最近在扩散模型方面的创新和突破大大扩展了为给定提示生成高质量视频的可能性。大多数现有作品都处理在单个背景中仅发生一个视频事件的单个场景场景。然而，扩展以生成多场景视频并非易事，需要很好地管理其间的逻辑，同时保持视频场景中关键内容的一致视觉外观。在本文中，我们提出了一个新的框架，即VideoDrafter，用于内容一致的多场景视频生成。从技术上讲，VideoDrafter利用大型语言模型（LLM）将输入提示转换为综合的多场景脚本，该脚本受益于LLM所学到的逻辑知识。每个场景的脚本都包括一个描述事件、前景/背景实体以及相机移动的提示。VideoDrafter识别整个脚本中的常见实体，并要求LLM详细说明每个实体。然后将得到的实体描述馈送到文本到图像模型中，以生成每个实体的参考图像。最后，VideoDrafter通过考虑参考图像、事件的描述性提示和相机移动的扩散过程生成每个场景视频，从而输出多场景视频。扩散模型结合参考图像作为条件和对齐，以增强多场景视频的内容一致性。大量实验表明，VideoDrafter在视觉质量、内容一致性和用户偏好方面优于SOTA视频生成模型。 et.al.|[2401.01256](http://arxiv.org/abs/2401.01256)|null|
|**2024-01-02**|**Joint Generative Modeling of Scene Graphs and Images via Diffusion Models**|本文提出了一种新的生成任务：场景图-图像联合生成。虽然之前的工作已经探索了以场景图或布局为条件的图像生成，但我们的任务是独特而重要的，因为它涉及无条件地从噪声中生成场景图，从而实现对图像生成的有效和可解释的控制。我们的任务具有挑战性，需要为节点（对象）和边（对象之间的关系）生成具有异构属性的看似合理的场景图，包括连续的对象边界框以及离散的对象和关系类别。我们介绍了一种新的扩散模型DiffuseSG，它将邻接矩阵与异构节点和边缘属性联合建模。我们探索了分类数据的各种类型的编码，将其放松到一个连续的空间中。以图转换器为去噪器，DiffuseSG在连续空间中依次对场景图表示进行去噪，并对最终表示进行离散化，以生成干净的场景图。此外，我们引入了IoU正则化来增强经验性能。我们的模型在Visual Genome和COCO Stuff数据集上的场景图生成方面显著优于现有方法，无论是在更好地捕捉问题复杂性的标准指标还是新引入的指标上。此外，我们在两个下游应用程序中展示了我们的模型的额外优势：1）在一系列场景图完成任务中表现出色，以及2）通过使用从DiffuseSG生成的额外训练样本来改进场景图检测模型。 et.al.|[2401.01130](http://arxiv.org/abs/2401.01130)|null|
|**2024-01-02**|**SSP: A Simple and Safe automatic Prompt engineering method towards realistic image synthesis on LVM**|最近，文本到图像（T2I）合成取得了重大进展，特别是随着大型语言模型（LLM）的出现及其在大型视觉模型（LVM）中的增强，大大增强了传统T2I模型的指令跟随能力。尽管如此，以前的方法侧重于提高生成质量，但在提示中引入了不安全因素。我们探讨了在提示中添加特定的摄像头描述可以提高安全性能。因此，我们提出了一种简单安全的即时工程方法（SSP），通过提供最佳的相机描述来提高图像生成质量。具体来说，我们从多个数据集创建一个数据集作为原始提示。为了选择最佳的摄像头，我们设计了一种最佳的摄像头匹配方法，并实现了一个能够自动匹配的原始提示分类器。将相机描述附加到原始提示会生成用于进一步生成LVM图像的优化提示。实验表明，与其他方法相比，SSP平均提高了16%的语义一致性，安全指标提高了48.9%。 et.al.|[2401.01128](http://arxiv.org/abs/2401.01128)|null|
|**2023-12-31**|**TrailBlazer: Trajectory Control for Diffusion-Based Video Generation**|在最近的文本到视频（T2V）生成方法中，在合成视频中实现可控性通常是一个挑战。通常，通过以边缘图、深度图或待更改的现有视频的形式提供低级别的每帧引导来解决此问题。然而，获得这种指导的过程可能是劳动密集型的。本文的重点是通过使用简单的边界框以各种方式引导受试者，增强视频合成的可控性，而无需神经网络训练、微调、推理时的优化或使用预先存在的视频。我们的算法TrailBlazer是在预先训练的（T2V）模型上构建的，易于实现。通过所提出的空间和时间注意力图编辑，受试者由边界框引导。此外，我们引入了关键帧的概念，允许受试者的轨迹和整体外观由移动的边界框和相应的提示来引导，而无需提供详细的遮罩。该方法是有效的，相对于基础的预训练模型，额外的计算可以忽略不计。尽管边界框引导很简单，但产生的运动令人惊讶地自然，随着框大小的增加，出现了包括透视和向虚拟相机移动在内的紧急效果。 et.al.|[2401.00896](http://arxiv.org/abs/2401.00896)|null|
|**2023-12-30**|**Improving the Stability of Diffusion Models for Content Consistent Super-Resolution**|预训练的潜在扩散模型的生成先验在提高图像超分辨率（SR）结果的感知质量方面表现出巨大的潜力。不幸的是，现有的基于扩散先验的SR方法遇到了一个共同的问题，即，它们倾向于为具有不同噪声样本的同一低分辨率图像生成相当不同的输出。这种随机性对于文本到图像生成任务是期望的，但是对于SR任务是有问题的，其中图像内容被期望被很好地保留。为了提高基于扩散先验的SR的稳定性，我们建议使用扩散模型来细化图像结构，同时使用生成对抗性训练来增强图像的精细细节。具体来说，我们提出了一种非均匀时间步长学习策略来训练紧凑的扩散网络，该网络具有高效和稳定的图像主结构再现能力，并通过对抗性训练来微调变分自动编码器（VAE）的预训练解码器，以增强细节。大量实验表明，我们提出的方法，即内容一致超分辨率（CCSR），可以显著降低基于扩散先验的SR的随机性，提高SR输出的内容一致性，加快图像生成过程。代码和型号可在{https://github.com/csslc/CCSR}. et.al.|[2401.00877](http://arxiv.org/abs/2401.00877)|**[link](https://github.com/csslc/ccsr)**|
|**2024-01-01**|**New Job, New Gender? Measuring the Social Bias in Image Generation Models**|图像生成模型可以根据给定的文本生成或编辑图像。图像生成技术的最新进展，以DALL-E和Midtravel为例，具有开创性意义。尽管这些先进的模型具有令人印象深刻的功能，但它们往往是在大规模的互联网数据集上进行训练的，这使得它们很容易生成使社会刻板印象和偏见永久化的内容，从而导致严重后果。先前关于评估图像生成模型中的偏差的研究存在几个缺点，包括准确性有限、依赖大量人力以及缺乏全面分析。在本文中，我们提出了BiasPaint，这是一种新的变形测试框架，可以准确、自动、全面地触发图像生成模型中的社会偏见。BiasPaint使用各种各样的个人种子图像，并提示图像生成模型使用性别、种族和年龄中立的查询来编辑这些图像。这些查询涉及62种职业、39项活动、57种对象类型和70种人格特征。然后，该框架将编辑后的图像与原始种子图像进行比较，重点关注与性别、种族和年龄相关的任何变化。BiasPaint采用了一种测试预言，即当受到中性提示时，不应修改这些特性。基于这种设计，BiasPaint可以触发社会偏见，并评估图像生成模型的公平性。为了评估BiasPaint的有效性，我们使用BiasPaint测试了五种广泛使用的商业图像生成软件和模型，如稳定扩散和Midtravel。实验结果表明，在图像生成模型中，100%生成的测试用例可以成功触发社会偏见。 et.al.|[2401.00763](http://arxiv.org/abs/2401.00763)|null|
|**2024-01-01**|**DiffMorph: Text-less Image Morphing with Diffusion Models**|基于文本的图像生成模型是人工智能图像合成的普遍使用，但在艺术家的指导下直观地控制输出仍然具有挑战性。当前的方法需要多个图像和每个对象的文本提示，以将它们指定为概念来生成单个自定义图像。另一方面，我们的工作\verb|DiffMorph|引入了一种新颖的方法，该方法在不使用文本提示的情况下合成混合概念的图像。我们的工作集成了草图到图像模块，将用户草图作为输入。\verb|DiffMorph|通过条件化艺术家绘制的草图来获取初始图像，以生成变形图像。我们使用预先训练的文本到图像扩散模型，并对其进行微调，以忠实地重建每个图像。我们将草图中的图像和概念无缝地融合到一个连贯的构图中。通过我们的结果以及与基于提示的图像生成的比较，证明了我们工作的图像生成能力。 et.al.|[2401.00739](http://arxiv.org/abs/2401.00739)|null|
|**2023-12-31**|**Generative Model-Driven Synthetic Training Image Generation: An Approach to Cognition in Rail Defect Detection**|随着深度学习技术的集成，认知计算的最新进展促进了智能认知系统（ICS）的发展。这在轨道缺陷检测的背景下尤其有益，其中ICS将模拟缺陷模式的图像数据的类人分析。尽管卷积神经网络（CNN）在视觉缺陷分类方面取得了成功，但由于很少发生会导致缺陷零件和图像的事故事件，用于轨道缺陷检测的大型数据集的稀缺性仍然是一个挑战。当代研究人员通过探索基于规则和生成的数据增强模型，解决了这一数据稀缺的挑战。其中，变分自动编码器（VAE）模型可以生成真实的数据，而无需用于噪声建模的大量基线数据集。本研究提出了一种基于VAE的轨道缺陷合成图像生成技术，该技术结合了权重衰减正则化和图像重建损失，以防止过拟合。所提出的方法被应用于创建加拿大太平洋铁路（CPR）的合成数据集，该数据集仅包含五个类别的50个真实样本。值得注意的是，生成了500个合成样本，重建损失最小为0.021。使用该合成CPR数据集对视觉转换器（ViT）模型进行了微调，在对五个缺陷类别进行分类时实现了高准确率（98%-99%）。这项研究为铁路缺陷检测中的数据稀缺挑战提供了一个有前景的解决方案，展示了该领域强大的ICS开发潜力。 et.al.|[2401.00393](http://arxiv.org/abs/2401.00393)|null|
|**2023-12-30**|**GAN-GA: A Generative Model based on Genetic Algorithm for Medical Image Generation**|医学影像学是诊断和治疗疾病的重要工具。然而，缺乏医学图像可能导致诊断不准确和治疗无效。生成模型为解决医学图像短缺问题提供了一个很有前途的解决方案，因为它们能够从现有数据集生成新数据并检测这些数据中的异常。使用缩放、裁剪、翻转、填充、旋转和平移等位置增强方法进行数据增强，可能会在数据较少的领域（如医学图像数据）中导致更多的过拟合。本文提出了通过嵌入遗传算法优化的生成模型GAN-GA。所提出的模型提高了图像的保真度和多样性，同时保留了独特的特征。所提出的医学图像合成方法提高了医学图像的质量和保真度，这是图像解释的一个重要方面。为了评估合成图像：使用Frechet Inception Distance（FID）。通过生成急性淋巴细胞白血病（ALL）医学图像（图像数据集）来测试所提出的GAN-GA模型，这是首次用于生成模型。我们的结果与作为基线模型的InfoGAN的结果进行了比较。实验结果表明，所提出的优化GAN-GA将FID分数提高了约6.8%，尤其是在早期训练时期。源代码和数据集将在以下位置提供：https://github.com/Mustafa-AbdulRazek/InfoGAN-GA. et.al.|[2401.00314](http://arxiv.org/abs/2401.00314)|null|
|**2023-12-30**|**CamPro: Camera-based Anti-Facial Recognition**|从数百万台相机拍摄的图像的激增和面部识别（FR）技术的进步使FR的滥用成为严重的隐私威胁。现有的工作通常依靠模糊处理、合成或对抗性示例来修改图像中的人脸，以实现反人脸识别（AFR）。然而，包含敏感个人身份信息（PII）的相机模块捕获的未经修改的图像仍可能被泄露。在本文中，我们提出了一种新的方法，CamPro，来捕捉天生的AFR图像。CamPro使包装良好的商品相机模块能够生成包含很少PII但仍包含足够信息的图像，以支持其他非敏感视觉应用，如人脸检测。具体而言，CamPro调整相机图像信号处理器（ISP）内部的配置设置，即颜色校正矩阵和伽马校正，以实现AFR，并设计图像增强器，为可能的人类观众保持图像质量。我们在概念验证相机上实现并验证了CamPro，我们的实验在十个最先进的黑匣子FR模型上证明了它的有效性。结果表明，CamPro图像可以将人脸识别精度显著降低到0.3%，而对目标非敏感视觉应用的影响很小。此外，我们发现CamPro对使用CamPro生成的图像重新训练其FR模型的自适应攻击者具有弹性，即使他们完全了解保护隐私的ISP参数。 et.al.|[2401.00151](http://arxiv.org/abs/2401.00151)|**[link](https://github.com/forget2save/campro)**|

## avatar

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-01**|**Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade**|我们介绍了一个图形感知的自动编码器集成框架，以及相关的形式主义和工具，旨在促进人文学科学术的深度学习。通过组成子体系结构以生成同构于人文领域的模型，我们保持了可解释性，同时为每个子体系结构选择提供功能签名，允许传统和计算研究人员在不干扰既定实践的情况下进行合作。我们展示了我们的方法在美国后大西洋奴隶贸易历史研究中的实际应用，并做出了一些具体的技术贡献：一种新的混合图卷积自动编码器机制，用于常见图拓扑的批处理策略，以及用于特定用例的掩蔽技术。越来越多的20多项研究证明了该框架扩大不同领域参与的有效性，这些研究既有与人文主义者的合作，也有机器学习文献中的既定任务，涵盖了各种领域和数据模式。我们对几种不同的体系结构选择进行了性能比较，最后列出了这项研究即将采取的下一步行动。 et.al.|[2401.00824](http://arxiv.org/abs/2401.00824)|null|
|**2023-12-23**|**Human101: Training 100+FPS Human Gaussians in 100s from 1 View**|从单视角视频中重构人体在虚拟现实领域发挥着关键作用。一种流行的应用场景需要快速重建高保真3D数字人，同时确保实时渲染和交互。现有的方法往往难以满足这两个要求。在本文中，我们介绍了Human101，这是一种新颖的框架，通过在100秒内训练3D高斯人并以100+FPS进行渲染，能够从单视图视频中生成高保真动态3D人体重建。我们的方法利用了3D高斯飞溅的优势，它提供了3D人类的明确而有效的表示。与之前基于NeRF的管道不同，Human101巧妙地应用了以人为中心的前向高斯动画方法来变形3D高斯的参数，从而提高了渲染速度（即，以令人印象深刻的60+FPS渲染1024个分辨率的图像，以100+FPS渲染512个分辨率的图片）。实验结果表明，我们的方法大大超过了当前的方法，每秒帧数激增了10倍，并提供了相当或卓越的渲染质量。代码和演示将在上发布https://github.com/longxiang-ai/Human101. et.al.|[2312.15258](http://arxiv.org/abs/2312.15258)|**[link](https://github.com/longxiang-ai/human101)**|
|**2023-12-23**|**Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models**|扩散模型以文本提示为条件，生成具有复杂细节的逼真图像。但是，当涉及到手、牙齿等人类特征时，大多数预先训练的模型都无法生成准确的图像。我们假设，可以通过注释良好的高质量数据来克服扩散模型的这种无能。在本文中，我们专门研究了使用扩散模型改进手-物体交互图像的生成。我们收集了一个注释良好的手-物交互合成数据集，该数据集使用Prompt Propose-Verify框架进行策划，并在其上微调稳定的扩散模型。我们根据CLIPScore、ImageReward、Fedility和alignment等定性和定量指标对图像-文本数据集进行评估，并显示出比当前最先进的基准测试更好的性能。 et.al.|[2312.15247](http://arxiv.org/abs/2312.15247)|null|
|**2023-12-23**|**TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**|直接语音到语音翻译通过引入自监督学习中获得的离散单元来实现高质量的结果。这种方法避免了与模型级联相关的延迟和级联错误。然而，与音频语音相比，有声头翻译，即将视听语音（即有声头视频）从一种语言转换为另一种语言，仍然面临着几个挑战：（1）现有方法总是依赖于级联，通过音频和文本进行合成，导致延迟和级联错误。（2） 会说话的头翻译有一套有限的参考框架。如果生成的翻译超过了原始语音的长度，则需要通过重复帧来补充视频序列，从而导致不和谐的视频转换。在这项工作中，我们提出了一个有声头翻译模型\textbf{TransFace}，它可以直接将视听语音翻译成其他语言的视听语音。它由一个语音到单元的翻译模型和一个基于单元的视听语音合成器Unit2Lip组成，前者将音频语音转换为离散单元，后者并行地从离散单元重新合成同步的视听语音。此外，我们还引入了一个有界持续时间预测器，确保等轴测头的平移并防止重复的参考帧。实验表明，我们提出的Unit2Lip模型显著提高了同步性（原始和生成的音频语音在LSE-C上分别为1.601和0.982），并将LRS2上的推理速度提高了4.35倍。此外，TransFace在LRS3-T和100%等时翻译上的Es-En和Fr-En的BLEU得分分别为61.93和47.55。 et.al.|[2312.15197](http://arxiv.org/abs/2312.15197)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|
|**2023-12-22**|**Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images**|随着元宇宙慢慢成为现实，以及数字人类的快速发展，对人脸原则风格编辑管道的需求势必会增加。我们通过引入潜在2语义自动编码器（L2SAE）来满足这一需求，这是一种生成式自动编码器模型，有助于对人脸图像中几个感兴趣区域（ROI）的风格属性进行高度本地化编辑。L2SAE学习编码图像的结构和风格信息的单独潜在表示。因此，允许对所选择的ROI进行保留结构的样式编辑。编码的结构表示是具有减少的空间维度的多通道2D张量，其捕获局部和全局结构属性。样式表示是捕获全局样式属性的1D张量。在我们的框架中，我们对结构表示进行切片，以建立与不同ROI的强且不纠缠的对应关系。因此，所选ROI的样式编辑相当于（a）从切片结构表示生成的ROI掩模和（b）具有全局样式变化的解码图像的简单组合，该全局样式变化是从操纵的（使用高斯噪声）全局样式和不变的结构张量生成的。在没有额外人力监督的情况下进行风格编辑是对SOTA风格编辑管道的重大胜利，因为大多数现有作品都需要额外的人力（监督）后期培训，才能将语义归因于风格编辑。我们还取消了基于迭代优化的反演或在训练后确定可控的潜在方向，这需要额外的计算成本高昂的操作。我们在多个应用程序中为相同的应用程序提供定性和定量结果，例如使用从多个数据集采样的测试图像进行选择性风格编辑和交换。 et.al.|[2312.15037](http://arxiv.org/abs/2312.15037)|null|
|**2023-12-21**|**From Past to Future: Digital Methods Towards Artefact Analysis**|在过去的二十年里，数字人文改变了人文和社会科学的格局，实现了对广泛数据集的高级计算分析和解释。值得注意的是，东南亚，特别是新加坡最近的举措侧重于对历史数据进行分类和归档，如艺术品、文学作品，尤其是考古文物。这项研究通过在两个不同的人工制品数据集上应用统计方法，展示了数字人文的巨大潜力。具体而言，我们展示了对公元1千年中期东南亚大陆“旭日”铸币的自动模具研究结果，随后对从新加坡中部殖民前圣安德鲁大教堂遗址挖掘的13-14世纪陶器的2D图像使用了无监督统计方法。这项研究提供了一个比较评估，展示了基于统计的方法对不同考古材料的解释和分析以及数字人文整体的变革影响。 et.al.|[2312.13790](http://arxiv.org/abs/2312.13790)|null|
|**2023-12-18**|**Relightable Neural Actor with Intrinsic Decomposition and Pose Control**|在视觉和图形学中，创建一个可欣赏、可驾驶和逼真的数字人类化身是一个具有挑战性的重要问题。人类是高度立体化的，会产生依赖姿势的外观效果，如自我阴影和皱纹，皮肤和衣服需要复杂且空间变化的BRDF模型。虽然最近的人类重新照明方法可以从多视图视频中恢复看似合理的材料光分解，但它们不能推广到新颖的姿势，并且仍然存在视觉伪影。为了解决这一问题，我们提出了Relightable Neural Actor，这是第一种基于视频的方法，用于学习照片真实感的神经人体模型，该模型可以重新照明，允许外观编辑，并可以由任意骨骼姿势控制。重要的是，为了学习我们的人类化身，我们只需要在已知但静态的照明条件下对人类进行多视图记录。为了实现这一点，我们用可驱动的密度场来表示演员的几何体，该密度场对姿势相关的服装变形进行建模，并提供3D和UV空间之间的映射，其中对法线、可见性和材质进行编码。为了在现实世界场景中评估我们的方法，我们收集了一个新的数据集，其中包括在室内和室外不同光照条件下记录的四个参与者，为人类重新照明提供了第一个此类基准，并展示了最先进的新人类姿势的重新照明结果。 et.al.|[2312.11587](http://arxiv.org/abs/2312.11587)|null|
|**2023-12-18**|**VectorTalker: SVG Talking Face Generation with Progressive Vectorisation**|高保真度和高效的音频驱动谈话头生成一直是计算机图形学和计算机视觉领域的一个关键研究课题。在这项工作中，我们研究了基于矢量图像的音频驱动的谈话头生成。与现有作品中使用最广泛的光栅图像直接动画相比，矢量图像具有良好的可扩展性，可用于多种应用。基于矢量图像的会说话的头生成面临两个主要挑战：高质量的矢量图像重建（相对于源肖像图像）和生动的动画（相对于音频信号）。为了解决这些问题，我们提出了一种新的可扩展矢量图形重建和动画方法，称为VectorTalker。具体而言，对于高保真度重建，VectorTalker以从粗到细的方式分层重建矢量图像。对于生动的音频驱动的面部动画，我们建议使用面部标志作为中间运动表示，并提出一个有效的标志驱动的矢量图像变形模块。我们的方法可以在一个统一的框架内处理各种风格的肖像图像，包括日本漫画、卡通和照片真实感图像。我们进行了广泛的定量和定性评估，实验结果证明了VectorTalker在矢量图形重建和音频驱动动画方面的优势。 et.al.|[2312.11568](http://arxiv.org/abs/2312.11568)|null|
|**2023-12-18**|**AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis**|音频驱动的会说话的头部合成是一个很有前途的课题，在数字人、电影制作和虚拟现实等领域有着广泛的应用。与之前的研究相比，最近基于NeRF的方法在质量和保真度方面显示出优势。然而，当涉及到少镜头会说话的头部生成时，在一个身份只有几秒钟会说话的视频的实际场景中，出现了两个限制：1）它们要么没有基本模型，作为快速收敛的面部先验，要么在构建先验时忽略了音频的重要性；2） 它们大多忽略了不同面部区域与音频之间的相关性，例如，嘴与音频相关，而耳朵与音频无关。在本文中，我们提出了音频增强神经辐射场（AE NeRF）来解决上述问题，它可以用最少的镜头数据集生成新扬声器的逼真肖像。具体来说，我们在参考方案的特征融合阶段引入了音频感知聚合模块，其中权重由参考图像和目标图像之间音频的相似性决定。然后，提出了一种基于双NeRF框架的音频对齐人脸生成策略，分别对音频相关区域和音频无关区域进行建模。大量实验表明，即使在有限的训练集或训练迭代中，AE NeRF在图像保真度、音频嘴唇同步和泛化能力方面也超过了最先进的技术。 et.al.|[2312.10921](http://arxiv.org/abs/2312.10921)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

