---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.30
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-27**|**Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting**|我们展示了将基于物理的固体和流体动画与3D高斯飞溅（3DGS）相结合的可行性，以在使用3DGS重建的虚拟场景中创建新的效果。利用底层表示中高斯飞溅和基于位置的动力学（PBD）的一致性，我们以内聚的方式管理渲染、视图合成以及固体和流体的动力学。与高斯着色器类似，我们使用添加的法线增强每个高斯内核，将内核的方向与曲面法线对齐，以优化PBD模拟。这种方法有效地消除了由固体中的旋转变形引起的尖锐噪声。它还允许我们集成基于物理的渲染，以增强流体上的动态曲面反射。因此，我们的框架能够真实地再现动态流体上的曲面高光，并促进场景对象和新视图中流体之间的交互。有关更多信息，请访问我们的项目页面\url{https://amysteriouscat.github.io/GaussianSplashing/}. et.al.|[2401.15318](http://arxiv.org/abs/2401.15318)|null|
|**2024-01-26**|**3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field**|同时实现室内环境的3D重建和新视图合成具有广泛的应用，但在技术上非常具有挑战性。基于隐式神经函数的现有技术方法可以获得极好的三维重建结果，但它们在新视图合成方面的性能可能不令人满意。神经辐射场（NeRF）的令人兴奋的发展彻底改变了新的视图合成，然而，基于NeRF的模型可能无法重建干净的几何表面。我们开发了一种双神经辐射场（Du-NeRF），以同时实现高质量的几何重建和视图渲染。Du-NeRF包含两个几何场，一个从SDF场导出以便于几何重建，另一个从密度场导出以促进新视图合成。Du NeRF的一个创新特征是，它将视图无关分量与密度场解耦，并将其用作标签来监督SDF场的学习过程。这减少了形状辐射模糊性，并使几何图形和颜色在学习过程中相互受益。大量实验表明，Du-NeRF可以显著提高室内环境下新视图合成和3D重建的性能，并且在构建包含不服从多视图颜色一致性的精细几何图形的区域时尤其有效。 et.al.|[2401.14726](http://arxiv.org/abs/2401.14726)|null|
|**2024-01-23**|**IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images**|尽管许多3D重建和新颖的视图合成方法允许从消费者相机轻松捕捉的多视图图像中真实地渲染场景，但它们在表示中烘焙照明，无法支持高级应用程序，如材质编辑、重新照明和虚拟对象插入。通过反向渲染重建基于物理的材料特性和照明有望实现此类应用。然而，大多数反向渲染技术都需要高动态范围（HDR）图像作为输入，这是大多数用户无法访问的设置。我们提出了一种方法，从多视图、低动态范围（LDR）图像中恢复场景的基于物理的材料特性和空间变化的HDR照明。我们在反向渲染管道中对LDR图像形成过程进行建模，并提出了一种新的材料、照明和相机响应模型的优化策略。与采用LDR或HDR输入的最先进的反向渲染方法相比，我们使用合成场景和真实场景来评估我们的方法。我们的方法优于以LDR图像作为输入的现有方法，并允许高度逼真的重新照明和对象插入。 et.al.|[2401.12977](http://arxiv.org/abs/2401.12977)|null|
|**2024-01-24**|**RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos**|我们介绍了一种新的在野外捕获的RGB-D对象数据集，称为WildRGB-D。与大多数现有的仅带有RGB捕获的以对象为中心的真实世界数据集不同，深度通道的直接捕获允许更好的3D注释和更广泛的下游应用。WildRGB-D包括大型类别级RGB-D对象视频，这些视频是用iPhone 360度环绕对象拍摄的。它包含大约8500个记录对象和近20000个RGB-D视频，涉及46个常见对象类别。这些视频是在三种设置的不同杂乱背景下拍摄的，以覆盖尽可能多的真实世界场景：（i）一个视频中的单个对象；（ii）一个视频中的多个对象；以及（iii）在一个视频中具有静止手的对象。该数据集由对象遮罩、真实世界比例的相机姿态和RGBD视频中重建的聚合点云进行注释。我们用WildRGB-D对四个任务进行了基准测试，包括新颖的视图合成、相机姿态估计、物体6d姿态估计和物体表面重建。我们的实验表明，RGB-D对象的大规模捕获为推进3D对象学习提供了巨大的潜力。我们的项目页面是https://wildrgbd.github.io/. et.al.|[2401.12592](http://arxiv.org/abs/2401.12592)|null|
|**2024-01-23**|**Methods and strategies for improving the novel view synthesis quality of neural radiation field**|神经辐射场（NeRF）技术可以从2D图像中学习场景的3D隐式模型，并合成逼真的新视图图像。该技术得到了业界的广泛关注，具有良好的应用前景。针对NeRF图像渲染质量需要提高的问题，近三年来，许多研究人员提出了各种方法来提高渲染质量。对最新的相关论文进行了分类和综述，分析了质量改进背后的技术原理，并讨论了质量改进方法的未来发展方向。这项研究可以帮助研究人员快速了解该领域技术的现状和发展脉络，有助于激发更高效算法的发展，促进NeRF技术在相关领域的应用。 et.al.|[2401.12451](http://arxiv.org/abs/2401.12451)|null|
|**2024-01-22**|**HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs**|神经辐射场（NeRF）作为一种通过从离散观测中学习场景表示的新型视图合成范式，已经引起了人们的极大关注。然而，当面对稀疏视图输入时，NeRF表现出明显的性能退化，从而限制了其进一步的适用性。在这项工作中，我们介绍了层次几何、语义和光度引导的NeRF（HG3-NeRF），这是一种新的方法，可以解决上述限制，并增强不同视图中几何、语义内容和外观的一致性。我们提出了分层几何制导（HGG），将运动结构的附加（SfM），即稀疏深度先验，纳入场景表示中。与直接深度监督不同，HGG从局部几何区域到全局几何区域对体积点进行采样，减轻了深度先验中固有偏差引起的偏差。此外，我们从不同分辨率的图像中观察到的语义一致性的显著变化中获得了灵感，并提出了层次语义引导（HSG）来学习粗到细的语义内容，该内容对应于粗到细场景表示。实验结果表明，HG3-NeRF可以在不同的标准基准上优于其他最先进的方法，并实现稀疏视图输入的高保真度合成结果。 et.al.|[2401.11711](http://arxiv.org/abs/2401.11711)|null|
|**2024-01-18**|**Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions**|隐式神经表示（INRs）的许多变体，其中神经网络被训练为信号的连续表示，对于下游任务具有巨大的实用性，包括新颖的视图合成、视频压缩和图像超分辨率。不幸的是，对这些网络的内部运作方式的研究严重不足。我们的工作，即解释隐式神经画布（XINC），是一个统一的框架，用于通过检查每个神经元对每个输出像素的贡献强度来解释INRs的特性。我们将这些贡献图的集合称为隐式神经画布，并使用这一概念来证明我们研究的INR学会了以令人惊讶的方式“观察”它们所代表的帧。例如，INR往往具有高度分布的表示。虽然缺乏高级对象语义，但它们对颜色和边缘有很大的偏见，而且几乎完全是空间不可知的。我们通过研究视频INR中对象在时间上的表现方式得出了我们的结论，使用聚类来可视化跨层和架构的相似神经元，并表明这是由运动主导的。这些见解证明了我们的分析框架的普遍有用性。我们的项目页面位于https://namithap10.github.io/xinc. et.al.|[2401.10217](http://arxiv.org/abs/2401.10217)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Objects With Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting**|从照片中重建对象并将其虚拟地放置在新环境中超出了标准的新颖视图合成任务，因为对象的外观不仅要适应新颖的视点，还要适应新的照明条件，而且反向渲染方法的评估依赖于新颖的视图合成数据或用于定量分析的简单合成数据集。这项工作提供了一个真实世界的数据集，用于测量重新照明对象的重建和渲染。为此，我们捕获了多个环境中相同对象的环境照明和地面实况图像，从而可以从一个环境中拍摄的图像中重建对象，并量化看不见的照明环境的渲染视图的质量。此外，我们介绍了一个由现成方法组成的简单基线，并在重新照明任务中测试了几种最先进的方法，表明新的视图合成不是衡量性能的可靠指标。代码和数据集可在https://github.com/isl-org/objects-with-lighting . et.al.|[2401.09126](http://arxiv.org/abs/2401.09126)|**[link](https://github.com/isl-org/objects-with-lighting)**|
|**2024-01-17**|**ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization**|在给定一组2D图像的情况下，神经辐射场（NeRF）在新视图合成（NVS）中表现出显著的性能。然而，NeRF训练需要每个输入视图的精确相机姿势，通常通过运动结构（SfM）管道获得。最近的作品试图放松这种限制，但它们仍然经常依赖于可以改进的体面的初始姿势。在这里，我们旨在消除姿势初始化的要求。我们提出了增量置信（ICON），这是一种从2D视频帧中训练NeRF的优化过程。ICON仅假设相机运动平滑，以估计姿势的初始猜测。此外，ICON引入了“置信度”：一种用于动态重加权梯度的模型质量自适应度量。ICON依赖于高置信度姿势来学习NeRF，并依赖于高置信度3D结构（由NeRF编码）来学习姿势。我们表明，与使用SfM姿势的方法相比，ICON在没有预先初始化姿势的情况下，在CO3D和HO3D中都实现了卓越的性能。 et.al.|[2401.08937](http://arxiv.org/abs/2401.08937)|null|

## MLLM

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**|我们介绍了InternetLM-XComposer2，这是一种在自由形式文本图像合成和理解方面表现出色的尖端视觉语言模型。该模型超越了传统的视觉语言理解，从轮廓、详细的文本规范和参考图像等不同输入中熟练地制作交错的文本图像内容，实现了高度可定制的内容创建。InternLM-XComposer2提出了一种部分LoRA（PLoRA）方法，该方法将额外的LoRA参数专门应用于图像标记，以保持预先训练的语言知识的完整性，在精确的视觉理解和具有文学天赋的文本合成之间取得平衡。实验结果表明，基于InternetLM2-7B的InternetLM-XComposer2在生成高质量的长文本多模式内容方面具有优势，并且在各种基准测试中具有卓越的视觉语言理解性能，不仅显著优于现有的多模式模型，而且在某些评估中与GPT-4V和Gemini Pro相匹配甚至超过。这突出了它在多模式理解领域的非凡熟练程度。具有7B参数的InternetLM-XComposer2型号系列可在https://github.com/InternLM/InternLM-XComposer. et.al.|[2401.16420](http://arxiv.org/abs/2401.16420)|null|
|**2024-01-29**|**CO2: Efficient Distributed Training with Full Communication-Computation Overlap**|大型语言模型的根本成功取决于大规模分布式训练技术的有效实现。尽管如此，构建一个具有高速通信互连性的庞大、高性能集群的成本高得令人望而却步，而且只有知名实体才能访问。在这项工作中，我们的目标是降低这一障碍，并在带宽有限的集群中实现大规模训练的民主化。我们提出了一种称为CO2的新方法，该方法将本地更新和异步通信引入分布式数据并行训练，从而促进了CO2通信与计算的完全重叠。即使在受非常有限的通信带宽限制的广泛的多节点集群上，CO2也能够获得高可扩展性。我们进一步提出了过时间隙惩罚和外部动量修剪技术以及CO2，以增强其收敛性和训练稳定性。此外，CO2与成熟的ZeRO系列优化器无缝集成，通过大型模型训练减少模型状态的内存消耗。我们还提供了收敛性的数学证明，并建立了严格的上界。此外，我们通过一系列广泛的实践实验验证了我们的发现，这些实验涵盖了计算机视觉和自然语言处理领域的广泛任务。这些实验用于证明当在包括多达128个A100 GPU的配置中部署时，CO2在收敛性、通用性和可扩展性方面的能力。无论是在具有800Gbps RDMA或80Gbps TCP/IP节点间连接的集群上，结果都强调了CO2的卓越容量，可以极大地提高可扩展性。 et.al.|[2401.16265](http://arxiv.org/abs/2401.16265)|null|
|**2024-01-27**|**A Survey on Data Augmentation in Large Model Era**|包括大型语言和扩散模型在内的大型模型在接近人类水平的智力方面表现出了非凡的前景，引起了学术界和工业界的极大兴趣。然而，这些大型模型的训练需要大量高质量的数据，随着这些模型的不断更新，现有的高质量数据库可能很快就会耗尽。这一挑战推动了对数据增强方法的研究激增。利用大型模型，这些数据增强技术的性能优于传统方法。本文从全面的角度对大型模型驱动的数据扩充方法进行了详尽的综述。我们首先将相关研究分类为三大类：图像增强、文本增强和配对数据增强。接下来，我们将深入研究与基于模型的大型数据扩充相关的各种数据后处理技术。然后，我们的讨论扩展到包括自然语言处理、计算机视觉和音频信号处理中这些数据增强方法的一系列应用。我们继续评估基于模型的大型数据增强在不同场景中的成功和局限性。在结束我们的审查时，我们强调了数据增强领域未来探索的潜在挑战和途径。我们的目标是为研究人员提供关键见解，最终为更复杂的大型模型的发展做出贡献。我们一贯将相关开源材料保存在：https://github.com/MLGroup-JLU/LLM-data-aug-survey. et.al.|[2401.15422](http://arxiv.org/abs/2401.15422)|null|
|**2024-01-26**|**F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods**|大型语言模型（LLM）以其前所未有的性能而备受关注，导致越来越多的研究评估LLM。然而，这些评估基准仅限于评估遵循指令的能力，而忽略了培训前阶段出现的基本能力。以往的主观评价方法主要是根据API模型进行评分。然而，在缺乏参考文献的情况下，大型模型辨别细微差异的能力有限。为了弥补这一差距，我们提出了F-Eval，这是一个双语评估基准，用于评估基本能力，包括表达能力、常识能力和逻辑能力。F-Eval中的任务包括多选客观任务、开放式客观任务、基于参考的主观任务和无参考的主观工作。对于无参考的主观任务，我们设计了新的评估方法，作为API模型评分的替代方案。我们对13个高级LLM进行了评估。结果表明，与其他评估者相比，我们的评估方法显示出更高的相关系数和更大的差异。此外，我们还讨论了不同模型大小、维度和归一化方法的影响。我们预计F-Eval将促进LLM基本能力的研究。 et.al.|[2401.14869](http://arxiv.org/abs/2401.14869)|null|
|**2024-01-26**|**Leveraging Large Models for Crafting Narrative Visualization: A Survey**|叙事可视化有效地将数据转化为引人入胜的故事，使复杂的信息可供广大受众访问。大型模型对叙事可视化至关重要，通过其处理自然语言查询和答案、生成连贯叙事和增强视觉交流的卓越能力，从本质上促进了这一过程。受先前叙事可视化工作和大型模型最新进展的启发，我们综合了大型模型在叙事可视化各个阶段的潜在任务和机会。在我们的研究中，我们调查了79篇论文，探讨了大型模型在叙事可视化自动化创建中的作用。我们提出了一个综合的管道，利用大型模型来制作叙事可视化，将回顾的文献分为四个基本阶段：数据、叙事、可视化和呈现。此外，我们确定了九个特定任务，其中大型模型应用于这些阶段。这项研究描绘了LM4NV过程中的挑战和机遇，为未来的研究提供了深刻的方向，并为该领域的学者提供了宝贵的指导。 et.al.|[2401.14010](http://arxiv.org/abs/2401.14010)|null|
|**2024-01-23**|**Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study**|在大模型时代，解码的自回归性质往往导致延迟成为一个重要的瓶颈。我们提出了一种非自回归LM融合ASR系统，该系统有效地利用了加速器硬件的并行化能力。我们的方法在每段评分模式中结合了通用语音模型（USM）和PaLM 2语言模型，在FLEURS和YouTube字幕上，所有语言的平均相对WER提高了10.8%和3.6%。此外，我们的综合消融研究分析了LLM大小、上下文长度、词汇大小、融合方法等关键参数。例如，我们探讨了LLM大小在128M到340B参数范围内对ASR性能的影响。这项研究为影响实际大规模LM融合语音识别系统有效性的因素提供了有价值的见解。 et.al.|[2401.12789](http://arxiv.org/abs/2401.12789)|null|
|**2024-01-23**|**Full-Stack Optimization for CAM-Only DNN Inference**|在过去的几年里，神经网络的准确性在各个领域都有了很大的提高。然而，它们不断增加的复杂性导致了冯·诺依曼系统中令人望而却步的高能量需求和延迟。最近已经提出了几种内存计算（CIM）系统来克服这一问题，但涉及大型模型的准确性、硬件可靠性和可扩展性的权衡仍然是一个挑战。此外，对于一些CIM设计，激活运动仍然需要相当大的时间和能量。本文探讨了三元权重神经网络和使用跑道存储器（RTM）实现的关联处理器（AP）的算法优化的组合。我们提出了一种新的编译流程，通过降低AP的算术强度来优化AP上的卷积。通过利用基于RTM的AP的优势，这种方法大大减少了存储器内的数据传输，同时解决了准确性、能效和可靠性问题。具体而言，与内存加速器中的纵横制相比，我们的解决方案将ImageNet上的ResNet-18推理的能效提高了7.5倍，同时保持了软件的准确性。 et.al.|[2401.12630](http://arxiv.org/abs/2401.12630)|null|
|**2024-01-22**|**A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network**|多尺度卷积神经网络（CNN）在解决各种视觉问题方面表现出了非凡的能力。然而，融合不同尺度的特征总是导致模型尺寸大，阻碍了多尺度细胞神经网络在RGB-D显著性检测中的应用。在本文中，我们提出了一种定制的特征融合模块，称为显著性增强特征融合（SEFF），用于RGB-D显著性检测。SEFF利用相邻尺度的显著性图来增强融合所需的特征，从而产生更具代表性的融合特征。我们的多尺度RGB-D显著性检测器使用SEFF并处理具有三个不同尺度的图像。SEFF用于融合RGB和深度图像的特征，以及不同尺度的解码器的特征。在五个基准数据集上的大量实验已经证明了我们的方法优于十个SOTA显著性检测器。 et.al.|[2401.11914](http://arxiv.org/abs/2401.11914)|null|
|**2024-01-22**|**SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese**|我们介绍了SuperCLUE-Math6（SC-Math6），这是一个新的基准数据集，用于评估汉语模型的数学推理能力。SC-Math6是GSM8K数据集的升级中文版，具有更强的难度、多样性和应用范围。它由2000多个数学单词问题组成，需要多步骤推理并提供自然语言解决方案。我们提出了一种创新方案，根据不同推理步骤问题的性能来量化大型模型的推理能力。在12个具有代表性的中国模型上进行的实验表明，推理水平分层清晰，GPT-4等顶级模型表现出优异的性能。SC-Math6填补了中国数学推理基准的空白，为提高汉语模型的智能性提供了一个全面的测试平台。 et.al.|[2401.11819](http://arxiv.org/abs/2401.11819)|null|
|**2024-01-22**|**P2DT: Mitigating Forgetting in task-incremental Learning with progressive prompt Decision Transformer**|灾难性遗忘对管理由大型模型控制的智能代理提出了重大挑战，当这些代理面临新任务时，会导致性能下降。在我们的工作中，我们提出了一种新的解决方案——渐进式即时决策转换器（P2DT）。该方法通过在新任务训练期间动态附加决策令牌来增强基于转换器的模型，从而促进特定于任务的策略。我们的方法可以缓解持续和离线强化学习场景中的遗忘。此外，P2DT利用了通过传统强化学习从所有任务中收集的轨迹，并在训练过程中生成新的任务特定令牌，从而保留了先前研究的知识。初步结果表明，我们的模型有效地缓解了灾难性遗忘，并能很好地适应不断增加的任务环境。 et.al.|[2401.11666](http://arxiv.org/abs/2401.11666)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**Spatial-Aware Latent Initialization for Controllable Image Generation**|最近，文本到图像的扩散模型已经证明了以文本输入为条件生成高质量图像的令人印象深刻的能力。然而，这些模型很难准确地遵循关于空间布局信息的文本指令。虽然之前的研究主要集中在将交叉注意力图与布局条件对齐，但它们忽略了初始化噪声对布局指导的影响。为了实现更好的布局控制，我们建议在去噪过程中利用空间感知的初始化噪声。具体而言，我们发现具有有限反转步骤的反转参考图像包含关于对象位置的宝贵空间意识，从而在生成的图像中产生类似的布局。基于这一观察结果，我们开发了一个开放的词汇框架，为每个布局条件定制空间感知的初始化噪声。在不修改除初始化噪声之外的其他模块的情况下，我们的方法可以作为即插即用模块无缝集成到其他无训练布局指导框架中。我们在可用的稳定扩散模型和COCO数据集上对我们的方法进行了定量和定性评估。配备了空间感知的潜在初始化，我们的方法在保留高质量内容的同时显著提高了布局指导的有效性。 et.al.|[2401.16157](http://arxiv.org/abs/2401.16157)|null|
|**2024-01-29**|**Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You**|文本到图像生成模型最近在图像质量、灵活性和文本对齐方面取得了惊人的成果，因此被应用于数量快速增长的应用中。通过提高多语言能力，一个更大的社区现在可以使用这种技术。然而，正如我们将要展示的那样，多语言模型与单语模型同样存在（性别）偏见。此外，人们自然期望这些模型能在不同语言之间提供相似的结果，但事实并非如此，而且语言之间存在重要差异。因此，我们提出了一个新的基准MAGBIG，旨在促进无性别偏见的多语言模型研究。我们研究了多语言T2I模型是否利用MAGBIG放大了性别偏见。为此，我们使用多语言提示来请求某一职业或特征的人的肖像图像（使用形容词）。我们的研究结果表明，模型不仅偏离了每个性别产生的可能性应该相等的规范假设，而且不同语言之间也存在很大差异。此外，我们研究了即时工程策略，即使用间接、中性的配方，作为这些偏见的可能补救措施。不幸的是，它们只在有限的程度上起作用，并导致更差的文本到图像对齐。因此，这项工作需要对图像生成器中不同语言的不同表示进行更多的研究。 et.al.|[2401.16092](http://arxiv.org/abs/2401.16092)|null|
|**2024-01-29**|**Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling**|我们介绍了Motion-I2V，这是一种用于一致可控的图像到视频生成（I2V）的新框架。与之前直接学习复杂的图像到视频映射的方法不同，Motion-I2V通过显式运动建模将I2V分解为两个阶段。对于第一阶段，我们提出了一种基于扩散的运动场预测器，该预测器专注于推导参考图像像素的轨迹。对于第二阶段，我们提出了运动增强的时间注意力来增强视频潜在扩散模型中的有限一维时间注意力。该模块可以在第一阶段预测轨迹的引导下，有效地将参考图像的特征传播到合成帧。与现有方法相比，即使在存在大的运动和视点变化的情况下，Motion-I2V也可以生成更一致的视频。通过为第一阶段训练稀疏轨迹ControlNet，Motion-I2V可以支持用户使用稀疏轨迹和区域注释精确控制运动轨迹和运动区域。这提供了I2V过程的更多可控性，而不仅仅依赖于文本指令。此外，Motion-I2V的第二阶段自然支持零样本视频到视频的转换。定性和定量比较都证明了Motion-I2V在一致和可控的图像到视频生成方面优于现有方法。 et.al.|[2401.15977](http://arxiv.org/abs/2401.15977)|null|
|**2024-01-29**|**Diffusion Facial Forgery Detection**|检测扩散生成的图像最近已发展成为一个新兴的研究领域。现有的基于扩散的数据集主要集中于一般图像生成。然而，到目前为止，面部伪造的研究还较少，这会带来更严重的社会风险。为了解决这一差距，本文介绍了DiFF，这是一个专门用于人脸聚焦扩散生成图像的综合数据集。DiFF包括在四种条件下使用十三种不同的生成方法合成的500000多幅图像。特别是，该数据集利用了30000个精心收集的文本和视觉提示，确保了图像的合成具有高保真度和语义一致性。我们通过人体测试和几种有代表性的伪造检测方法在DiFF数据集上进行了广泛的实验。结果表明，人类观察者和自动检测器的二进制检测精度通常低于30%，这揭示了检测扩散生成的面部伪造品的挑战。此外，我们提出了一种边缘图正则化方法，以有效地增强现有检测器的泛化能力。 et.al.|[2401.15859](http://arxiv.org/abs/2401.15859)|null|
|**2024-01-29**|**2L3: Lifting Imperfect Generated 2D Images into Accurate 3D**|从单个图像重建3D对象是一个有趣但具有挑战性的问题。一个有前途的解决方案是利用多视图（MV）3D重建将生成的MV图像融合成一致的3D对象。然而，生成的图像通常存在照明不一致、几何体错位和视图稀疏的问题，导致重建质量较差。为了解决这些问题，我们提出了一种新的3D重建框架，该框架利用固有分解引导、瞬态单先验引导和视图增强来分别解决这三个问题。具体来说，我们首先利用阴影信息从生成的图像中解耦，以减少不一致照明的影响；然后，我们引入了具有视点相关瞬态编码的单声道先验来增强重构的法线；最后，我们设计了一种视图增强融合策略，最大限度地减少生成的稀疏视图中的像素级损失和增强的随机视图中的语义损失，从而获得视图一致的几何结构和详细的纹理。因此，我们的方法能够集成预先训练的MV图像生成器和基于神经网络的体积有符号距离函数（SDF）表示，用于单个图像到3D对象的重建。我们在各种数据集上评估了我们的框架，并证明了其在定量和定性评估中的卓越性能，这意味着3D对象重建方面取得了重大进展。与最新的同步梦想家方法相比，我们将倒角距离误差降低了约36%，PSNR提高了约30%。 et.al.|[2401.15841](http://arxiv.org/abs/2401.15841)|null|
|**2024-01-28**|**Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding**|随着大规模文本图像生成模型在文本图像生成领域取得了显著进展，人们提出了许多微调方法。然而，这些模型往往难以处理新颖的对象，尤其是一次性场景。我们提出的方法旨在以对象驱动的方式解决可推广性和保真度的挑战，仅使用单个输入图像和感兴趣的特定对象区域。为了提高可推广性并缓解过度拟合，在我们的范式中，在微调扩散模型之前，基于对象的外观及其类别初始化原型嵌入。在微调过程中，我们提出了一个类特征正则化，以保留对象类的先验知识。为了进一步提高保真度，我们引入了特定于对象的损失，它也可以用于植入多个对象。总体而言，我们提出的用于植入新对象的对象驱动方法可以与现有概念无缝集成，并且具有高保真度和通用性。我们的方法优于现有的几种方法。代码将被发布。 et.al.|[2401.15708](http://arxiv.org/abs/2401.15708)|null|
|**2024-01-28**|**Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation**|尽管在生成高质量图像的文本到图像模型方面取得了重大进展，但在复杂的文本提示背景下，这些方法仍然难以确保文本提示相对于图像的可控性，尤其是在保留对象属性和关系方面。在本文中，我们提出了CompAgent，这是一种无需训练的合成文本到图像生成方法，以大型语言模型（LLM）代理为核心。CompAgent的基本思想是以分而治之的方法论为前提的。给定包含多个概念（包括对象、属性和关系）的复杂文本提示，LLM代理首先对其进行分解，这需要提取单个对象及其相关属性，并预测连贯的场景布局。然后可以独立地征服这些单独的物体。随后，代理通过分析文本进行推理，计划并使用工具来组成这些孤立的对象。验证和人类反馈机制最终被纳入我们的代理中，以进一步纠正潜在的属性错误并细化生成的图像。在LLM代理的指导下，我们提出了一个无调整的多概念定制模型和布局到图像生成模型作为概念合成的工具，并提出了一种局部图像编辑方法作为与代理交互进行验证的工具。场景布局控制这些工具之间的图像生成过程，以防止多个对象之间的混淆。大量实验证明了我们的合成文本到图像生成方法的优越性：CompAgent在T2I CompBench（开放世界合成T2I生成的综合基准）上实现了10%以上的改进。对各种相关任务的扩展也说明了CompAgent在潜在应用中的灵活性。 et.al.|[2401.15688](http://arxiv.org/abs/2401.15688)|null|
|**2024-01-28**|**IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models**|微调有助于文本到图像生成模型适应新颖的概念（例如，风格和肖像），使用户能够打造创造性的定制内容。最近在微调方面的努力侧重于减少训练数据和减轻计算过载，但忽略了与用户意图的一致性，特别是在多模式训练数据的手动管理和面向意图的评估方面。根据一项针对微调从业者理解用户意图的形成性研究，我们提出了IntentTuner，这是一个交互式框架，在微调工作流程的每个阶段智能地结合了人类意图。IntentTuner使用户能够通过图像示例和文本描述阐明训练意图，并自动将其转换为有效的数据增强策略。此外，IntentTuner引入了新的度量来测量用户意图一致性，允许对模型训练进行意图感知监测和评估。应用程序示例和用户研究表明，与通用基线工具相比，IntentTuner简化了微调，减少了认知努力，并产生了更好的模型。 et.al.|[2401.15559](http://arxiv.org/abs/2401.15559)|null|
|**2024-01-27**|**GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis**|由于玻璃区域的透明度和反射特性的模糊性，检测玻璃区域是一项具有挑战性的任务。这些透明眼镜共享透射的任意背景场景和反射物体的视觉外观，因此没有固定的图案。最近的视觉基础模型基于大量数据进行训练，在图像感知和图像生成方面表现出惊人的性能。为了更高精度地分割玻璃表面，我们充分利用了两个视觉基础模型：分割任何东西（SAM）和稳定扩散。具体来说，我们设计了一个名为GEM的简单玻璃表面分割器，它只由SAM主干、简单特征金字塔、辨别查询选择模块和掩码解码器组成。辨别查询选择可以自适应地识别玻璃表面特征，并将其分配为掩码解码器中的初始化查询。我们还提出了一个合成但逼真的大规模玻璃表面检测数据集，称为S-GSD，通过四种不同尺度的扩散模型，包含原始真实数据大小的1x、5x、10x和20x。该数据集是迁移学习的一个可行来源。合成数据的规模对迁移学习有积极影响，而随着数据量的增加，这种改善将逐渐饱和。大量实验表明，GEM在GSD-S验证集上达到了最先进的水平（IoU+2.1%）。代码和数据集可在：https://github.com/isbrycee/GEM-Glass-Segmentor. et.al.|[2401.15282](http://arxiv.org/abs/2401.15282)|null|
|**2024-01-26**|**Annotated Hands for Generative Models**|诸如GANs和扩散模型之类的生成模型已经展示了令人印象深刻的图像生成能力。尽管取得了这些成功，但这些系统在用手创建图像方面却出奇地差。我们为生成模型提出了一种新的训练框架，该框架大大提高了此类系统创建手部图像的能力。我们的方法是用三个额外的通道来增强训练图像，这些通道为图像中的手提供注释。这些注释提供了附加结构，该附加结构诱使生成模型产生更高质量的手部图像。我们在两个不同的生成模型上演示了这种方法：生成对抗性网络和扩散模型。我们在一个新的手部图像合成数据集和包含手的真实照片上展示了我们的方法。我们通过使用现成的手部检测器对手指关节识别进行更高的置信度来测量生成的手部的改进质量。 et.al.|[2401.15075](http://arxiv.org/abs/2401.15075)|**[link](https://github.com/YY-GX/Annotated-Hands-Dataset)**|

## avatar

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis**|在生物特征安全成为现代身份验证系统基石的时代，确保这些生物特征样本的真实性至关重要。活体检测，即区分真实和伪造生物特征样本的能力，是这一挑战的前沿。这项研究对活跃度检测模型进行了全面评估，特别关注它们在跨数据库场景中的性能，这是一种因其复杂性和现实世界相关性而臭名昭著的测试范式。我们的研究首先在单个数据集上仔细评估模型，揭示其性能指标的细微差别。深入研究了一半总错误率、错误接受率和错误拒绝率等指标，我们发现了对模型优势和劣势的宝贵见解。至关重要的是，我们对跨数据库测试的探索提供了一个独特的视角，突出了在一个数据集上训练和在另一个数据集中部署之间的鸿沟。与现有方法的比较分析，从卷积网络到更复杂的策略，丰富了我们对当前形势的理解。即使在最先进的模型之间，性能的差异也突显了这一领域的固有挑战。从本质上讲，这篇论文既是研究结果的宝库，也是对生物特征活体检测中更细致、数据多样和适应性更强的方法的明确呼吁。在真实性和欺骗性之间的动态舞蹈中，我们的工作为驾驭生物识别安全的发展节奏提供了蓝图。 et.al.|[2401.16232](http://arxiv.org/abs/2401.16232)|null|
|**2024-01-27**|**AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**|最近的社区在从稀疏的多视图视频构建照片逼真的可动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色呈现逼真的服装动态，因为它们主要依赖裸体模型进行人体建模，同时保留未建模的服装部分。这主要是因为宽松服装产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为4-8）生成宽松衣服中的可动画化人类化身的新方法。为了能够在这种情况下捕捉和学习宽松服装的外观，我们使用了从基于物理的模拟数据中获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕捉和渲染复杂的服装动力学。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动力学的新方法。为了使用粗略估计为看不见的衣服状态建立逼真的渲染，引入了一个以身体和衣服运动为条件的姿势驱动的可变形神经辐射场，提供了对这两个部分的显式控制。在测试时，可以从看不见的情况中捕捉新的服装姿势，这些姿势来自基于物理或神经网络的模拟器，以驱动看不见服装的动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松、动作各异的表演者。实验表明，我们的方法能够呈现出与身体高度偏离的自然服装动力学，并很好地推广到看不见的视图和姿势，超过了现有方法的性能。代码和数据将公开。 et.al.|[2401.15348](http://arxiv.org/abs/2401.15348)|null|
|**2024-01-09**|**DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation**|我们提出了DiffSHEG，这是一种基于扩散的方法，用于语音驱动的任意长度的整体三维表达和手势生成。虽然之前的工作专注于共同语音手势或单独生成表情，但同步表情和手势的联合生成仍然很少被探索。为了解决这一问题，我们基于扩散的协同语音运动生成转换器实现了从表情到手势的单向信息流，有助于改进联合表情手势分布的匹配。此外，我们还介绍了一种基于外绘的采样策略，用于扩散模型中的任意长序列生成，提供了灵活性和计算效率。我们的方法提供了一种实用的解决方案，可以产生由语音驱动的高质量同步表情和手势生成。在两个公共数据集上进行评估后，我们的方法在数量和质量上都达到了最先进的性能。此外，一项用户研究证实了DiffSHEG优于先前的方法。通过实现表达和同步运动的实时生成，DiffSHEG展示了其在数字人和具体代理开发中的各种应用潜力。 et.al.|[2401.04747](http://arxiv.org/abs/2401.04747)|null|
|**2024-01-11**|**Jump Cut Smoothing for Talking Heads**|跳跃式剪裁给观看体验带来了一种突然的、有时是不必要的变化。我们提出了一个新颖的框架来平滑这些跳跃剪辑，在谈话头部视频的背景下。我们利用视频中其他源帧中受试者的外观，将其与由DensePose关键点和面部标志驱动的中级表示融合。为了实现运动，我们在切割周围的结束帧之间插入关键点和地标。然后，我们使用来自关键点和源帧的图像翻译网络来合成像素。由于关键点可能包含错误，我们提出了一种跨模态注意力方案，以在每个关键点的多个选项中选择最合适的来源。通过利用这种中级表示，我们的方法可以获得比强大的视频插值基线更强的结果。我们在会说话的头部视频中演示了我们的方法，如剪切填充词、停顿，甚至随机剪切。我们的实验表明，即使在有挑战性的情况下，我们也可以实现无缝转换，其中会说话的头部在跳跃切割中旋转或剧烈移动。 et.al.|[2401.04718](http://arxiv.org/abs/2401.04718)|null|
|**2024-01-01**|**Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade**|我们介绍了一个图形感知的自动编码器集成框架，以及相关的形式主义和工具，旨在促进人文学科学术的深度学习。通过组成子体系结构以生成同构于人文领域的模型，我们保持了可解释性，同时为每个子体系结构选择提供功能签名，允许传统和计算研究人员在不干扰既定实践的情况下进行合作。我们展示了我们的方法在美国后大西洋奴隶贸易历史研究中的实际应用，并做出了一些具体的技术贡献：一种新的混合图卷积自动编码器机制，用于常见图拓扑的批处理策略，以及用于特定用例的掩蔽技术。越来越多的20多项研究证明了该框架扩大不同领域参与的有效性，这些研究既有与人文主义者的合作，也有机器学习文献中的既定任务，涵盖了各种领域和数据模式。我们对几种不同的体系结构选择进行了性能比较，最后列出了这项研究即将采取的下一步行动。 et.al.|[2401.00824](http://arxiv.org/abs/2401.00824)|null|
|**2023-12-23**|**Human101: Training 100+FPS Human Gaussians in 100s from 1 View**|从单视角视频中重构人体在虚拟现实领域发挥着关键作用。一种流行的应用场景需要快速重建高保真3D数字人，同时确保实时渲染和交互。现有的方法往往难以满足这两个要求。在本文中，我们介绍了Human101，这是一种新颖的框架，通过在100秒内训练3D高斯人并以100+FPS进行渲染，能够从单视图视频中生成高保真动态3D人体重建。我们的方法利用了3D高斯飞溅的优势，它提供了3D人类的明确而有效的表示。与之前基于NeRF的管道不同，Human101巧妙地应用了以人为中心的前向高斯动画方法来变形3D高斯的参数，从而提高了渲染速度（即，以令人印象深刻的60+FPS渲染1024个分辨率的图像，以100+FPS渲染512个分辨率的图片）。实验结果表明，我们的方法大大超过了当前的方法，每秒帧数激增了10倍，并提供了相当或卓越的渲染质量。代码和演示将在上发布https://github.com/longxiang-ai/Human101. et.al.|[2312.15258](http://arxiv.org/abs/2312.15258)|**[link](https://github.com/longxiang-ai/human101)**|
|**2023-12-23**|**Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models**|扩散模型以文本提示为条件，生成具有复杂细节的逼真图像。但是，当涉及到手、牙齿等人类特征时，大多数预先训练的模型都无法生成准确的图像。我们假设，可以通过注释良好的高质量数据来克服扩散模型的这种无能。在本文中，我们专门研究了使用扩散模型改进手-物体交互图像的生成。我们收集了一个注释良好的手-物交互合成数据集，该数据集使用Prompt Propose-Verify框架进行策划，并在其上微调稳定的扩散模型。我们根据CLIPScore、ImageReward、Fedility和alignment等定性和定量指标对图像-文本数据集进行评估，并显示出比当前最先进的基准测试更好的性能。 et.al.|[2312.15247](http://arxiv.org/abs/2312.15247)|null|
|**2023-12-23**|**TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**|直接语音到语音翻译通过引入自监督学习中获得的离散单元来实现高质量的结果。这种方法避免了与模型级联相关的延迟和级联错误。然而，与音频语音相比，有声头翻译，即将视听语音（即有声头视频）从一种语言转换为另一种语言，仍然面临着几个挑战：（1）现有方法总是依赖于级联，通过音频和文本进行合成，导致延迟和级联错误。（2） 会说话的头翻译有一套有限的参考框架。如果生成的翻译超过了原始语音的长度，则需要通过重复帧来补充视频序列，从而导致不和谐的视频转换。在这项工作中，我们提出了一个有声头翻译模型\textbf{TransFace}，它可以直接将视听语音翻译成其他语言的视听语音。它由一个语音到单元的翻译模型和一个基于单元的视听语音合成器Unit2Lip组成，前者将音频语音转换为离散单元，后者并行地从离散单元重新合成同步的视听语音。此外，我们还引入了一个有界持续时间预测器，确保等轴测头的平移并防止重复的参考帧。实验表明，我们提出的Unit2Lip模型显著提高了同步性（原始和生成的音频语音在LSE-C上分别为1.601和0.982），并将LRS2上的推理速度提高了4.35倍。此外，TransFace在LRS3-T和100%等时翻译上的Es-En和Fr-En的BLEU得分分别为61.93和47.55。 et.al.|[2312.15197](http://arxiv.org/abs/2312.15197)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|
|**2023-12-22**|**Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images**|随着元宇宙慢慢成为现实，以及数字人类的快速发展，对人脸原则风格编辑管道的需求势必会增加。我们通过引入潜在2语义自动编码器（L2SAE）来满足这一需求，这是一种生成式自动编码器模型，有助于对人脸图像中几个感兴趣区域（ROI）的风格属性进行高度本地化编辑。L2SAE学习编码图像的结构和风格信息的单独潜在表示。因此，允许对所选择的ROI进行保留结构的样式编辑。编码的结构表示是具有减少的空间维度的多通道2D张量，其捕获局部和全局结构属性。样式表示是捕获全局样式属性的1D张量。在我们的框架中，我们对结构表示进行切片，以建立与不同ROI的强且不纠缠的对应关系。因此，所选ROI的样式编辑相当于（a）从切片结构表示生成的ROI掩模和（b）具有全局样式变化的解码图像的简单组合，该全局样式变化是从操纵的（使用高斯噪声）全局样式和不变的结构张量生成的。在没有额外人力监督的情况下进行风格编辑是对SOTA风格编辑管道的重大胜利，因为大多数现有作品都需要额外的人力（监督）后期培训，才能将语义归因于风格编辑。我们还取消了基于迭代优化的反演或在训练后确定可控的潜在方向，这需要额外的计算成本高昂的操作。我们在多个应用程序中为相同的应用程序提供定性和定量结果，例如使用从多个数据集采样的测试图像进行选择性风格编辑和交换。 et.al.|[2312.15037](http://arxiv.org/abs/2312.15037)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

