---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.31
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-27**|**Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting**|我们展示了将基于物理的固体和流体动画与3D高斯飞溅（3DGS）相结合的可行性，以在使用3DGS重建的虚拟场景中创建新的效果。利用底层表示中高斯飞溅和基于位置的动力学（PBD）的一致性，我们以内聚的方式管理渲染、视图合成以及固体和流体的动力学。与高斯着色器类似，我们使用添加的法线增强每个高斯内核，将内核的方向与曲面法线对齐，以优化PBD模拟。这种方法有效地消除了由固体中的旋转变形引起的尖锐噪声。它还允许我们集成基于物理的渲染，以增强流体上的动态曲面反射。因此，我们的框架能够真实地再现动态流体上的曲面高光，并促进场景对象和新视图中流体之间的交互。有关更多信息，请访问我们的项目页面\url{https://amysteriouscat.github.io/GaussianSplashing/}. et.al.|[2401.15318](http://arxiv.org/abs/2401.15318)|null|
|**2024-01-26**|**3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field**|同时实现室内环境的3D重建和新视图合成具有广泛的应用，但在技术上非常具有挑战性。基于隐式神经函数的现有技术方法可以获得极好的三维重建结果，但它们在新视图合成方面的性能可能不令人满意。神经辐射场（NeRF）的令人兴奋的发展彻底改变了新的视图合成，然而，基于NeRF的模型可能无法重建干净的几何表面。我们开发了一种双神经辐射场（Du-NeRF），以同时实现高质量的几何重建和视图渲染。Du-NeRF包含两个几何场，一个从SDF场导出以便于几何重建，另一个从密度场导出以促进新视图合成。Du NeRF的一个创新特征是，它将视图无关分量与密度场解耦，并将其用作标签来监督SDF场的学习过程。这减少了形状辐射模糊性，并使几何图形和颜色在学习过程中相互受益。大量实验表明，Du-NeRF可以显著提高室内环境下新视图合成和3D重建的性能，并且在构建包含不服从多视图颜色一致性的精细几何图形的区域时尤其有效。 et.al.|[2401.14726](http://arxiv.org/abs/2401.14726)|null|
|**2024-01-23**|**IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images**|尽管许多3D重建和新颖的视图合成方法允许从消费者相机轻松捕捉的多视图图像中真实地渲染场景，但它们在表示中烘焙照明，无法支持高级应用程序，如材质编辑、重新照明和虚拟对象插入。通过反向渲染重建基于物理的材料特性和照明有望实现此类应用。然而，大多数反向渲染技术都需要高动态范围（HDR）图像作为输入，这是大多数用户无法访问的设置。我们提出了一种方法，从多视图、低动态范围（LDR）图像中恢复场景的基于物理的材料特性和空间变化的HDR照明。我们在反向渲染管道中对LDR图像形成过程进行建模，并提出了一种新的材料、照明和相机响应模型的优化策略。与采用LDR或HDR输入的最先进的反向渲染方法相比，我们使用合成场景和真实场景来评估我们的方法。我们的方法优于以LDR图像作为输入的现有方法，并允许高度逼真的重新照明和对象插入。 et.al.|[2401.12977](http://arxiv.org/abs/2401.12977)|null|
|**2024-01-24**|**RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos**|我们介绍了一种新的在野外捕获的RGB-D对象数据集，称为WildRGB-D。与大多数现有的仅带有RGB捕获的以对象为中心的真实世界数据集不同，深度通道的直接捕获允许更好的3D注释和更广泛的下游应用。WildRGB-D包括大型类别级RGB-D对象视频，这些视频是用iPhone 360度环绕对象拍摄的。它包含大约8500个记录对象和近20000个RGB-D视频，涉及46个常见对象类别。这些视频是在三种设置的不同杂乱背景下拍摄的，以覆盖尽可能多的真实世界场景：（i）一个视频中的单个对象；（ii）一个视频中的多个对象；以及（iii）在一个视频中具有静止手的对象。该数据集由对象遮罩、真实世界比例的相机姿态和RGBD视频中重建的聚合点云进行注释。我们用WildRGB-D对四个任务进行了基准测试，包括新颖的视图合成、相机姿态估计、物体6d姿态估计和物体表面重建。我们的实验表明，RGB-D对象的大规模捕获为推进3D对象学习提供了巨大的潜力。我们的项目页面是https://wildrgbd.github.io/. et.al.|[2401.12592](http://arxiv.org/abs/2401.12592)|null|
|**2024-01-23**|**Methods and strategies for improving the novel view synthesis quality of neural radiation field**|神经辐射场（NeRF）技术可以从2D图像中学习场景的3D隐式模型，并合成逼真的新视图图像。该技术得到了业界的广泛关注，具有良好的应用前景。针对NeRF图像渲染质量需要提高的问题，近三年来，许多研究人员提出了各种方法来提高渲染质量。对最新的相关论文进行了分类和综述，分析了质量改进背后的技术原理，并讨论了质量改进方法的未来发展方向。这项研究可以帮助研究人员快速了解该领域技术的现状和发展脉络，有助于激发更高效算法的发展，促进NeRF技术在相关领域的应用。 et.al.|[2401.12451](http://arxiv.org/abs/2401.12451)|null|
|**2024-01-22**|**HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs**|神经辐射场（NeRF）作为一种通过从离散观测中学习场景表示的新型视图合成范式，已经引起了人们的极大关注。然而，当面对稀疏视图输入时，NeRF表现出明显的性能退化，从而限制了其进一步的适用性。在这项工作中，我们介绍了层次几何、语义和光度引导的NeRF（HG3-NeRF），这是一种新的方法，可以解决上述限制，并增强不同视图中几何、语义内容和外观的一致性。我们提出了分层几何制导（HGG），将运动结构的附加（SfM），即稀疏深度先验，纳入场景表示中。与直接深度监督不同，HGG从局部几何区域到全局几何区域对体积点进行采样，减轻了深度先验中固有偏差引起的偏差。此外，我们从不同分辨率的图像中观察到的语义一致性的显著变化中获得了灵感，并提出了层次语义引导（HSG）来学习粗到细的语义内容，该内容对应于粗到细场景表示。实验结果表明，HG3-NeRF可以在不同的标准基准上优于其他最先进的方法，并实现稀疏视图输入的高保真度合成结果。 et.al.|[2401.11711](http://arxiv.org/abs/2401.11711)|null|
|**2024-01-18**|**Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions**|隐式神经表示（INRs）的许多变体，其中神经网络被训练为信号的连续表示，对于下游任务具有巨大的实用性，包括新颖的视图合成、视频压缩和图像超分辨率。不幸的是，对这些网络的内部运作方式的研究严重不足。我们的工作，即解释隐式神经画布（XINC），是一个统一的框架，用于通过检查每个神经元对每个输出像素的贡献强度来解释INRs的特性。我们将这些贡献图的集合称为隐式神经画布，并使用这一概念来证明我们研究的INR学会了以令人惊讶的方式“观察”它们所代表的帧。例如，INR往往具有高度分布的表示。虽然缺乏高级对象语义，但它们对颜色和边缘有很大的偏见，而且几乎完全是空间不可知的。我们通过研究视频INR中对象在时间上的表现方式得出了我们的结论，使用聚类来可视化跨层和架构的相似神经元，并表明这是由运动主导的。这些见解证明了我们的分析框架的普遍有用性。我们的项目页面位于https://namithap10.github.io/xinc. et.al.|[2401.10217](http://arxiv.org/abs/2401.10217)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Objects With Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting**|从照片中重建对象并将其虚拟地放置在新环境中超出了标准的新颖视图合成任务，因为对象的外观不仅要适应新颖的视点，还要适应新的照明条件，而且反向渲染方法的评估依赖于新颖的视图合成数据或用于定量分析的简单合成数据集。这项工作提供了一个真实世界的数据集，用于测量重新照明对象的重建和渲染。为此，我们捕获了多个环境中相同对象的环境照明和地面实况图像，从而可以从一个环境中拍摄的图像中重建对象，并量化看不见的照明环境的渲染视图的质量。此外，我们介绍了一个由现成方法组成的简单基线，并在重新照明任务中测试了几种最先进的方法，表明新的视图合成不是衡量性能的可靠指标。代码和数据集可在https://github.com/isl-org/objects-with-lighting . et.al.|[2401.09126](http://arxiv.org/abs/2401.09126)|**[link](https://github.com/isl-org/objects-with-lighting)**|
|**2024-01-17**|**ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization**|在给定一组2D图像的情况下，神经辐射场（NeRF）在新视图合成（NVS）中表现出显著的性能。然而，NeRF训练需要每个输入视图的精确相机姿势，通常通过运动结构（SfM）管道获得。最近的作品试图放松这种限制，但它们仍然经常依赖于可以改进的体面的初始姿势。在这里，我们旨在消除姿势初始化的要求。我们提出了增量置信（ICON），这是一种从2D视频帧中训练NeRF的优化过程。ICON仅假设相机运动平滑，以估计姿势的初始猜测。此外，ICON引入了“置信度”：一种用于动态重加权梯度的模型质量自适应度量。ICON依赖于高置信度姿势来学习NeRF，并依赖于高置信度3D结构（由NeRF编码）来学习姿势。我们表明，与使用SfM姿势的方法相比，ICON在没有预先初始化姿势的情况下，在CO3D和HO3D中都实现了卓越的性能。 et.al.|[2401.08937](http://arxiv.org/abs/2401.08937)|null|

## MLLM

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**|我们介绍了InternetLM-XComposer2，这是一种在自由形式文本图像合成和理解方面表现出色的尖端视觉语言模型。该模型超越了传统的视觉语言理解，从轮廓、详细的文本规范和参考图像等不同输入中熟练地制作交错的文本图像内容，实现了高度可定制的内容创建。InternLM-XComposer2提出了一种部分LoRA（PLoRA）方法，该方法将额外的LoRA参数专门应用于图像标记，以保持预先训练的语言知识的完整性，在精确的视觉理解和具有文学天赋的文本合成之间取得平衡。实验结果表明，基于InternetLM2-7B的InternetLM-XComposer2在生成高质量的长文本多模式内容方面具有优势，并且在各种基准测试中具有卓越的视觉语言理解性能，不仅显著优于现有的多模式模型，而且在某些评估中与GPT-4V和Gemini Pro相匹配甚至超过。这突出了它在多模式理解领域的非凡熟练程度。具有7B参数的InternetLM-XComposer2型号系列可在https://github.com/InternLM/InternLM-XComposer. et.al.|[2401.16420](http://arxiv.org/abs/2401.16420)|**[link](https://github.com/internlm/internlm-xcomposer)**|
|**2024-01-29**|**CO2: Efficient Distributed Training with Full Communication-Computation Overlap**|大型语言模型的根本成功取决于大规模分布式训练技术的有效实现。尽管如此，构建一个具有高速通信互连性的庞大、高性能集群的成本高得令人望而却步，而且只有知名实体才能访问。在这项工作中，我们的目标是降低这一障碍，并在带宽有限的集群中实现大规模训练的民主化。我们提出了一种称为CO2的新方法，该方法将本地更新和异步通信引入分布式数据并行训练，从而促进了CO2通信与计算的完全重叠。即使在受非常有限的通信带宽限制的广泛的多节点集群上，CO2也能够获得高可扩展性。我们进一步提出了过时间隙惩罚和外部动量修剪技术以及CO2，以增强其收敛性和训练稳定性。此外，CO2与成熟的ZeRO系列优化器无缝集成，通过大型模型训练减少模型状态的内存消耗。我们还提供了收敛性的数学证明，并建立了严格的上界。此外，我们通过一系列广泛的实践实验验证了我们的发现，这些实验涵盖了计算机视觉和自然语言处理领域的广泛任务。这些实验用于证明当在包括多达128个A100 GPU的配置中部署时，CO2在收敛性、通用性和可扩展性方面的能力。无论是在具有800Gbps RDMA或80Gbps TCP/IP节点间连接的集群上，结果都强调了CO2的卓越容量，可以极大地提高可扩展性。 et.al.|[2401.16265](http://arxiv.org/abs/2401.16265)|null|
|**2024-01-27**|**A Survey on Data Augmentation in Large Model Era**|包括大型语言和扩散模型在内的大型模型在接近人类水平的智力方面表现出了非凡的前景，引起了学术界和工业界的极大兴趣。然而，这些大型模型的训练需要大量高质量的数据，随着这些模型的不断更新，现有的高质量数据库可能很快就会耗尽。这一挑战推动了对数据增强方法的研究激增。利用大型模型，这些数据增强技术的性能优于传统方法。本文从全面的角度对大型模型驱动的数据扩充方法进行了详尽的综述。我们首先将相关研究分类为三大类：图像增强、文本增强和配对数据增强。接下来，我们将深入研究与基于模型的大型数据扩充相关的各种数据后处理技术。然后，我们的讨论扩展到包括自然语言处理、计算机视觉和音频信号处理中这些数据增强方法的一系列应用。我们继续评估基于模型的大型数据增强在不同场景中的成功和局限性。在结束我们的审查时，我们强调了数据增强领域未来探索的潜在挑战和途径。我们的目标是为研究人员提供关键见解，最终为更复杂的大型模型的发展做出贡献。我们一贯将相关开源材料保存在：https://github.com/MLGroup-JLU/LLM-data-aug-survey. et.al.|[2401.15422](http://arxiv.org/abs/2401.15422)|null|
|**2024-01-26**|**F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods**|大型语言模型（LLM）以其前所未有的性能而备受关注，导致越来越多的研究评估LLM。然而，这些评估基准仅限于评估遵循指令的能力，而忽略了培训前阶段出现的基本能力。以往的主观评价方法主要是根据API模型进行评分。然而，在缺乏参考文献的情况下，大型模型辨别细微差异的能力有限。为了弥补这一差距，我们提出了F-Eval，这是一个双语评估基准，用于评估基本能力，包括表达能力、常识能力和逻辑能力。F-Eval中的任务包括多选客观任务、开放式客观任务、基于参考的主观任务和无参考的主观工作。对于无参考的主观任务，我们设计了新的评估方法，作为API模型评分的替代方案。我们对13个高级LLM进行了评估。结果表明，与其他评估者相比，我们的评估方法显示出更高的相关系数和更大的差异。此外，我们还讨论了不同模型大小、维度和归一化方法的影响。我们预计F-Eval将促进LLM基本能力的研究。 et.al.|[2401.14869](http://arxiv.org/abs/2401.14869)|**[link](https://github.com/juliasun623/f-eval)**|
|**2024-01-26**|**Leveraging Large Models for Crafting Narrative Visualization: A Survey**|叙事可视化有效地将数据转化为引人入胜的故事，使复杂的信息可供广大受众访问。大型模型对叙事可视化至关重要，通过其处理自然语言查询和答案、生成连贯叙事和增强视觉交流的卓越能力，从本质上促进了这一过程。受先前叙事可视化工作和大型模型最新进展的启发，我们综合了大型模型在叙事可视化各个阶段的潜在任务和机会。在我们的研究中，我们调查了79篇论文，探讨了大型模型在叙事可视化自动化创建中的作用。我们提出了一个综合的管道，利用大型模型来制作叙事可视化，将回顾的文献分为四个基本阶段：数据、叙事、可视化和呈现。此外，我们确定了九个特定任务，其中大型模型应用于这些阶段。这项研究描绘了LM4NV过程中的挑战和机遇，为未来的研究提供了深刻的方向，并为该领域的学者提供了宝贵的指导。 et.al.|[2401.14010](http://arxiv.org/abs/2401.14010)|null|
|**2024-01-23**|**Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study**|在大模型时代，解码的自回归性质往往导致延迟成为一个重要的瓶颈。我们提出了一种非自回归LM融合ASR系统，该系统有效地利用了加速器硬件的并行化能力。我们的方法在每段评分模式中结合了通用语音模型（USM）和PaLM 2语言模型，在FLEURS和YouTube字幕上，所有语言的平均相对WER提高了10.8%和3.6%。此外，我们的综合消融研究分析了LLM大小、上下文长度、词汇大小、融合方法等关键参数。例如，我们探讨了LLM大小在128M到340B参数范围内对ASR性能的影响。这项研究为影响实际大规模LM融合语音识别系统有效性的因素提供了有价值的见解。 et.al.|[2401.12789](http://arxiv.org/abs/2401.12789)|null|
|**2024-01-23**|**Full-Stack Optimization for CAM-Only DNN Inference**|在过去的几年里，神经网络的准确性在各个领域都有了很大的提高。然而，它们不断增加的复杂性导致了冯·诺依曼系统中令人望而却步的高能量需求和延迟。最近已经提出了几种内存计算（CIM）系统来克服这一问题，但涉及大型模型的准确性、硬件可靠性和可扩展性的权衡仍然是一个挑战。此外，对于一些CIM设计，激活运动仍然需要相当大的时间和能量。本文探讨了三元权重神经网络和使用跑道存储器（RTM）实现的关联处理器（AP）的算法优化的组合。我们提出了一种新的编译流程，通过降低AP的算术强度来优化AP上的卷积。通过利用基于RTM的AP的优势，这种方法大大减少了存储器内的数据传输，同时解决了准确性、能效和可靠性问题。具体而言，与内存加速器中的纵横制相比，我们的解决方案将ImageNet上的ResNet-18推理的能效提高了7.5倍，同时保持了软件的准确性。 et.al.|[2401.12630](http://arxiv.org/abs/2401.12630)|null|
|**2024-01-22**|**A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network**|多尺度卷积神经网络（CNN）在解决各种视觉问题方面表现出了非凡的能力。然而，融合不同尺度的特征总是导致模型尺寸大，阻碍了多尺度细胞神经网络在RGB-D显著性检测中的应用。在本文中，我们提出了一种定制的特征融合模块，称为显著性增强特征融合（SEFF），用于RGB-D显著性检测。SEFF利用相邻尺度的显著性图来增强融合所需的特征，从而产生更具代表性的融合特征。我们的多尺度RGB-D显著性检测器使用SEFF并处理具有三个不同尺度的图像。SEFF用于融合RGB和深度图像的特征，以及不同尺度的解码器的特征。在五个基准数据集上的大量实验已经证明了我们的方法优于十个SOTA显著性检测器。 et.al.|[2401.11914](http://arxiv.org/abs/2401.11914)|null|
|**2024-01-22**|**SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese**|我们介绍了SuperCLUE-Math6（SC-Math6），这是一个新的基准数据集，用于评估汉语模型的数学推理能力。SC-Math6是GSM8K数据集的升级中文版，具有更强的难度、多样性和应用范围。它由2000多个数学单词问题组成，需要多步骤推理并提供自然语言解决方案。我们提出了一种创新方案，根据不同推理步骤问题的性能来量化大型模型的推理能力。在12个具有代表性的中国模型上进行的实验表明，推理水平分层清晰，GPT-4等顶级模型表现出优异的性能。SC-Math6填补了中国数学推理基准的空白，为提高汉语模型的智能性提供了一个全面的测试平台。 et.al.|[2401.11819](http://arxiv.org/abs/2401.11819)|null|
|**2024-01-22**|**P2DT: Mitigating Forgetting in task-incremental Learning with progressive prompt Decision Transformer**|灾难性遗忘对管理由大型模型控制的智能代理提出了重大挑战，当这些代理面临新任务时，会导致性能下降。在我们的工作中，我们提出了一种新的解决方案——渐进式即时决策转换器（P2DT）。该方法通过在新任务训练期间动态附加决策令牌来增强基于转换器的模型，从而促进特定于任务的策略。我们的方法可以缓解持续和离线强化学习场景中的遗忘。此外，P2DT利用了通过传统强化学习从所有任务中收集的轨迹，并在训练过程中生成新的任务特定令牌，从而保留了先前研究的知识。初步结果表明，我们的模型有效地缓解了灾难性遗忘，并能很好地适应不断增加的任务环境。 et.al.|[2401.11666](http://arxiv.org/abs/2401.11666)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**Spatial-Aware Latent Initialization for Controllable Image Generation**|最近，文本到图像的扩散模型已经证明了以文本输入为条件生成高质量图像的令人印象深刻的能力。然而，这些模型很难准确地遵循关于空间布局信息的文本指令。虽然之前的研究主要集中在将交叉注意力图与布局条件对齐，但它们忽略了初始化噪声对布局指导的影响。为了实现更好的布局控制，我们建议在去噪过程中利用空间感知的初始化噪声。具体而言，我们发现具有有限反转步骤的反转参考图像包含关于对象位置的宝贵空间意识，从而在生成的图像中产生类似的布局。基于这一观察结果，我们开发了一个开放的词汇框架，为每个布局条件定制空间感知的初始化噪声。在不修改除初始化噪声之外的其他模块的情况下，我们的方法可以作为即插即用模块无缝集成到其他无训练布局指导框架中。我们在可用的稳定扩散模型和COCO数据集上对我们的方法进行了定量和定性评估。配备了空间感知的潜在初始化，我们的方法在保留高质量内容的同时显著提高了布局指导的有效性。 et.al.|[2401.16157](http://arxiv.org/abs/2401.16157)|null|
|**2024-01-29**|**Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You**|文本到图像生成模型最近在图像质量、灵活性和文本对齐方面取得了惊人的成果，因此被应用于数量快速增长的应用中。通过提高多语言能力，一个更大的社区现在可以使用这种技术。然而，正如我们将要展示的那样，多语言模型与单语模型同样存在（性别）偏见。此外，人们自然期望这些模型能在不同语言之间提供相似的结果，但事实并非如此，而且语言之间存在重要差异。因此，我们提出了一个新的基准MAGBIG，旨在促进无性别偏见的多语言模型研究。我们研究了多语言T2I模型是否利用MAGBIG放大了性别偏见。为此，我们使用多语言提示来请求某一职业或特征的人的肖像图像（使用形容词）。我们的研究结果表明，模型不仅偏离了每个性别产生的可能性应该相等的规范假设，而且不同语言之间也存在很大差异。此外，我们研究了即时工程策略，即使用间接、中性的配方，作为这些偏见的可能补救措施。不幸的是，它们只在有限的程度上起作用，并导致更差的文本到图像对齐。因此，这项工作需要对图像生成器中不同语言的不同表示进行更多的研究。 et.al.|[2401.16092](http://arxiv.org/abs/2401.16092)|null|
|**2024-01-29**|**Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling**|我们介绍了Motion-I2V，这是一种用于一致可控的图像到视频生成（I2V）的新框架。与之前直接学习复杂的图像到视频映射的方法不同，Motion-I2V通过显式运动建模将I2V分解为两个阶段。对于第一阶段，我们提出了一种基于扩散的运动场预测器，该预测器专注于推导参考图像像素的轨迹。对于第二阶段，我们提出了运动增强的时间注意力来增强视频潜在扩散模型中的有限一维时间注意力。该模块可以在第一阶段预测轨迹的引导下，有效地将参考图像的特征传播到合成帧。与现有方法相比，即使在存在大的运动和视点变化的情况下，Motion-I2V也可以生成更一致的视频。通过为第一阶段训练稀疏轨迹ControlNet，Motion-I2V可以支持用户使用稀疏轨迹和区域注释精确控制运动轨迹和运动区域。这提供了I2V过程的更多可控性，而不仅仅依赖于文本指令。此外，Motion-I2V的第二阶段自然支持零镜头视频到视频的转换。定性和定量比较都证明了Motion-I2V在一致和可控的图像到视频生成方面优于现有方法。 et.al.|[2401.15977](http://arxiv.org/abs/2401.15977)|null|
|**2024-01-29**|**Diffusion Facial Forgery Detection**|检测扩散生成的图像最近已发展成为一个新兴的研究领域。现有的基于扩散的数据集主要集中于一般图像生成。然而，到目前为止，面部伪造的研究还较少，这会带来更严重的社会风险。为了解决这一差距，本文介绍了DiFF，这是一个专门用于人脸聚焦扩散生成图像的综合数据集。DiFF包括在四种条件下使用十三种不同的生成方法合成的500000多幅图像。特别是，该数据集利用了30000个精心收集的文本和视觉提示，确保了图像的合成具有高保真度和语义一致性。我们通过人体测试和几种有代表性的伪造检测方法在DiFF数据集上进行了广泛的实验。结果表明，人类观察者和自动检测器的二进制检测精度通常低于30%，这揭示了检测扩散生成的面部伪造品的挑战。此外，我们提出了一种边缘图正则化方法，以有效地增强现有检测器的泛化能力。 et.al.|[2401.15859](http://arxiv.org/abs/2401.15859)|null|
|**2024-01-29**|**2L3: Lifting Imperfect Generated 2D Images into Accurate 3D**|从单个图像重建3D对象是一个有趣但具有挑战性的问题。一个有前途的解决方案是利用多视图（MV）3D重建将生成的MV图像融合成一致的3D对象。然而，生成的图像通常存在照明不一致、几何体错位和视图稀疏的问题，导致重建质量较差。为了解决这些问题，我们提出了一种新的3D重建框架，该框架利用固有分解引导、瞬态单先验引导和视图增强来分别解决这三个问题。具体来说，我们首先利用阴影信息从生成的图像中解耦，以减少不一致照明的影响；然后，我们引入了具有视点相关瞬态编码的单声道先验来增强重构的法线；最后，我们设计了一种视图增强融合策略，最大限度地减少生成的稀疏视图中的像素级损失和增强的随机视图中的语义损失，从而获得视图一致的几何结构和详细的纹理。因此，我们的方法能够集成预先训练的MV图像生成器和基于神经网络的体积有符号距离函数（SDF）表示，用于单个图像到3D对象的重建。我们在各种数据集上评估了我们的框架，并证明了其在定量和定性评估中的卓越性能，这意味着3D对象重建方面取得了重大进展。与最新的同步梦想家方法相比，我们将倒角距离误差降低了约36%，PSNR提高了约30%。 et.al.|[2401.15841](http://arxiv.org/abs/2401.15841)|null|
|**2024-01-28**|**Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding**|随着大规模文本图像生成模型在文本图像生成领域取得了显著进展，人们提出了许多微调方法。然而，这些模型往往难以处理新颖的对象，尤其是一次性场景。我们提出的方法旨在以对象驱动的方式解决可推广性和保真度的挑战，仅使用单个输入图像和感兴趣的特定对象区域。为了提高可推广性并缓解过度拟合，在我们的范式中，在微调扩散模型之前，基于对象的外观及其类别初始化原型嵌入。在微调过程中，我们提出了一个类特征正则化，以保留对象类的先验知识。为了进一步提高保真度，我们引入了特定于对象的损失，它也可以用于植入多个对象。总体而言，我们提出的用于植入新对象的对象驱动方法可以与现有概念无缝集成，并且具有高保真度和通用性。我们的方法优于现有的几种方法。代码将被发布。 et.al.|[2401.15708](http://arxiv.org/abs/2401.15708)|null|
|**2024-01-30**|**Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation**|尽管在生成高质量图像的文本到图像模型方面取得了重大进展，但在复杂的文本提示背景下，这些方法仍然难以确保文本提示相对于图像的可控性，尤其是在保留对象属性和关系方面。在本文中，我们提出了CompAgent，这是一种无需训练的合成文本到图像生成方法，以大型语言模型（LLM）代理为核心。CompAgent的基本思想是以分而治之的方法论为前提的。给定包含多个概念（包括对象、属性和关系）的复杂文本提示，LLM代理首先对其进行分解，这需要提取单个对象及其相关属性，并预测连贯的场景布局。然后可以独立地征服这些单独的物体。随后，代理通过分析文本进行推理，计划并使用工具来组成这些孤立的对象。验证和人类反馈机制最终被纳入我们的代理中，以进一步纠正潜在的属性错误并细化生成的图像。在LLM代理的指导下，我们提出了一个无调整的多概念定制模型和布局到图像生成模型作为概念合成的工具，并提出了一种局部图像编辑方法作为与代理交互进行验证的工具。场景布局控制这些工具之间的图像生成过程，以防止多个对象之间的混淆。大量实验证明了我们的合成文本到图像生成方法的优越性：CompAgent在T2I CompBench（开放世界合成T2I生成的综合基准）上实现了10%以上的改进。对各种相关任务的扩展也说明了CompAgent在潜在应用中的灵活性。 et.al.|[2401.15688](http://arxiv.org/abs/2401.15688)|null|
|**2024-01-28**|**IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models**|微调有助于文本到图像生成模型适应新颖的概念（例如，风格和肖像），使用户能够打造创造性的定制内容。最近在微调方面的努力侧重于减少训练数据和减轻计算过载，但忽略了与用户意图的一致性，特别是在多模式训练数据的手动管理和面向意图的评估方面。根据一项针对微调从业者理解用户意图的形成性研究，我们提出了IntentTuner，这是一个交互式框架，在微调工作流程的每个阶段智能地结合了人类意图。IntentTuner使用户能够通过图像示例和文本描述阐明训练意图，并自动将其转换为有效的数据增强策略。此外，IntentTuner引入了新的度量来测量用户意图一致性，允许对模型训练进行意图感知监测和评估。应用程序示例和用户研究表明，与通用基线工具相比，IntentTuner简化了微调，减少了认知努力，并产生了更好的模型。 et.al.|[2401.15559](http://arxiv.org/abs/2401.15559)|null|
|**2024-01-27**|**GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis**|由于玻璃区域的透明度和反射特性的模糊性，检测玻璃区域是一项具有挑战性的任务。这些透明眼镜共享透射的任意背景场景和反射物体的视觉外观，因此没有固定的图案。最近的视觉基础模型基于大量数据进行训练，在图像感知和图像生成方面表现出惊人的性能。为了更高精度地分割玻璃表面，我们充分利用了两个视觉基础模型：分割任何东西（SAM）和稳定扩散。具体来说，我们设计了一个名为GEM的简单玻璃表面分割器，它只由SAM主干、简单特征金字塔、辨别查询选择模块和掩码解码器组成。辨别查询选择可以自适应地识别玻璃表面特征，并将其分配为掩码解码器中的初始化查询。我们还提出了一个合成但逼真的大规模玻璃表面检测数据集，称为S-GSD，通过四种不同尺度的扩散模型，包含原始真实数据大小的1x、5x、10x和20x。该数据集是迁移学习的一个可行来源。合成数据的规模对迁移学习有积极影响，而随着数据量的增加，这种改善将逐渐饱和。大量实验表明，GEM在GSD-S验证集上达到了最先进的水平（IoU+2.1%）。代码和数据集可在：https://github.com/isbrycee/GEM-Glass-Segmentor. et.al.|[2401.15282](http://arxiv.org/abs/2401.15282)|null|
|**2024-01-26**|**Annotated Hands for Generative Models**|诸如GANs和扩散模型之类的生成模型已经展示了令人印象深刻的图像生成能力。尽管取得了这些成功，但这些系统在用手创建图像方面却出奇地差。我们为生成模型提出了一种新的训练框架，该框架大大提高了此类系统创建手部图像的能力。我们的方法是用三个额外的通道来增强训练图像，这些通道为图像中的手提供注释。这些注释提供了附加结构，该附加结构诱使生成模型产生更高质量的手部图像。我们在两个不同的生成模型上演示了这种方法：生成对抗性网络和扩散模型。我们在一个新的手部图像合成数据集和包含手的真实照片上展示了我们的方法。我们通过使用现成的手部检测器对手指关节识别进行更高的置信度来测量生成的手部的改进质量。 et.al.|[2401.15075](http://arxiv.org/abs/2401.15075)|**[link](https://github.com/YY-GX/Annotated-Hands-Dataset)**|

## avatar

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-27**|**AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**|最近的社区在从稀疏的多视图视频构建照片逼真的可动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色呈现逼真的服装动态，因为它们主要依赖裸体模型进行人体建模，同时保留未建模的服装部分。这主要是因为宽松服装产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为4-8）生成宽松衣服中的可动画化人类化身的新方法。为了能够在这种情况下捕捉和学习宽松服装的外观，我们使用了从基于物理的模拟数据中获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕捉和渲染复杂的服装动力学。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动力学的新方法。为了使用粗略估计为看不见的衣服状态建立逼真的渲染，引入了一个以身体和衣服运动为条件的姿势驱动的可变形神经辐射场，提供了对这两个部分的显式控制。在测试时，可以从看不见的情况中捕捉新的服装姿势，这些姿势来自基于物理或神经网络的模拟器，以驱动看不见服装的动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松、动作各异的表演者。实验表明，我们的方法能够呈现出与身体高度偏离的自然服装动力学，并很好地推广到看不见的视图和姿势，超过了现有方法的性能。代码和数据将公开。 et.al.|[2401.15348](http://arxiv.org/abs/2401.15348)|null|
|**2024-01-23**|**GALA: Generating Animatable Layered Assets from a Single Scan**|我们提出了GALA，这是一个框架，它以单层衣服的3D人体网格为输入，并将其分解为完整的多层3D资产。然后可以将输出与其他资产组合，以创建具有任何姿势的新颖的穿着衣服的人类化身。现有的重建方法通常将穿着衣服的人类视为单层几何体，并忽略了人类与发型、衣服和配饰的固有组成性，从而限制了网格在下游应用中的效用。将单层网格分解为单独的层是一项具有挑战性的任务，因为它需要为严重遮挡的区域合成合理的几何结构和纹理。此外，即使分解成功，网格也不能在姿势和体型方面进行归一化，从而无法实现具有新身份和姿势的连贯合成。为了应对这些挑战，我们建议利用预训练的2D扩散模型的一般知识作为人类和其他资产的几何和外观先验。我们首先使用从多视图2D分割中提取的3D表面分割来分离输入网格。然后，我们使用一种新的姿态引导的分数蒸馏采样（SDS）损失来合成姿态空间和规范空间中不同层的缺失几何。一旦我们完成了高保真3D几何体的修复，我们还将相同的SDS损失应用于其纹理，以获得包括初始遮挡区域在内的完整外观。通过一系列分解步骤，我们在一个共享的规范空间中获得了多层3D资产，这些资产根据姿势和人体形状进行了归一化，从而支持轻松合成新的身份，并用新的姿势进行复活。我们的实验证明了与现有解决方案相比，我们的方法在分解、规范化和组合任务方面的有效性。 et.al.|[2401.12979](http://arxiv.org/abs/2401.12979)|null|
|**2024-01-30**|**PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting**|尽管取得了很大进展，但实现实时高保真度的头部化身动画仍然很困难，并且现有的方法必须在速度和质量之间进行权衡。基于3DMM的方法通常无法对眼镜和发型等非面部结构进行建模，而神经隐式模型则存在变形不灵活和渲染效率低下的问题。尽管3D高斯已经被证明具有很好的几何表示和辐射场重建能力，但在头部化身创建中应用3D高斯仍然是一个主要挑战，因为3D高斯很难对由姿势和表情变化引起的头部形状变化进行建模。在本文中，我们介绍了PSAvatar，这是一种新的可动画化头部化身创建框架，它利用离散几何图元创建参数可变形形状模型，并使用3D高斯进行精细细节表示和高保真渲染。参数变形形状模型是一种基于点的变形形状模型（PMSM），它使用点代替网格进行三维表示，以实现增强的表示灵活性。PMSM首先通过在表面和网格外采样将FLAME网格转换为点，以不仅能够重建表面状结构，而且能够重建复杂的几何形状，如眼镜和发型。通过以综合分析的方式将这些点与头部形状对齐，PMSM可以利用3D高斯进行精细的细节表示和外观建模，从而能够创建高保真化身。我们展示了PSAvatar可以重建各种主题的高保真头部化身，并且化身可以实时动画化（以512 $\times$512的分辨率$\ge$ 25fps）。 et.al.|[2401.12900](http://arxiv.org/abs/2401.12900)|**[link](https://github.com/pcl3dv/PSAvatar)**|
|**2024-01-26**|**New spectral-parameter dependent solutions of the Yang-Baxter equation**|杨-巴克斯特方程（YBE）在研究可积多体量子系统中起着至关重要的作用。许多已知的YBE解决方案提供了从量子自旋链到超导系统的各种例子。可解统计力学模型及其化身也基于YBE。因此，YBE的新解决方案可以用于构建新的有趣的1D量子或2D经典系统，并具有许多其他深远的应用。在这项工作中，我们试图在对应于两个量子位情况的最低维度上找到YBE的（几乎）穷举的解集。我们开发了一种算法，该算法有可能用于生成YBE的新的高维解。 et.al.|[2401.12710](http://arxiv.org/abs/2401.12710)|null|
|**2024-01-20**|**UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures**|3D化身生成的最新进展已经引起了人们的极大关注。这些突破旨在制作更逼真的可动画化化身，缩小虚拟体验和现实世界体验之间的差距。大多数现有的工作都采用了分数蒸馏采样（SDS）损失，结合可微分的渲染器和文本条件，来指导扩散模型生成3D化身。然而，SDS通常生成的结果过于平滑，面部细节很少，因此与祖先采样相比缺乏多样性。另一方面，其他作品从单个图像生成3D化身，其中不想要的光照效果、透视图和较差的图像质量的挑战使得它们难以可靠地重建具有对齐的完整纹理的3D面部网格。在本文中，我们提出了一种新的3D化身生成方法，称为UltrAvatar，该方法具有增强的几何逼真度和卓越的基于物理的渲染（PBR）纹理质量，而没有不需要的照明。为此，该方法提出了一种漫射颜色提取模型和真实性引导的纹理漫射模型。前者去除了不需要的照明效果，以显示真实的漫反射颜色，从而可以在各种照明条件下渲染生成的化身。后者遵循两种基于梯度的指导，用于生成PBR纹理，以更好地与3D网格几何体对齐，呈现不同的人脸身份特征和细节。我们证明了所提出的方法的有效性和稳健性，在实验中大大优于最先进的方法。 et.al.|[2401.11078](http://arxiv.org/abs/2401.11078)|null|
|**2024-01-19**|**Fast Registration of Photorealistic Avatars for VR Facial Animation**|虚拟现实（VR）展示了社交互动的前景，这种互动比其他媒体更具沉浸感。其中的关键是能够在佩戴VR耳机的情况下准确地为自己肖像的真实感化身制作动画。尽管在离线设置中可以将特定于个人的化身高质量地注册到头戴式摄像机（HMC）图像，但通用实时模型的性能显著降低。由于相机视角倾斜和模态差异，在线注册也具有挑战性。在这项工作中，我们首先表明，化身和头戴式耳机相机图像之间的域间隙是困难的主要来源之一，其中基于转换器的架构在域一致性数据上实现了高精度，但当重新引入域间隙时会降低。基于这一发现，我们开发了一种系统设计，将问题解耦为两个部分：1）一个接受域输入的迭代细化模块，以及2）一个基于表情和头部姿势的当前估计的通用化身引导的图像到图像风格传递模块。这两个模块相互加强，因为当显示接近真实的例子时，图像风格的转移变得更容易，而更好的域间隙去除有助于配准。我们的系统可以高效地产生高质量的结果，从而无需昂贵的离线注册来生成个性化标签。我们通过在商品耳机上进行的大量实验验证了我们的方法的准确性和效率，证明了与直接回归方法和离线注册相比的显著改进。 et.al.|[2401.11002](http://arxiv.org/abs/2401.11002)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Tri $^{2}$-plane: Volumetric Avatar Reconstruction with Feature Pyramid**|近年来，在利用神经体积绘制进行面部化身重建方面取得了相当大的成就。尽管取得了显著的进步，但从单眼视频中重建复杂而动态的头部运动仍然需要捕捉和恢复细粒度的细节。在这项工作中，我们提出了一种新的方法，命名为Tri$^2$-plane，用于单目照片逼真的体积头部化身重建。与现有的依赖于单个三平面变形场进行动态面部建模的工作不同，所提出的tri$^2$-平面利用了特征金字塔和三个上下横向连接三平面的原理来改进细节。它在多个尺度上采样和渲染面部细节，从整个面部过渡到特定的局部区域，然后过渡到更精细的子区域。此外，我们在训练中加入了一种基于相机的几何感知滑动窗口方法作为增强，它提高了规范空间之外的鲁棒性，特别提高了交叉身份生成能力。实验结果表明，Tri$^2$ -平面不仅超越了现有的方法，而且通过实验在定量指标和定性评估方面都取得了卓越的性能。 et.al.|[2401.09386](http://arxiv.org/abs/2401.09386)|**[link](https://github.com/songluchuan/tri2plane)**|
|**2024-01-20**|**Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**|一次拍摄3D会说话的肖像生成旨在从看不见的图像中重建3D化身，然后用参考视频或音频将其动画化，以生成会说话的人像视频。现有的方法无法同时实现准确的三维化身重建和稳定的人脸动画。此外，虽然现有的作品主要集中在合成头部，但生成自然的躯干和背景片段以获得逼真的说话肖像视频也是至关重要的。为了解决这些限制，我们提出了Real3D Potrait，该框架（1）通过从3D人脸生成模型中提取3D先验知识的大图像到平面模型提高了单次3D重建能力；（2） 利用高效的运动适配器促进精确的运动条件动画；（3） 使用头部-躯干背景超分辨率模型来合成具有自然躯干运动和可切换背景的逼真视频；以及（4）支持具有可推广的音频到运动模型的单镜头音频驱动的谈话面部生成。大量实验表明，与以前的方法相比，Real3D Portrait很好地概括了看不见的身份，并生成了更逼真的谈话肖像视频。视频样本和源代码可在https://real3dportrait.github.io . et.al.|[2401.08503](http://arxiv.org/abs/2401.08503)|null|
|**2024-01-13**|**EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation**|随着虚拟环境的不断发展，对沉浸式和情感化体验的需求也在增长。为了满足这一需求，我们引入了使用优化知识提取（EVOKE）实现情感的虚拟化身映射，这是一种轻量级的情感识别框架，旨在将情感识别无缝集成到虚拟环境中的3D化身中。我们的方法利用了知识提取，包括在公开的DEAP数据集上进行多标签分类，该数据集涵盖了效价、唤醒和支配作为主要情绪类别。值得注意的是，我们的蒸馏模型，一个只有两个卷积层、参数比教师模型少18倍的CNN，取得了有竞争力的结果，其准确率为87%，同时所需的计算资源要少得多。这种性能和可部署性之间的平衡使我们的框架成为虚拟环境系统的理想选择。此外，多标签分类结果被用于将情绪映射到定制设计的3D化身上。 et.al.|[2401.06957](http://arxiv.org/abs/2401.06957)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

