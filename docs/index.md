---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.02
> Usage instructions: [here](./docs/README.md#usage)

## MLLM

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2023-12-28**|**Structured Packing in LLM Training Improves Long Context Utilization**|长上下文大型语言模型（LCLMs）的最新进展引起了人们的极大兴趣，尤其是在查询科学研究论文等应用中。然而，它们的潜力往往受到上下文利用不足的限制。我们将典型训练数据中缺乏长程语义依赖性确定为主要障碍。为了解决这一问题，我们深入研究了经常将相关文档纳入培训输入的好处。使用代码数据的固有目录结构作为训练示例的来源，我们展示了困惑的改善，即使是对于与编码无关的任务也是如此。在这些发现的基础上，但着眼于更广泛的领域，我们引入了长上下文结构化包装（SPLiCe）。SPLiCe是一种创新的创建训练示例的方法，它使用检索方法将最相关的文档整理到单个训练上下文中。我们的结果表明，\方法｛｝提高了模型性能，可以用于训练大型模型更好地利用长上下文。我们通过训练一个价值30亿美元的大型模型来验证我们的结果，显示出困惑的改善和下游任务上更好的长上下文性能。 et.al.|[2312.17296](http://arxiv.org/abs/2312.17296)|null|
|**2023-12-27**|**PanGu- $π$: Enhancing Language Model Architectures via Nonlinearity Compensation**|大型语言模型（LLM）最近的趋势是增加模型大小（即参数的数量）和数据集的规模，以获得更好的生成能力，这一点已经被著名的GPT和Llama等许多工作所证明。然而，大型模型往往涉及巨大的计算成本，实际应用无法承受如此高的价格。然而，构建LLM的强模型体系结构的方法很少被讨论。我们首先分析了最先进的语言模型体系结构，并观察了特征崩溃问题。基于理论分析，我们提出非线性对于语言模型也非常重要，这通常在卷积神经网络中用于视觉任务。然后引入序列通知激活函数，并进行可以忽略的微小计算，并进一步使用增广快捷方式来增强模型的非线性。然后，我们证明了所提出的方法在通过精心设计的烧蚀来增强模型非线性方面是显著有效的；因此，我们提出了一种新的建立现代有效的模型体系结构，即PanGu-$\pi$。然后使用相同的数据集和训练策略进行实验，将PanGu-$\pi$与最先进的LLM进行比较。结果表明，PanGu-$\pi$-7B可以以大约10%的推理加速实现与基准测试相当的性能，PanGu-\pi$-1B可以在准确性和效率方面实现最先进的性能。此外，我们还在金融和法律的高价值领域部署了PanGu-$\pi$ -7B，开发了一个名为云山的LLM以供实际应用。结果表明，在基准测试中，云山模型可以超越其他尺度相似的模型。 et.al.|[2312.17276](http://arxiv.org/abs/2312.17276)|null|
|**2023-12-28**|**Multi-Prompts Learning with Cross-Modal Alignment for Attribute-based Person Re-Identification**|细粒度的属性描述可以显著地补充人物图像中有价值的语义信息，这对人物重新识别任务的成功至关重要。然而，当前的ReID算法通常无法有效利用可用的丰富上下文信息，主要是因为它们依赖于图像属性的简单和粗略利用。人工智能生成内容的最新进展使自动生成大量细粒度的属性描述并充分利用它们成为可能。因此，本文探索了在现有（大型）模型的ReID任务中使用生成的多人属性作为提示的潜力，以获得更准确的检索结果。为此，我们提出了一个新的框架，称为多提示ReID（MP ReID），基于提示学习和语言模型，以充分挖掘精细属性来帮助ReID任务。具体来说，MP ReID首先学会产生幻觉，产生各种各样的、信息丰富的、可提示的句子来描述查询图像。该程序包括（i）一个人具有哪些属性的明确提示，以及（ii）用于调整/调节用于该人身份匹配的标准的隐含可学习提示。显式提示是通过集合生成模型（如ChatGPT和VQA模型）来获得的。此外，还设计了一个对齐模块，以逐步融合多提示（即显式提示和隐式提示），并减轻跨模态间隙。在现有的涉及属性的ReID数据集，即Market1501和DukeMTMC ReID上进行的大量实验证明了所提出的MP ReID解决方案的有效性和合理性。 et.al.|[2312.16797](http://arxiv.org/abs/2312.16797)|null|
|**2023-12-27**|**Mobility and Cost Aware Inference Accelerating Algorithm for Edge Intelligence**|边缘智能近年来得到了广泛的应用。在设备、边缘服务器和云之间拆分模型可以大大提高EI的性能。先前的工作已经深入研究了没有用户移动性的模型分割。然而，在EI的大多数使用情况下，终端设备是移动的。在这方面只进行了少数工作。这些工作仍然存在许多问题，如忽视移动设备的能耗、网络假设不当、自适应用户移动性的有效性低等。因此，为了解决以往工作中模型分割和资源分配的不足，我们提出了移动和成本感知的模型分割和资源分配算法，以加速边缘推理（MCSA）。具体地，在没有用户移动性的场景中，提供了循环交互梯度下降（Li-GD）算法。当移动用户有较大的模型推理任务需要计算时，它会考虑移动用户的能耗、通信和计算资源租用成本以及推理延迟，以找到最优的模型分割和资源分配策略。在具有用户移动性的场景中，提出了移动感知Li-GD（MLi-GD）算法来计算最优策略。然后，研究了所提出算法的性质，包括收敛性、复杂度和近似率。实验结果验证了所提算法的有效性。 et.al.|[2312.16497](http://arxiv.org/abs/2312.16497)|null|
|**2023-12-30**|**Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding**|近年来，基于视图的三维形状识别方法的结果已经饱和，并且具有优异性能的模型由于其庞大的参数大小而无法部署在内存有限的设备上。为了解决这个问题，我们为该领域引入了一种基于知识蒸馏的压缩方法，该方法在尽可能保持模型性能的同时，大大减少了参数的数量。具体来说，为了增强较小模型的功能，我们设计了一个高性能的大型模型，称为Group Multi-view Vision Transformer（GMViT）。在GMViT中，视图级ViT首先建立视图级特征之间的关系。此外，为了捕捉更深层次的特征，我们使用分组模块将视图级特征增强为组级特征。最后，组级ViT将组级特征聚合为完整的、良好形成的3D形状描述符。值得注意的是，在这两个ViT中，我们都引入了相机坐标的空间编码作为创新的位置嵌入。此外，我们提出了两个基于GMViT的压缩版本，即GMViT simple和GMViT mini。为了提高小模型的训练有效性，我们在整个GMViT过程中引入了一种知识提取方法，其中每个GMViT组件的关键输出作为提取目标。大量实验证明了该方法的有效性。大模型GMViT在基准数据集ModelNet、ShapeNetCore55和MCB上实现了出色的3D分类和检索结果。较小的模型GMViT simple和GMViT mini分别将参数大小减少了8倍和17.6倍，形状识别速度平均提高了1.5倍，同时保持了至少90%的分类和检索性能。 et.al.|[2312.16477](http://arxiv.org/abs/2312.16477)|null|
|**2023-12-25**|**Revisiting Knowledge Distillation under Distribution Shift**|知识提炼将知识从大模型转移到小模型，最近取得了显著的成就。然而，很少有研究探讨知识蒸馏对抗分布转移的机制。分布偏移是指训练和测试阶段之间的数据分布漂移。在这篇文章中，我们通过重新表述转移情况下的目标函数来重新考虑知识提炼的范式。在真实场景下，我们提出了一个统一而系统的框架，以对照两种普遍的分布变化（包括多样性和相关性变化）来衡量知识提取。评估基准涵盖了五个基准数据集的30多种算法、数据驱动和优化方法。总的来说，我们对学生模型进行了广泛的实验。我们揭示了分布变化下教学表现不佳的有趣观察结果；特别是，复杂的算法和数据扩充在许多情况下提供了有限的增益。 et.al.|[2312.16242](http://arxiv.org/abs/2312.16242)|null|
|**2023-12-26**|**Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation**|参数有效微调（PEFT）方法为使大型视觉语言模型适应特定任务或场景提供了一种有效的方法。通常，他们在白盒公式中为预先训练的模型学习非常小范围的参数，该公式假设模型架构是已知的，参数是可访问的。然而，出于防止滥用或商业因素的考虑，大型模型往往不是开源的，因此对白盒PEFT方法的部署构成了障碍。为了减轻对模型可访问性的依赖，我们引入了协作黑盒调优（CBBT），用于黑盒模型的文本提示优化和输出特征自适应。具体来说，考虑到反向传播梯度被阻塞，我们通过分析具有扰动提示的预测来近似文本提示的梯度。其次，在不可访问模型的输出特性上部署了一个轻量级适配器，进一步方便了模型的自适应过程。有了这些设计，我们的CBBT在11个下游基准上进行了广泛评估，与现有的黑盒VL自适应方法相比，取得了显著的改进。代码发布于https://github.com/guozix/cbbt. et.al.|[2312.15901](http://arxiv.org/abs/2312.15901)|**[link](https://github.com/guozix/cbbt)**|
|**2023-12-26**|**High Efficiency Inference Accelerating Algorithm for NOMA-based Mobile Edge Computing**|在设备、边缘服务器和云之间拆分推理模型可以大大提高EI的性能。此外，非正交多址（NOMA）作为B5G/6G的关键支持技术，可以实现大规模连接和高频谱效率。受NOMA优点的启发，将NOMA与MEC中的模型分割相结合以减少推理延迟变得更有吸引力。然而，在以前的工作中，没有适当考虑分裂推理过程中基于NOMA的通信。因此，在本文中，我们将NOMA集成到MEC中的分裂推理中，并提出了有效的通信和计算资源分配算法来加速边缘的模型推理。具体来说，当移动用户在基于NOMA的MEC中有大量的模型推理任务需要计算时，它将考虑设备和边缘服务器的能耗以及推理延迟，以找到最佳的模型分割策略、子信道分配策略（上行链路和下行链路）和传输功率分配策略（下行链路和上行链路）。由于不能同时满足最小推理延迟和能量消耗，并且子信道分配和模型分割的变量是离散的，因此采用梯度下降算法来寻找它们之间的最优折衷。此外，还提出了循环迭代GD方法（Li-GD），以降低参数离散引起的GD算法的复杂度。此外，还研究了所提出的算法的性质，证明了所提出算法的有效性。 et.al.|[2312.15850](http://arxiv.org/abs/2312.15850)|null|
|**2023-12-25**|**IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models**|大型语言模型（LLM），如ChatGPT，在各种任务中表现出了令人印象深刻的功能，并作为跨许多领域的自然语言接口吸引了越来越多的兴趣。最近，像BLIP-2和GPT-4这样的大型视觉语言模型（VLM）被深入研究，它们从图像-文本对中学习丰富的视觉语言相关性。然而，尽管有这些发展，LLM和VLM在图像质量评估（IQA）中的应用，特别是在医学成像中的应用仍有待探索，这对于客观的性能评估和放射科医生意见的潜在补充甚至替代是有价值的。为此，本文介绍了IQAGPT，这是一种创新的图像质量评估系统，将图像质量字幕VLM与ChatGPT集成在一起，用于生成质量分数和文本报告。首先，我们构建了一个用于训练和评估的CT-IQA数据集，包括1000个具有不同质量水平的专业注释的CT切片。为了更好地利用LLM的功能，我们使用提示模板将带注释的质量分数转换为语义丰富的文本描述。其次，我们对CT-IQA数据集上的图像质量字幕VLM进行微调，以生成质量描述。字幕模型通过跨模态注意力融合了图像和文本特征。第三，基于质量描述，用户可以与ChatGPT对话，对图像质量分数进行评分或生成放射质量报告。我们的初步结果证明了用大型模型评估图像质量的可行性。值得注意的是，我们的IQAGPT优于GPT-4和CLIP-IQA，以及仅依赖图像的多任务分类和回归模型。 et.al.|[2312.15663](http://arxiv.org/abs/2312.15663)|null|
|**2023-12-24**|**Fairness-Aware Structured Pruning in Transformers**|大型语言模型（LLM）的规模不断扩大，给它们的训练和推理带来了挑战。删除模型组件被视为解决大模型大小的解决方案，然而，现有的修剪方法只关注性能，而没有考虑LLM的负责任使用的一个重要方面：模型公平性。至关重要的是，要解决LLM对不同群体的公平性问题，如妇女、黑人、LGBTQ+、犹太社区等，因为LLM正在部署并向广大受众开放。在这项工作中，首先，我们研究了在预先训练的基于transformer的语言模型中，注意力头如何影响公平性和性能。然后，我们提出了一种新的方法来修剪对公平性产生负面影响的注意力头部，同时保留对性能至关重要的头部，即语言建模能力。我们的方法在时间和资源方面是实用的，因为它不需要对最终修剪过的、更公平的模型进行微调。我们的研究结果表明，与有偏见的模型相比，两种不同大小的DistilGPT-2、GPT-2和GPT-Neo模型（GPT-J和Llama 2模型）的性别偏见分别减少了19%、19.5%、39.5%、34.7%、23%和8%，性能仅略有下降。 et.al.|[2312.15398](http://arxiv.org/abs/2312.15398)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2023-12-27**|**RefineNet: Enhancing Text-to-Image Conversion with High-Resolution and Detail Accuracy through Hierarchical Transformers and Progressive Refinement**|在这项研究中，我们介绍了RefineNet，这是一种新颖的架构，旨在解决文本到图像转换系统中的分辨率限制问题。我们探讨了从文本描述生成高分辨率图像的挑战，重点是细节准确性和计算效率之间的权衡。RefineNet利用分层Transformer与渐进和条件细化技术相结合，在生成详细和高质量的图像方面优于现有模型。通过在不同数据集上进行广泛的实验，我们展示了RefineNet在清晰度和分辨率方面的优势，特别是在动物、植物和人脸等复杂图像类别中。我们的工作不仅推进了图像到文本转换领域，而且为各种应用中的高保真图像生成开辟了新的途径。 et.al.|[2312.17274](http://arxiv.org/abs/2312.17274)|null|
|**2023-12-28**|**Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action**|我们提出了Unified IO 2，这是第一个能够理解和生成图像、文本、音频和动作的自回归多模式模型。为了统一不同的模态，我们将输入和输出（图像、文本、音频、动作、边界框等）标记到共享的语义空间中，然后使用单个编码器-解码器-转换器模型对其进行处理。由于使用如此多样化的模式进行训练是具有挑战性的，我们提出了各种架构改进来稳定模型训练。我们在来自不同来源的大型多模式预训练语料库上从头开始训练我们的模型，目标是多模式混合去噪器。为了学习一套广泛的技能，例如遵循多模式指令，我们通过提示和增强在120个数据集的集合上构建和微调。通过单一的统一模型，unified IO 2在GRIT基准测试中实现了最先进的性能，并在超过35个基准测试中取得了优异的成绩，包括图像生成和理解、自然语言理解、视频和音频理解以及机器人操作。我们向研究界发布所有模型。 et.al.|[2312.17172](http://arxiv.org/abs/2312.17172)|null|
|**2023-12-27**|**Prompt Expansion for Adaptive Text-to-Image Generation**|文本到图像生成模型功能强大，但难以使用。用户制作特定的提示以获得更好的图像，尽管图像可能是重复的。本文提出了一个Prompt Expansion框架，帮助用户用更少的精力生成高质量、多样化的图像。Prompt Expansion模型将文本查询作为输入，并输出一组经过优化的扩展文本提示，以便在传递到文本到图像模型时，生成更广泛的吸引人的图像。我们进行了一项人类评估研究，该研究表明，通过Prompt Expansion生成的图像比通过基线方法生成的图像更具美感和多样性。总的来说，本文提出了一种新颖有效的方法来改善文本到图像的生成体验。 et.al.|[2312.16720](http://arxiv.org/abs/2312.16720)|null|
|**2023-12-27**|**I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models**|在数字内容生成的快速发展领域，焦点已经从文本到图像（T2I）模型转移到更先进的视频扩散模型，尤其是文本到视频（T2V）和图像到视频（I2V）。本文解决了I2V带来的复杂挑战：将静态图像转换为动态、逼真的视频序列，同时保持原始图像的保真度。传统的方法通常包括将整个图像集成到扩散过程中，或者使用预训练的编码器进行交叉关注。然而，这些方法往往需要改变T2I模型的基本权重，从而限制了它们的可重用性。我们介绍了一种新的解决方案，即I2V适配器，旨在克服这些限制。我们的方法保留了T2I模型及其固有运动模块的结构完整性。I2V适配器通过使用轻量级适配器模块与输入图像并行处理带噪视频帧来进行操作。该模块充当桥梁，有效地将输入链接到模型的自注意机制，从而在不需要对T2I模型进行结构更改的情况下保持空间细节。此外，I2V适配器只需要传统模型的一小部分参数，并确保与现有社区驱动的T2I模型和控制工具的兼容性。我们的实验结果证明了I2V适配器能够产生高质量的视频输出。这种性能，加上其多功能性和对可训练参数的需求减少，代表着人工智能驱动的视频生成领域的重大进步，尤其是在创意应用领域。 et.al.|[2312.16693](http://arxiv.org/abs/2312.16693)|null|
|**2023-12-27**|**Participatory prompting: a user-centric research method for eliciting AI assistance opportunities in knowledge workflows**|生成型人工智能，如图像生成模型和大型语言模型，将在创意和知识工作流程中为最终用户程序员提供巨大价值。目前的研究方法很难让最终用户参与到一场现实的对话中，以平衡生成人工智能的实际现有能力与用户工作流程的开放性以及该技术应用的许多机会。在这篇正在进行的工作中，我们介绍了参与式提示，这是一种在最终用户工作流程中为生成人工智能寻找机会的方法。参与式提示方法将情境探究和研究者介导的互动与生成模型相结合，这有助于研究参与者与生成模型互动，而不必制定自己的提示策略。我们讨论了一项正在进行的研究的进展，该研究的目的是确定数据分析工作流程中生成人工智能的最终用户编程机会。 et.al.|[2312.16633](http://arxiv.org/abs/2312.16633)|null|
|**2023-12-27**|**A Non-Uniform Low-Light Image Enhancement Method with Multi-Scale Attention Transformer and Luminance Consistency Loss**|微光图像增强旨在提高在昏暗环境中采集的图像的感知能力，并为图像识别任务提供高质量的数据支持。在处理非均匀光照下拍摄的照片时，现有的方法无法自适应地提取差异化的亮度信息，容易导致曝光过度和曝光不足。从无监督学习的角度来看，我们提出了一种名为MSATr的多尺度注意力转换器，该转换器充分提取局部和全局特征以实现光平衡，从而提高视觉质量。具体来说，我们提出了一种多尺度窗口划分方案，该方案使用指数序列来调整每一层的窗口大小。在不同大小的窗口内，可以细化自关注计算，确保模型的像素级特征处理能力。对于跨窗口的特征交互，构建了全局变换器分支，以提供全面的亮度感知并缓解曝光问题。此外，我们提出了一种循环训练策略，利用加权混合生成的不同图像和亮度一致性损失来有效提高模型的泛化能力。在几个基准数据集上进行的大量实验定量和定性地证明，我们的MSATr优于最先进的微光图像增强方法，增强后的图像具有更自然的亮度和突出的细节。代码发布于https://github.com/fang001021/MSATr. et.al.|[2312.16498](http://arxiv.org/abs/2312.16498)|**[link](https://github.com/fang001021/msatr)**|
|**2023-12-29**|**PanGu-Draw: Advancing Resource-Efficient Text-to-Image Synthesis with Time-Decoupled Training and Reusable Coop-Diffusion**|当前的大规模扩散模型代表了条件图像合成的巨大飞跃，能够解释文本、人体姿势和边缘等各种线索。然而，它们对大量计算资源和广泛数据收集的依赖仍然是一个瓶颈。另一方面，由于图像分辨率和潜在空间嵌入结构不兼容，阻碍了它们的联合使用，现有扩散模型的集成带来了挑战，每个扩散模型专门用于不同的控制，并在独特的潜在空间中运行。针对这些限制，我们提出了“PanGu Draw”，这是一种新的潜在扩散模型，旨在实现资源高效的文本到图像合成，能够熟练地适应多个控制信号。我们首先提出了一种资源高效的时间解耦训练策略，该策略将单片文本到图像模型拆分为结构生成器和纹理生成器。每个生成器都使用一种方案进行训练，该方案可以最大限度地提高数据利用率和计算效率，将数据准备减少48%，并将训练资源减少51%。其次，我们介绍了“Coop Diffusion”，这是一种算法，能够在统一的去噪过程中合作使用具有不同潜在空间和预定义分辨率的各种预先训练的扩散模型。这允许以任意分辨率进行多控制图像合成，而不需要额外的数据或重新训练。Pangu Draw的经验验证显示了其在文本到图像和多控件图像生成方面的非凡能力，为未来的模型训练效率和生成多功能性提供了一个有希望的方向。最大的5B T2I PanGu Draw模型在Ascend平台上发布。项目页面： $\href{https://pangu-draw.github.io}｛this~https~URL｝$ et.al.|[2312.16486](http://arxiv.org/abs/2312.16486)|null|
|**2023-12-27**|**Bellman Optimal Step-size Straightening of Flow-Matching Models**|流匹配是在各种应用中生成高质量样本的强大框架，尤其是在图像合成中。然而，这些模型的密集计算需求，特别是在微调过程和采样过程中，对低资源场景提出了重大挑战。本文介绍了用于提取流匹配生成模型的Bellman最优步长校正（BOSS）技术：它专门针对在遵守计算预算约束的情况下进行几步有效的图像采样。首先，该技术涉及一种动态规划算法，该算法优化预训练网络的步长。然后，它对速度网络进行细化，以匹配最佳步长，旨在拉直生成路径。对图像生成任务的广泛实验评估证明了BOSS在资源利用率和图像质量方面的有效性。我们的研究结果表明，BOSS在保持有竞争力的样本质量的同时，实现了显著的效率提升，有效地弥合了低资源约束和流匹配生成模型的苛刻要求之间的差距。我们的论文还加强了人工智能的负责任开发，提供了一个更可持续的生成模型，减少了计算成本和环境足迹。我们的代码可以在https://anonymous.4open.science/r/DRL-8E88. et.al.|[2312.16414](http://arxiv.org/abs/2312.16414)|null|
|**2023-12-26**|**SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation**|主题驱动图像生成的最新进展导致了零样本生成，但精确选择和关注关键主题表示仍然具有挑战性。针对这一点，我们介绍了SSR编码器，这是一种新颖的架构，旨在从单个或多个参考图像中选择性地捕捉任何对象。它响应包括文本和掩码在内的各种查询模式，而无需对测试时间进行微调。SSR编码器结合了将查询输入与图像补丁对齐的令牌到补丁对齐器和用于提取和保存主题的精细特征的细节保留主题编码器，从而生成主题嵌入。这些嵌入与原始文本嵌入一起使用，为生成过程提供条件。SSR编码器具有模型通用性和效率的特点，适用于一系列自定义模型和控制模块。通过改进训练的嵌入一致性正则化损失增强，我们的大量实验证明了它在多功能和高质量图像生成中的有效性，表明它具有广泛的适用性。项目页面：https://ssr-encoder.github.io et.al.|[2312.16272](http://arxiv.org/abs/2312.16272)|null|
|**2023-12-26**|**One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications**|用于文本到图像生成的商业和开源扩散模型（DM）的普遍使用促使风险缓解以防止不期望的行为。学术界现有的概念擦除方法都是基于全参数或基于规范的微调，从中我们观察到以下问题：1）向侵蚀的生成交替：目标消除过程中的参数漂移导致所有生成的交替和潜在变形，甚至不同程度地侵蚀其他概念，这一点在多概念消除后更为明显；2） 转移能力和部署效率低下：以前的特定于模型的擦除阻碍了概念的灵活组合和向其他模型的无训练转移，导致随着部署场景的增加，成本线性增长。为了实现非侵入性、精确、可定制和可转移的消除，我们将擦除框架建立在一维适配器上，以便在多功能擦除应用程序中同时从大多数DM中擦除多个概念。将概念半渗透结构作为膜（SPM）注入到任何DM中，以学习有针对性的擦除，同时通过一种新的潜在锚定微调策略有效地缓解了变化和侵蚀现象。一旦获得，SPM可以灵活组合，并为其他DM即插即用，而无需特定的重新调整，从而能够及时有效地适应不同的场景。在生成过程中，我们的促进传输机制动态调节每个SPM的渗透率，以响应不同的输入提示，进一步将对其他概念的影响降至最低。40个概念、7个DM和4个擦除应用程序的定量和定性结果证明了SPM的卓越擦除。我们的代码和预调SPM将在项目页面上提供https://lyumengyao.github.io/projects/spm. et.al.|[2312.16145](http://arxiv.org/abs/2312.16145)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

