---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.31
> Usage instructions: [here](./docs/README.md#usage)

## avatar

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**Democratizing the Creation of Animatable Facial Avatars**|在高端视觉效果管道中，（通常）使用定制的（且昂贵的）光舞台系统来扫描演员，以获取各种表情的几何形状和纹理。为了实现民主化，我们提出了一种新的管道，用于获得几何体和纹理以及足够的表达信息，以在不使用灯光舞台或任何其他高端硬件（或手动清理）的情况下构建定制的特定于个人的动画装备。一个关键的新颖想法包括扭曲真实世界的图像以与模板化身的几何形状对齐，并随后将扭曲的图像投影到模板化身的纹理中；重要的是，这使我们能够利用真实世界中的烘焙照明/纹理信息来创建替代面部特征（并弥合领域差距），以便进行几何重建。我们的方法不仅可以用于获得中性的表达几何体和去光纹理，而且还可以用于在将化身导入动画系统后改进化身（注意，这种导入往往是有损的，同时也会产生各种特征的幻觉）。由于默认的动画装备将包含与特定个体的模板表达式不正确对应的模板表达式，因此我们使用Simon Says方法来捕捉各种表达式并构建特定于个人的动画装备（像它们一样移动）。我们前面提到的翘曲/投影方法具有足够高的效率来重建对应于每个表达式的几何结构。 et.al.|[2401.16534](http://arxiv.org/abs/2401.16534)|null|
|**2024-01-27**|**AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**|最近的社区在从稀疏的多视图视频构建照片逼真的可动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色呈现逼真的服装动态，因为它们主要依赖裸体模型进行人体建模，同时保留未建模的服装部分。这主要是因为宽松服装产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为4-8）生成宽松衣服中的可动画化人类化身的新方法。为了能够在这种情况下捕捉和学习宽松服装的外观，我们使用了从基于物理的模拟数据中获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕捉和渲染复杂的服装动力学。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动力学的新方法。为了使用粗略估计为看不见的衣服状态建立逼真的渲染，引入了一个以身体和衣服运动为条件的姿势驱动的可变形神经辐射场，提供了对这两个部分的显式控制。在测试时，可以从看不见的情况中捕捉新的服装姿势，这些姿势来自基于物理或神经网络的模拟器，以驱动看不见服装的动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松、动作各异的表演者。实验表明，我们的方法能够呈现出与身体高度偏离的自然服装动力学，并很好地推广到看不见的视图和姿势，超过了现有方法的性能。代码和数据将公开。 et.al.|[2401.15348](http://arxiv.org/abs/2401.15348)|null|
|**2024-01-10**|**A General-purpose AI Avatar in Healthcare**|机器学习和自然语言处理的最新进展导致人工智能（AI）作为医疗保健行业的一种宝贵工具得到了快速发展。使用大型语言模型（LLM）作为会话代理或聊天机器人有可能帮助医生诊断患者，检测疾病的早期症状，并为患者提供健康建议。本文重点关注聊天机器人在医疗保健中的作用，并探索使用化身使人工智能交互对患者更具吸引力。通过使用三类提示字典和提示改进机制，展示了通用AI化身应用程序的框架。建议采用两阶段方法来微调通用人工智能语言模型，并创建不同的人工智能化身，与用户讨论医疗问题。即时工程增强了聊天机器人的对话能力和个性特征，培养了与患者更人性化的互动。最终，在聊天机器人中注入个性可能会增加患者的参与度。未来的研究方向包括研究如何提高聊天机器人对上下文的理解，并通过对专业医疗数据集进行微调来确保其输出的准确性。 et.al.|[2401.12981](http://arxiv.org/abs/2401.12981)|null|
|**2024-01-23**|**GALA: Generating Animatable Layered Assets from a Single Scan**|我们提出了GALA，这是一个框架，它以单层衣服的3D人体网格为输入，并将其分解为完整的多层3D资产。然后可以将输出与其他资产组合，以创建具有任何姿势的新颖的穿着衣服的人类化身。现有的重建方法通常将穿着衣服的人类视为单层几何体，并忽略了人类与发型、衣服和配饰的固有组成性，从而限制了网格在下游应用中的效用。将单层网格分解为单独的层是一项具有挑战性的任务，因为它需要为严重遮挡的区域合成合理的几何结构和纹理。此外，即使分解成功，网格也不能在姿势和体型方面进行归一化，从而无法实现具有新身份和姿势的连贯合成。为了应对这些挑战，我们建议利用预训练的2D扩散模型的一般知识作为人类和其他资产的几何和外观先验。我们首先使用从多视图2D分割中提取的3D表面分割来分离输入网格。然后，我们使用一种新的姿态引导的分数蒸馏采样（SDS）损失来合成姿态空间和规范空间中不同层的缺失几何。一旦我们完成了高保真3D几何体的修复，我们还将相同的SDS损失应用于其纹理，以获得包括初始遮挡区域在内的完整外观。通过一系列分解步骤，我们在一个共享的规范空间中获得了多层3D资产，这些资产根据姿势和人体形状进行了归一化，从而支持轻松合成新的身份，并用新的姿势进行复活。我们的实验证明了与现有解决方案相比，我们的方法在分解、规范化和组合任务方面的有效性。 et.al.|[2401.12979](http://arxiv.org/abs/2401.12979)|null|
|**2024-01-30**|**PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting**|尽管取得了很大进展，但实现实时高保真度的头部化身动画仍然很困难，并且现有的方法必须在速度和质量之间进行权衡。基于3DMM的方法通常无法对眼镜和发型等非面部结构进行建模，而神经隐式模型则存在变形不灵活和渲染效率低下的问题。尽管3D高斯已经被证明具有很好的几何表示和辐射场重建能力，但在头部化身创建中应用3D高斯仍然是一个主要挑战，因为3D高斯很难对由姿势和表情变化引起的头部形状变化进行建模。在本文中，我们介绍了PSAvatar，这是一种新的可动画化头部化身创建框架，它利用离散几何图元创建参数可变形形状模型，并使用3D高斯进行精细细节表示和高保真渲染。参数变形形状模型是一种基于点的变形形状模型（PMSM），它使用点代替网格进行三维表示，以实现增强的表示灵活性。PMSM首先通过在表面和网格外采样将FLAME网格转换为点，以不仅能够重建表面状结构，而且能够重建复杂的几何形状，如眼镜和发型。通过以综合分析的方式将这些点与头部形状对齐，PMSM可以利用3D高斯进行精细的细节表示和外观建模，从而能够创建高保真化身。我们展示了PSAvatar可以重建各种主题的高保真头部化身，并且化身可以实时动画化（以512 $\times$512的分辨率$\ge$ 25fps）。 et.al.|[2401.12900](http://arxiv.org/abs/2401.12900)|**[link](https://github.com/pcl3dv/PSAvatar)**|
|**2024-01-26**|**New spectral-parameter dependent solutions of the Yang-Baxter equation**|杨-巴克斯特方程（YBE）在研究可积多体量子系统中起着至关重要的作用。许多已知的YBE解决方案提供了从量子自旋链到超导系统的各种例子。可解统计力学模型及其化身也基于YBE。因此，YBE的新解决方案可以用于构建新的有趣的1D量子或2D经典系统，并具有许多其他深远的应用。在这项工作中，我们试图在对应于两个量子位情况的最低维度上找到YBE的（几乎）穷举的解集。我们开发了一种算法，该算法有可能用于生成YBE的新的高维解。 et.al.|[2401.12710](http://arxiv.org/abs/2401.12710)|null|
|**2024-01-20**|**UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures**|3D化身生成的最新进展已经引起了人们的极大关注。这些突破旨在制作更逼真的可动画化化身，缩小虚拟体验和现实世界体验之间的差距。大多数现有的工作都采用了分数蒸馏采样（SDS）损失，结合可微分的渲染器和文本条件，来指导扩散模型生成3D化身。然而，SDS通常生成的结果过于平滑，面部细节很少，因此与祖先采样相比缺乏多样性。另一方面，其他作品从单个图像生成3D化身，其中不想要的光照效果、透视图和较差的图像质量的挑战使得它们难以可靠地重建具有对齐的完整纹理的3D面部网格。在本文中，我们提出了一种新的3D化身生成方法，称为UltrAvatar，该方法具有增强的几何逼真度和卓越的基于物理的渲染（PBR）纹理质量，而没有不需要的照明。为此，该方法提出了一种漫射颜色提取模型和真实性引导的纹理漫射模型。前者去除了不需要的照明效果，以显示真实的漫反射颜色，从而可以在各种照明条件下渲染生成的化身。后者遵循两种基于梯度的指导，用于生成PBR纹理，以更好地与3D网格几何体对齐，呈现不同的人脸身份特征和细节。我们证明了所提出的方法的有效性和稳健性，在实验中大大优于最先进的方法。 et.al.|[2401.11078](http://arxiv.org/abs/2401.11078)|null|
|**2024-01-19**|**Fast Registration of Photorealistic Avatars for VR Facial Animation**|虚拟现实（VR）展示了社交互动的前景，这种互动比其他媒体更具沉浸感。其中的关键是能够在佩戴VR耳机的情况下准确地为自己肖像的真实感化身制作动画。尽管在离线设置中可以将特定于个人的化身高质量地注册到头戴式摄像机（HMC）图像，但通用实时模型的性能显著降低。由于相机视角倾斜和模态差异，在线注册也具有挑战性。在这项工作中，我们首先表明，化身和头戴式耳机相机图像之间的域间隙是困难的主要来源之一，其中基于转换器的架构在域一致性数据上实现了高精度，但当重新引入域间隙时会降低。基于这一发现，我们开发了一种系统设计，将问题解耦为两个部分：1）一个接受域输入的迭代细化模块，以及2）一个基于表情和头部姿势的当前估计的通用化身引导的图像到图像风格传递模块。这两个模块相互加强，因为当显示接近真实的例子时，图像风格的转移变得更容易，而更好的域间隙去除有助于配准。我们的系统可以高效地产生高质量的结果，从而无需昂贵的离线注册来生成个性化标签。我们通过在商品耳机上进行的大量实验验证了我们的方法的准确性和效率，证明了与直接回归方法和离线注册相比的显著改进。 et.al.|[2401.11002](http://arxiv.org/abs/2401.11002)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Tri $^{2}$-plane: Volumetric Avatar Reconstruction with Feature Pyramid**|近年来，在利用神经体积绘制进行面部化身重建方面取得了相当大的成就。尽管取得了显著的进步，但从单眼视频中重建复杂而动态的头部运动仍然需要捕捉和恢复细粒度的细节。在这项工作中，我们提出了一种新的方法，命名为Tri$^2$-plane，用于单目照片逼真的体积头部化身重建。与现有的依赖于单个三平面变形场进行动态面部建模的工作不同，所提出的tri$^2$-平面利用了特征金字塔和三个上下横向连接三平面的原理来改进细节。它在多个尺度上采样和渲染面部细节，从整个面部过渡到特定的局部区域，然后过渡到更精细的子区域。此外，我们在训练中加入了一种基于相机的几何感知滑动窗口方法作为增强，它提高了规范空间之外的鲁棒性，特别提高了交叉身份生成能力。实验结果表明，Tri$^2$ -平面不仅超越了现有的方法，而且通过实验在定量指标和定性评估方面都取得了卓越的性能。 et.al.|[2401.09386](http://arxiv.org/abs/2401.09386)|**[link](https://github.com/songluchuan/tri2plane)**|
|**2024-01-20**|**Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**|一次拍摄3D会说话的肖像生成旨在从看不见的图像中重建3D化身，然后用参考视频或音频将其动画化，以生成会说话的人像视频。现有的方法无法同时实现准确的三维化身重建和稳定的人脸动画。此外，虽然现有的作品主要集中在合成头部，但生成自然的躯干和背景片段以获得逼真的说话肖像视频也是至关重要的。为了解决这些限制，我们提出了Real3D Potrait，该框架（1）通过从3D人脸生成模型中提取3D先验知识的大图像到平面模型提高了单次3D重建能力；（2） 利用高效的运动适配器促进精确的运动条件动画；（3） 使用头部-躯干背景超分辨率模型来合成具有自然躯干运动和可切换背景的逼真视频；以及（4）支持具有可推广的音频到运动模型的单镜头音频驱动的谈话面部生成。大量实验表明，与以前的方法相比，Real3D Portrait很好地概括了看不见的身份，并生成了更逼真的谈话肖像视频。视频样本和源代码可在https://real3dportrait.github.io . et.al.|[2401.08503](http://arxiv.org/abs/2401.08503)|null|
|**2024-01-13**|**EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation**|随着虚拟环境的不断发展，对沉浸式和情感化体验的需求也在增长。为了满足这一需求，我们引入了使用优化知识提取（EVOKE）实现情感的虚拟化身映射，这是一种轻量级的情感识别框架，旨在将情感识别无缝集成到虚拟环境中的3D化身中。我们的方法利用了知识提取，包括在公开的DEAP数据集上进行多标签分类，该数据集涵盖了效价、唤醒和支配作为主要情绪类别。值得注意的是，我们的蒸馏模型，一个只有两个卷积层、参数比教师模型少18倍的CNN，取得了有竞争力的结果，其准确率为87%，同时所需的计算资源要少得多。这种性能和可部署性之间的平衡使我们的框架成为虚拟环境系统的理想选择。此外，多标签分类结果被用于将情绪映射到定制设计的3D化身上。 et.al.|[2401.06957](http://arxiv.org/abs/2401.06957)|null|
|**2024-01-10**|**Analysis and Perspectives on the ANA Avatar XPRIZE Competition**|ANA化身XPRIZE是一项为期四年的竞赛，旨在开发一种机器人“化身”系统，使人类操作员能够在远程环境中感知、交流和行动，就好像身体存在一样。比赛有一个独特的要求，即评委在人机界面上进行不到一个小时的培训后，将操作化身，并根据客观和主观评分标准对化身系统进行评判。本文从技术、评判和组织的角度对竞争进行了统一的总结和分析。我们研究了远程机器人技术的使用以及参赛团队在其化身系统中追求的创新，并将这些技术的使用与评委的任务表现和主观调查评分相关联。它还总结了团队领导、评委和组织者对比赛执行和影响的看法，为远程机器人和远程呈现的未来发展提供信息。 et.al.|[2401.05290](http://arxiv.org/abs/2401.05290)|null|
|**2024-01-09**|**A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars**|本文的目的是开发一个将口语翻译成手语的功能系统，称为Spoken2Sign翻译。Spoken2Sign任务与传统的手语到口语（Sign2Spoken）翻译是正交和互补的。为了实现Spoken2Sign翻译，我们提出了一个简单的基线，包括三个步骤：1）使用现有的Sign2Spoken基准创建一个光泽视频词典；2） 为字典中的每个符号视频估计3D符号；3） 借助生成的gloss-3D符号字典，训练Spoken2Sign模型，该模型由Text2Gloss翻译器、符号连接器和渲染模块组成。翻译结果然后通过标志化身来显示。据我们所知，我们是第一个以3D符号的输出格式呈现Spoken2Sign任务的公司。除了Spoken2Sign翻译的能力外，我们还证明了我们的方法的两个副产品——三维关键点增强和多视图理解——可以帮助实现基于关键点的手语理解。代码和型号将在https://github.com/FangyunWei/SLRT et.al.|[2401.04730](http://arxiv.org/abs/2401.04730)|**[link](https://github.com/FangyunWei/SLRT)**|
|**2024-01-09**|**Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation**|生成扩散模型的最新进展已经实现了从单个输入图像或文本提示生成3D资产的先前不可行的能力。在这项工作中，我们的目标是提高这些模型的质量和功能，以完成创建可控、照片真实感的人类化身的任务。我们通过将3D可变形模型集成到最先进的多视角一致扩散方法中来实现这一点。我们证明了生成管道在关节式3D模型上的精确调节增强了基线模型在从单个图像合成新视图任务中的性能。更重要的是，这种集成有助于将面部表情和身体姿势控制无缝准确地结合到生成过程中。据我们所知，我们提出的框架是第一个扩散模型，能够从看不见的物体的单个图像中创建完全3D一致、可动画化和照片真实感的人类化身；大量的定量和定性评估证明了我们的方法在新视角和新表情合成任务上优于现有的最先进的化身创建模型。 et.al.|[2401.04728](http://arxiv.org/abs/2401.04728)|null|
|**2024-01-08**|**AKN_Regie: bridging digital and performing arts**|AvatarStaging框架包括在混合戏剧舞台上指挥化身，实现物理演员的物质性和由动作捕捉或特定动画播放器实时控制的化身的虚拟性之间的共同存在。它导致了AKN_Regie创作工具的实现，该工具使用蓝图视觉语言编程，作为虚幻引擎（UE）视频游戏引擎的插件。本文描述了AKN_Regie作为非程序员戏剧人的工具的主要功能。它以UE特有的蓝图可视化语言提供了其实现的见解。它详细介绍了该工具是如何随着其在大约十部戏剧作品中的使用而演变的。讨论了AKN_Regie的非编程观点Plugin Perspective和编程文化对其发展的适应Blueprint Perspective之间的循环过程。最后，建议从C++的角度来加强技术问题的文化挪用，弥合深深涉及人类物质性的表演艺术和邀请发现新世界的化身之间的差距。 et.al.|[2401.03761](http://arxiv.org/abs/2401.03761)|null|
|**2024-01-07**|**Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness**|当前的说话化身大多基于说话的音频和文本生成共同说话手势，而不考虑说话者的非说话动作。此外，先前关于协同语音手势生成的工作已经基于单个手势数据集设计了网络结构，这导致数据量有限、可推广性受损和说话者运动受限。为了解决这些问题，我们引入了FreeTalker，据我们所知，它是第一个生成自发（例如，共同发言手势）和非自发（例如在讲台上移动）说话者动作的框架。具体来说，我们训练了一个基于扩散的说话人运动生成模型，该模型利用来自各种运动数据集的异构数据，采用语音驱动手势和文本驱动运动的统一表示。在推理过程中，我们利用无分类器引导来高度控制剪辑中的风格。此外，为了在片段之间创建平滑的过渡，我们使用DoubleTake，这是一种利用生成先验并确保无缝运动混合的方法。大量实验表明，我们的方法可以产生自然可控的扬声器运动。我们的代码、模型和演示可在\url上获得{https://youngseng.github.io/FreeTalker/}. et.al.|[2401.03476](http://arxiv.org/abs/2401.03476)|null|
|**2024-01-07**|**Deformation of a planar ferromagnetic elastic ribbon**|虽然对纯弹性带进行了广泛的研究，但在本文中，我们探讨了磁化对平面铁磁弹性带变形的影响。我们通过推导与弯曲的平面铁磁弹性带相关的前导阶磁能来开始研究。磁能和弹性能之和就是带状物的总能量。我们通过取总能量的第一个变化来导出平衡方程。然后，我们系统地确定和分析这些平衡方程在各种规范边界条件下的解。我们还分析了平衡解的稳定性。将我们的发现与研究充分的欧拉弹性进行比较，可以深入了解磁性对弹性带变形行为的影响。我们的分析有助于更深入地理解平面铁磁结构的磁化和机械响应之间的相互作用，并为理论和实际应用提供有价值的见解。 et.al.|[2401.03447](http://arxiv.org/abs/2401.03447)|null|
|**2024-01-09**|**Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system**|人工智能和机器人技术在过去十年中取得了显著进步，改变了各个领域的工作模式和机会。这些技术的应用将社会推向了一个人与机器共生的时代。为了促进人类与智能机器人之间的高效通信，我们提出了“阿凡达”系统，这是一个沉浸式低延迟全景人机交互平台。我们设计并测试了一个坚固的移动平台原型，该平台集成了边缘计算单元、全景视频捕获设备、动力电池、机械臂和网络通信设备。在良好的网络条件下，我们实现了延迟357ms的低延迟高清全景视觉体验。操作员可以利用VR耳机和控制器对机器人和设备进行实时沉浸式控制。该系统能够实现跨越校园、省份、国家甚至大洲（纽约到深圳）的远距离远程控制。此外，该系统结合了用于地图和轨迹记录的视觉SLAM技术，提供了自主导航功能。我们相信，这个直观的系统平台可以提高人机协作的效率和情景体验，随着相关技术的进一步进步，它将成为人工智能与人类高效共生合作的通用工具。 et.al.|[2401.03398](http://arxiv.org/abs/2401.03398)|null|
|**2024-01-06**|**Dress-Me-Up: A Dataset & Method for Self-Supervised 3D Garment Retargeting**|我们提出了一种新的自监督框架，用于将非参数化的3D服装重新定位到任意形状和姿势的3D人体化身上，从而实现3D虚拟试穿（VTON）。现有的自监督3D重定目标方法仅支持参数化和规范化服装，这些服装只能覆盖在参数化身体上，例如SMPL。为了便于非参数化的服装和身体，我们提出了一种新的方法，该方法引入了基于Isomap嵌入的服装和人体之间的对应匹配，以获得两个网格之间的粗略对齐。我们在自监督设置中执行粗对准的神经细化。此外，我们利用拉普拉斯细节积分方法来保留输入服装的固有细节。为了评估我们的3D非参数服装重定目标框架，我们提出了一个由255件具有真实噪声和拓扑变形的真实世界服装组成的数据集。该数据集包含15名不同受试者以5种不同姿势穿着的价值44美元的独特服装，这些服装是使用多视图RGBD捕捉设置捕捉的。与现有的最先进的方法相比，我们在非参数服装和人类化身上显示出卓越的重定目标质量，这是所提出的非参数3D服装重定目标数据集上的第一个基线。 et.al.|[2401.03108](http://arxiv.org/abs/2401.03108)|null|
|**2024-01-05**|**Integrating Open-World Shared Control in Immersive Avatars**|远程操作的化身机器人允许人们将他们的操作技能转移到可能难以或危险的工作环境中。目前的系统能够让操作员直接控制机器人的许多组件，使他们沉浸在远程环境中，但操作员仍然难以亲自完成任务。我们提出了一个将开放世界共享控制纳入化身机器人的框架，以结合直接控制和共享控制的好处。该框架通过最大限度地减少对操作员视图的障碍，并使用相同的界面进行直接、共享和完全自主的控制，来保持我们化身界面的流畅性。在一项人类受试者研究（N=19）中，我们发现使用该框架的操作员比不使用该框架者更快、更可靠地完成一系列任务。 et.al.|[2401.03079](http://arxiv.org/abs/2401.03079)|null|
|**2024-01-04**|**Real-and-Present: Investigating the Use of Life-Size 2D Video Avatars in HMD-Based AR Teleconferencing**|增强现实（AR）电话会议允许单独定位的用户在自己的物理环境中通过代理进行3D交互。利用体积捕获和重建的现有方法可以提供高保真度体验，但对于日常使用来说往往过于复杂和昂贵。其他解决方案针对移动设备，在AR头戴式显示器（HMD）上轻松设置电话会议。他们直接将传统的视频会议移植到AR-HMD平台上，或者使用化身来代表远程参与者。然而，它们只能支持高保真度或高水平的共存。此外，HMD有限的视场（FoV）可能会进一步影响用户的沉浸式体验。为了实现保真度和共现性之间的平衡，我们探索在AR电话会议中使用真人大小的基于2D视频的化身（简称视频化身）。具体而言，鉴于FoV对用户感知接近度的潜在影响，我们首先进行了一项试点研究，以探索视频化身在小组AR对话中以本地用户为中心的最佳位置。根据放置结果，我们实现了基于视频化身的电话会议的概念验证原型。我们对原型进行了用户评估，以验证其在平衡保真度和共存性方面的有效性。根据试点研究中的指示，我们通过一项涉及VR模拟环境中更多FoV条件的用户研究，进一步定量探讨了FoV大小对视频化身最佳位置的影响。我们回归放置模型，作为在各种现有AR HMD和未来具有更大FoV的AR HMD上计算确定此类电话会议应用中视频化身放置的参考。 et.al.|[2401.02171](http://arxiv.org/abs/2401.02171)|null|
|**2024-01-03**|**From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations**|我们提出了一个框架，用于生成根据二元交互的会话动态进行手势的完整照片真实感化身。给定语音音频，我们为个人输出多种手势运动的可能性，包括面部、身体和手。我们的方法背后的关键是将矢量量化的样本多样性与通过扩散获得的高频细节相结合，以生成更动态、更具表现力的运动。我们使用高度逼真的化身来可视化生成的运动，这些化身可以表达手势中的关键细微差别（例如冷笑和假笑）。为了促进这一研究，我们引入了第一个允许照片真实感重建的多视图会话数据集。实验表明，我们的模型可以生成适当且多样化的手势，优于仅扩散和仅VQ的方法。此外，我们的感知评估强调了真实感（相对于网格）在准确评估会话手势中细微运动细节方面的重要性。在线提供代码和数据集。 et.al.|[2401.01885](http://arxiv.org/abs/2401.01885)|**[link](https://github.com/facebookresearch/audio2photoreal)**|
|**2024-01-03**|**Teaching with a companion: the case of gravity**|虚拟现实（VR）已经在学生学习中反复证明了它的有效性。然而，尽管有其好处，配备个人耳机的学生在沉浸在虚拟空间中时仍与现实世界隔绝，经典的师生学习模式在这种情况下很难转换。本研究旨在让教师在学生使用VR耳机时回到学习过程中。我们以重力的概念为测试案例，描述了将伴侣用于教育目的的好处。我们提出了一个实验设置，旨在比较三种不同的教学环境：与真实的老师，使用老师的实时视频，以及与老师的虚拟现实化身。我们设计并评估了三个场景来教授重力的概念：自由落体概念介绍、抛物线轨迹研讨会和结合这两种方法的最后练习。由于卫生条件的原因，只报告了预测试。结果表明，使用虚拟现实模拟进行学习的有效性和学生的自信心水平也有所提高。访谈显示，学生对教学模式的排序依次为：虚拟现实陪伴模式、视频交流模式和真实教师模式。 et.al.|[2401.01832](http://arxiv.org/abs/2401.01832)|null|
|**2024-01-02**|**En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data**|我们展示了En3D，这是一种用于雕刻高质量3D人体头像的增强生成方案。与以前的工作依赖于稀缺的3D数据集或具有不平衡视角和不精确姿态先验的有限2D集合不同，我们的方法旨在开发一种零镜头3D生成方案，该方案能够在不依赖预先存在的3D或2D资产的情况下生成视觉逼真、几何准确和内容多样的3D人类。为了应对这一挑战，我们引入了一个精心制作的工作流，该工作流实现了精确的物理建模，以从合成的2D数据中学习增强的3D生成模型。在推理过程中，我们集成了优化模块，以弥合逼真外观和粗略3D形状之间的差距。具体而言，En3D包括三个模块：一个3D生成器，从合成的平衡、多样化和结构化的人体图像中准确地对具有逼真外观的可推广3D人体进行建模；几何雕塑家，其使用用于复杂人体解剖的多视图法线约束来增强形状质量；以及纹理化模块，该模块利用语义UV划分和可微分光栅化器，将具有保真度和可编辑性的显式纹理图解开。实验结果表明，我们的方法在图像质量、几何精度和内容多样性方面显著优于先前的工作。我们还展示了我们生成的化身在动画和编辑方面的适用性，以及我们无内容风格改编方法的可扩展性。 et.al.|[2401.01173](http://arxiv.org/abs/2401.01173)|null|
|**2024-01-01**|**Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven Body Controllable Attribute**|直接从文本生成3D人体模型有助于减少角色建模的成本和时间。然而，由于特征耦合和逼真的3D人体化身数据集的稀缺性，实现多属性可控和逼真的三维人体化身生成仍然具有挑战性。为了解决这些问题，我们提出了Text2Avatar，它可以基于耦合的文本提示生成逼真风格的3D化身。Text2Avatar利用离散代码簿作为中间特征，在文本和化身之间建立连接，从而实现特征的解开。此外，为了缓解逼真风格的3D人体化身数据的稀缺性，我们利用预先训练的无条件3D人体化身生成模型来获得大量的3D化身伪数据，这使得Text2Avatar能够实现逼真风格的生成。实验结果表明，我们的方法可以从耦合的文本数据中生成逼真的3D化身，这对该领域的其他现有方法具有挑战性。 et.al.|[2401.00711](http://arxiv.org/abs/2401.00711)|null|
|**2023-12-30**|**ASL Champ!: A Virtual Reality Game with Deep-Learning Driven Sign Recognition**|我们在虚拟现实（VR）环境中开发了一个美国手语（ASL）学习平台，为ASL学习者提供身临其境的互动和实时反馈。我们描述了第一款使用交互式教学风格的游戏，其中用户从流畅的签名化身中学习，以及第一款在VR环境中使用深度学习实现ASL签名识别。先进的动作捕捉技术为身临其境的三维环境中富有表现力的ASL教学化身提供了动力。老师为对象演示ASL符号，提示用户复制该符号。在用户签名后，第三方插件将与深度学习模型一起执行符号识别过程。根据用户的手势制作的准确性，化身重复手势或引入新的手势。我们收集了来自15名不同参与者的3D VR ASL数据集，为手势识别模型提供支持。所提出的深度学习模型的训练、验证和测试准确率分别为90.12%、89.37%和86.66%。该功能原型可以教授手语词汇，并成功地应用于虚拟现实中的交互式ASL学习平台。 et.al.|[2401.00289](http://arxiv.org/abs/2401.00289)|null|
|**2023-12-28**|**Entanglement entropies in the abelian arithmetic Chern-Simons theory**|量子力学系统中的纠缠熵概念是一个重要的量，它衡量复合系统中物理状态的纠缠程度。在数学上，它测量状态向量在多大程度上不可分解为两个希尔伯特空间的张量积中的元素。在本文中，我们寻求它的算术化身：具有有限规范群 $G$的算术Chern-Simons理论自然地将两个量子希尔伯特空间的乘积内的状态向量相关联，并且当$G$ 是素数阶循环群时，我们提供了这种状态向量的｛\em-von Neumann纠缠熵｝的公式。 et.al.|[2312.17138](http://arxiv.org/abs/2312.17138)|null|
|**2023-12-28**|**Dynamic Appearance Modeling of Clothed 3D Human Avatars using a Single Camera**|穿着衣服的人的外表不仅受姿势的驱动，还受其时间背景（即运动）的驱动。然而，现有的单目人体建模方法在很大程度上忽略了这种背景，由于运动模糊性，神经网络往往难以学习具有大动态的人的视频，即，即使对于相同的姿势，也存在许多依赖于运动背景的衣服几何配置。在本文中，我们介绍了一种使用动态运动的人的视频对穿着衣服的3D人体化身进行高质量建模的方法。主要挑战来自于缺乏几何的3D地面实况数据及其时间对应关系。我们通过引入一种新颖的组合人体建模框架来应对这一挑战，该框架利用了显式和隐式人体建模。对于显式建模，神经网络通过比较其2D渲染结果和原始图像来学习生成3D身体模型的逐点形状残差和外观特征。该显式模型允许通过编码它们的时间对应关系来从UV空间重构有区别的3D运动特征。对于隐式建模，隐式网络将外观和3D运动特征相结合，以解码具有运动相关几何形状和纹理的高保真穿着衣服的3D人类化身。实验表明，我们的方法可以以物理上合理的方式产生大的二次运动变化。 et.al.|[2312.16842](http://arxiv.org/abs/2312.16842)|null|
|**2023-12-29**|**DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption by Combining 3D GANs and Diffusion Priors**|文本引导的领域自适应和3D感知肖像的生成在各个领域都有许多应用。然而，由于缺乏训练数据以及在处理高度多样的几何形状和外观方面的挑战，这些任务的现有方法存在灵活性、不稳定性和低保真度等问题。在本文中，我们提出了一种新的框架DiffusionGAN3D，该框架通过结合3D GANs和扩散先验来增强文本引导的3D域自适应和生成。具体来说，我们集成了预先训练的3D生成模型（例如，EG3D）和文本到图像的扩散模型。前者为从文本生成稳定、高质量的头像提供了坚实的基础。扩散模型反过来提供了强大的先验，并引导3D生成器以信息方向进行微调，以实现灵活高效的文本引导域自适应。为了增强领域自适应的多样性和文本到化身的生成能力，我们分别引入了相对距离损失和特定案例的可学习三平面。此外，我们还设计了一个渐进式纹理细化模块来提高以上两项任务的纹理质量。大量实验表明，所提出的框架在域自适应和文本到化身任务方面都取得了优异的结果，在生成质量和效率方面优于现有方法。项目主页位于https://younglbw.github.io/DiffusionGAN3D-homepage/. et.al.|[2312.16837](http://arxiv.org/abs/2312.16837)|null|
|**2023-12-24**|**Make-A-Character: High Quality Text-to-3D Character Generation within Minutes**|随着人工智能代理和元宇宙的出现，人们对定制和表达3D角色的需求越来越大，但使用传统的计算机图形工具创建3D角色是一项复杂而耗时的任务。为了应对这些挑战，我们提出了一个名为Make-a-Character（Mach）的用户友好框架，用于从文本描述中创建逼真的3D化身。该框架利用大型语言和视觉模型的力量进行文本意图理解和中间图像生成，然后是一系列以人为本的视觉感知和3D生成模块。我们的系统为用户提供了一种直观的方法，可以在2分钟内制作出可控、逼真、完全实现的3D角色，满足他们的期望，同时还可以轻松地与现有的CG管道集成，以获得动态表现力。有关更多信息，请访问项目页面https://human3daigc.github.io/MACH/. et.al.|[2312.15430](http://arxiv.org/abs/2312.15430)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|
|**2023-12-21**|**DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation**|从一张肖像图像中生成情绪化的会说话的面孔仍然是一个重大挑战。同时实现富有表现力的情感对话和准确的唇同步尤其困难，因为唇同步的准确性往往会影响表达能力。正如许多先前的工作所广泛采用的那样，LSTM网络往往无法捕捉情感表达的微妙和变化。为了应对这些挑战，我们引入了DREAM Talk，这是一个基于两阶段扩散的音频驱动框架，专门用于同时生成不同的表情和准确的嘴唇同步。在第一阶段，我们提出了EmoDiff，这是一个新颖的扩散模块，根据音频和参考的情绪风格生成各种高度动态的情绪表达和头部姿势。考虑到嘴唇运动和音频之间的强相关性，我们使用音频特征和情感风格来改进动态，提高嘴唇同步的准确性。为此，我们部署了一个视频到视频渲染模块，将表情和嘴唇运动从我们的代理3D化身转移到任意肖像。无论是在数量上还是在质量上，DREAM Talk在表现力、唇同步准确性和感知质量方面都优于最先进的方法。 et.al.|[2312.13578](http://arxiv.org/abs/2312.13578)|null|
|**2023-12-22**|**MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading**|从肖像图像重建化身在多媒体中有许多应用，但仍然是一个具有挑战性的研究问题。从一张图像中提取反射率图和几何图形是不合适的：恢复几何图形是一个一对多的映射问题，反射率和光很难解开。可以在光台的受控条件下捕获精确的几何形状和反射率，但以这种方式获取大型数据集的成本很高。此外，仅使用这种类型的数据进行训练会导致野生图像的泛化能力较差。这促使MoSAR的引入，这是一种从单目图像生成3D化身的方法。我们提出了一种半监督训练方案，通过从光阶段和野外数据集学习来提高泛化能力。这是使用一种新颖的可微分着色公式实现的。我们证明，我们的方法有效地解开了固有的人脸参数，产生了可重新点亮的化身。因此，与现有的最先进的方法相比，MoSAR估计了一组更丰富的皮肤反射率图，并生成了更逼真的化身。我们还介绍了一个新的数据集，名为FFHQ UV Intrensics，这是第一个为总共10k名受试者提供大规模内在人脸属性（漫反射、镜面反射、环境遮挡和半透明图）的公共数据集。项目网站和数据集可通过以下链接获得：https://ubisoft-laforge.github.io/character/mosar/ et.al.|[2312.13091](http://arxiv.org/abs/2312.13091)|null|
|**2023-12-20**|**Relightable and Animatable Neural Avatars from Videos**|3D数字化身的轻量级创建是一项非常理想但具有挑战性的任务。在未知照明下，只有稀疏的人的视频，我们提出了一种创建可重新照明和可动画化的神经化身的方法，该方法可用于在新的视角、身体姿势和照明下合成人类的真实感图像。这里的关键挑战是理清几何结构、衣服的材质和照明，由于身体运动引起的复杂几何结构和阴影变化，这变得更加困难。为了解决这个不适定问题，我们提出了新的技术来更好地模拟几何和阴影的变化。对于几何变化建模，我们提出了一个可逆变形场，这有助于解决反向蒙皮问题，并提高几何质量。为了对空间和时间变化的阴影线索进行建模，我们提出了一种姿态感知的部分光可见性网络来估计光遮挡。在合成和真实数据集上进行的大量实验表明，我们的方法重建了高质量的几何体，并在不同的身体姿势下生成了逼真的阴影。代码和数据位于\url{https://wenbin-lin.github.io/RelightableAvatar-page/}. et.al.|[2312.12877](http://arxiv.org/abs/2312.12877)|null|
|**2023-12-20**|**DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular Videos**|用宽松的衣服重建一个充满活力的人是一项重要但困难的任务。为了应对这一挑战，我们提出了一种名为DLCA-Recon的方法，从单眼视频中创建人类化身。当人类自由活动和行动时，从宽松的衣服到下半身的距离在每一帧中都会迅速变化。以前的方法缺乏有效的几何初始化和约束来指导变形的优化，以解释这种急剧的变化，导致重建表面不连续和不完整。为了更准确地对变形建模，我们建议在规范空间中初始化估计的3D穿着衣服的人，因为变形场更容易从穿着衣服的人类学习，而不是从SMPL学习。通过显式网格和隐式SDF的表示，我们利用连续帧之间的物理连接信息，提出了一个动态变形场（DDF）来优化变形场。DDF考虑了宽松衣服上的作用力，以增强变形的可解释性，并有效地捕捉宽松衣服的自由运动。此外，我们将SMPL蒙皮权重传播到每个个体，并在优化过程中细化姿势和蒙皮权重，以改进蒙皮变换。基于更合理的初始化和DDF，我们可以更准确地模拟真实世界的物理。在公共数据集和我们自己的数据集上进行的大量实验验证了，与SOTA方法相比，我们的方法可以为穿着宽松衣服的人产生更好的结果。 et.al.|[2312.12096](http://arxiv.org/abs/2312.12096)|null|
|**2023-12-19**|**GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction**|人类运动预测对于虚拟现实（VR）应用是重要的，例如对于逼真的化身动画。现有的方法仅根据观察到的过去的运动来合成身体运动，尽管已知人类凝视与身体运动密切相关，并且在最近的VR耳机中很容易获得。我们提出了GazeMoDiff——一种新的凝视引导去噪扩散模型，用于生成随机人体运动。我们的方法首先使用图注意力网络来学习眼睛凝视和人类运动之间的时空相关性，并将其融合为跨模态凝视运动特征。这些跨模态特征通过交叉注意力机制被注入到噪声预测网络中，并逐渐去噪以生成真实的人体全身运动。在MoGaze和GIMO数据集上的实验结果表明，我们的方法在平均位移误差方面大大优于最先进的方法（MoGaze为15.03%，GIMO为9.20%）。我们进一步进行了一项在线用户研究，将我们的方法与最先进的方法进行了比较，23名参与者的回答验证了我们的方法产生的运动比其他方法产生的更真实。总之，我们的工作朝着凝视引导的随机人类运动预测迈出了重要的第一步，并指导了未来VR研究中这一重要主题的工作。 et.al.|[2312.12090](http://arxiv.org/abs/2312.12090)|null|
|**2023-12-19**|**Pose2Gaze: Generating Realistic Human Gaze Behaviour from Full-body Poses using an Eye-body Coordination Model**|虽然在计算机视觉和图形学中广泛研究了生成逼真的身体运动，例如为虚拟现实中的化身生成逼真的肢体运动，但对与身体表现出逼真协调的眼球运动的生成仍有待探索。我们首先根据MoGaze和GIMO数据集的数据，对日常活动中人眼和全身运动的协调进行了全面分析。我们发现，眼睛凝视与头部方向和全身运动有很强的相关性，身体和眼睛运动之间存在明显的时间延迟。受这些分析的启发，我们提出了Pose2Gaze——一种新颖的眼体协调模型，它首先使用卷积神经网络和时空图卷积神经网络分别从头部方向和全身姿势中提取特征，然后应用卷积神经网络生成逼真的眼球运动。我们将我们的方法与仅从三个不同生成任务的头部运动预测眼睛凝视的最先进方法进行了比较，并证明Pose2Gaze在两个数据集上都显著优于这些基线，平均角误差分别提高了26.4%和21.6%。我们的研究结果强调了跨模态人类凝视行为分析和建模的巨大潜力。 et.al.|[2312.12042](http://arxiv.org/abs/2312.12042)|null|
|**2023-12-18**|**Relightable Neural Actor with Intrinsic Decomposition and Pose Control**|在视觉和图形学中，创建一个可欣赏、可驾驶和逼真的数字人类化身是一个具有挑战性的重要问题。人类是高度立体化的，会产生依赖姿势的外观效果，如自我阴影和皱纹，皮肤和衣服需要复杂且空间变化的BRDF模型。虽然最近的人类重新照明方法可以从多视图视频中恢复看似合理的材料光分解，但它们不能推广到新颖的姿势，并且仍然存在视觉伪影。为了解决这一问题，我们提出了Relightable Neural Actor，这是第一种基于视频的方法，用于学习照片真实感的神经人体模型，该模型可以重新照明，允许外观编辑，并可以由任意骨骼姿势控制。重要的是，为了学习我们的人类化身，我们只需要在已知但静态的照明条件下对人类进行多视图记录。为了实现这一点，我们用可驱动的密度场来表示演员的几何体，该密度场对姿势相关的服装变形进行建模，并提供3D和UV空间之间的映射，其中对法线、可见性和材质进行编码。为了在现实世界场景中评估我们的方法，我们收集了一个新的数据集，其中包括在室内和室外不同光照条件下记录的四个参与者，为人类重新照明提供了第一个此类基准，并展示了最先进的新人类姿势的重新照明结果。 et.al.|[2312.11587](http://arxiv.org/abs/2312.11587)|null|
|**2023-12-18**|**GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning**|高斯飞溅已经成为一种强大的3D表示，它利用了显式（网格）和隐式（NeRF）3D表示的优势。在本文中，我们试图利用高斯飞溅从文本描述中生成逼真的可动画化身，解决基于网格或NeRF的表示所带来的限制（例如灵活性和效率）。然而，高斯飞溅的天真应用无法生成高质量的可动画化身，并且存在学习不稳定性；它也不能捕捉到精细的化身几何形状，并且经常导致退化的身体部位。为了解决这些问题，我们首先提出了一种基于基元的3D高斯表示，其中高斯在姿势驱动的基元内定义，以便于动画制作。其次，为了稳定和摊销数百万高斯人的学习，我们建议使用神经隐式场来预测高斯属性（例如，颜色）。最后，为了捕捉精细的化身几何图形并提取详细的网格，我们提出了一种新的基于SDF的三维高斯隐式网格学习方法，该方法对底层几何图形进行正则化，并提取高度详细的纹理网格。我们提出的方法GAvatar仅使用文本提示就可以大规模生成各种可动画化的化身。GAvatar在外观和几何质量方面显著优于现有方法，并在1K分辨率下实现了极快的渲染（100fps）。 et.al.|[2312.11461](http://arxiv.org/abs/2312.11461)|null|
|**2023-12-18**|**Learning to Generate Pseudo Personal Mobility**|个人移动数据的重要性在各个领域得到了广泛认可。然而，真实个人移动数据的使用引发了隐私问题。因此，在保护用户隐私的同时，生成准确反映真实世界移动模式的伪个人移动数据至关重要。然而，现有的生成伪移动性数据的方法，如基于机制和基于深度学习的方法，在捕捉足够的个体异质性方面存在局限性。为了解决这些差距，以伪人（化身）为原点，提出了一种新的基于个体的人类流动生成器，称为GeoAvatar，它考虑了空间和时间决策中的个体异质性，结合了人口统计学特征，并提供了可解释性。我们的方法利用深度生成模型来模拟异质的个人生活模式，利用可靠的标注器来推断个人人口特征，并利用贝叶斯方法来生成空间选择。通过我们的方法，我们实现了在不访问个人层面个人信息的情况下生成异构的个人-人类流动数据，质量良好-我们根据身体特征、活动模式和时空特征对所提出的方法进行了评估，证明了其良好的性能，与基于机制的建模和黑匣子深度学习方法相比。此外，这种方法为更广泛的应用程序保持了可扩展性，使其成为生成人类移动数据的一种很有前途的范例。 et.al.|[2312.11289](http://arxiv.org/abs/2312.11289)|null|
|**2023-12-15**|**Attention-Based VR Facial Animation with Visual Mouth Camera Guidance for Immersive Telepresence Avatars**|虚拟现实环境中的面部动画对于需要用户面部的清晰可见性和传达情感信号的能力的应用程序至关重要。在我们的场景中，我们为控制机器人阿凡达系统的操作员的脸设置动画。当想要与特定个体而不仅仅是机器人互动时，面部动画的使用尤其有价值。纯关键点驱动的动画方法与面部运动的复杂性作斗争。我们提出了一种混合方法，既使用关键点，又使用口腔摄像头的直接视觉引导。我们的方法适用于看不见的运营商，只需要快速注册一步，即可捕获两个短视频。选择多个源图像以覆盖不同的面部表情。给定HMD的口腔摄像头帧，我们动态构建目标关键点，并应用注意力机制来确定每个源图像的重要性。为了解决关键点模糊性并使更广泛的口腔表情动画化，我们建议将视觉口腔摄像头信息注入潜在空间。我们通过模拟口腔摄像头输入的视角差异和面部变形，实现了在大规模说话头部数据集上的训练。我们的方法在质量、能力和时间一致性方面优于基线。此外，我们还重点介绍了面部动画是如何帮助我们在ANA阿凡达XPRIZE总决赛中获胜的。 et.al.|[2312.09750](http://arxiv.org/abs/2312.09750)|null|
|**2023-12-15**|**3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting**|我们介绍了一种使用3D高斯飞溅（3DGS）从单眼视频创建可动画化人类化身的方法。现有的基于神经辐射场（NeRFs）的方法实现了高质量的新视图/新姿态图像合成，但通常需要数天的训练，并且在推理时非常慢。最近，该社区探索了快速网格结构，以有效训练穿着衣服的化身。尽管训练速度极快，但这些方法几乎无法实现约15 FPS的交互式渲染帧速率。在本文中，我们使用3D高斯飞溅并学习非刚性变形网络来重建可动画化的穿着衣服的人类化身，这些化身可以在30分钟内训练并以实时帧速率（50+FPS）渲染。考虑到我们表示的显式性质，我们进一步在高斯均值向量和协方差矩阵上引入尽可能等距的正则化，增强了我们的模型在高度清晰的看不见姿态上的泛化能力。实验结果表明，与最先进的方法相比，我们的方法在从单目输入创建可动画化身方面实现了相当甚至更好的性能，同时在训练和推理方面分别快了400倍和250倍。 et.al.|[2312.09228](http://arxiv.org/abs/2312.09228)|null|
|**2023-12-26**|**SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance**|在大规模文本到图像生成模型的支持下，文本到3D化身生成取得了可喜的进展。然而，由于不精确的几何形状和低质量的外观，大多数方法都无法产生逼真的结果。为了更实用的化身生成，我们提出了SEEAvatar，这是一种从文本中生成照片级真实感3D化身的方法，具有解耦几何和外观的SElf进化约束。对于几何体，我们建议使用模板化身将优化后的化身约束为适当的全局形状。模板化身是用人类先验初始化的，并且可以由优化的化身周期性地更新为演进的模板，这使得能够更灵活地生成形状。此外，在面部和手部等局部区域，几何结构也受到静态人类先验的约束，以保持精细的结构。对于外观生成，我们使用通过提示工程增强的扩散模型来引导基于物理的渲染管道生成逼真的纹理。亮度约束应用于反照率纹理，以抑制不正确的照明效果。实验表明，我们的方法在全局和局部几何以及外观质量方面都大大优于以前的方法。由于我们的方法可以生成高质量的网格和纹理，因此这些资源可以直接应用于经典图形管道中，以便在任何照明条件下进行逼真的渲染。项目页面位于：https://yoxu515.github.io/SEEAvatar/. et.al.|[2312.08889](http://arxiv.org/abs/2312.08889)|null|
|**2023-12-14**|**A Local Appearance Model for Volumetric Capture of Diverse Hairstyle**|头发在个人身份和外表方面发挥着重要作用，使其成为高质量、逼真的化身的重要组成部分。现有的方法要么只关注面部区域的建模，要么依赖于个性化模型，限制了其可推广性和可扩展性。在本文中，我们提出了一种新颖的方法来创建具有不同发型的高保真化身。我们的方法利用了不同发型之间的局部相似性，并从数百人的多视图捕捉中学习了普遍的头发外观。该先验模型以3D对齐的特征作为输入，并生成以具有颜色的稀疏点云为条件的密集辐射场。当我们的模型将不同的发型拆分为局部基本体并在该级别上预先构建时，它能够处理各种头发拓扑。通过实验，我们证明了我们的模型捕捉到了各种各样的发型，并很好地概括了具有挑战性的新发型。经验结果表明，我们的方法改进了最先进的方法来捕捉和生成具有完整头发的照片真实感、个性化化身。 et.al.|[2312.08679](http://arxiv.org/abs/2312.08679)|null|
|**2023-12-12**|**GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance**|尽管现有的语音驱动的谈话人脸生成方法取得了显著进展，但由于特定于化身的训练需求和不稳定的嘴唇运动，它们远未在现实世界中应用。为了解决上述问题，我们提出了GSmoothFace，这是一种以细粒度三维人脸模型为指导的两阶段广义会说话人脸生成模型，它可以在保持说话人身份的同时合成平滑的嘴唇动态。我们提出的GSmoothFace模型主要由音频到表达式预测（A2EP）模块和目标自适应人脸转换（TAFT）模块组成。具体来说，我们首先开发了A2EP模块来预测与驱动语音同步的表达参数。它使用转换器来捕捉长期音频上下文，并从细粒度的3D面部顶点学习参数，从而实现准确平滑的嘴唇同步性能。之后，设计良好的TAFT模块在形态学增强人脸混合（MAFB）的支持下，将预测的表情参数和目标视频作为输入，在不失真背景内容的情况下修改目标视频的面部区域。TAFT有效地利用了目标视频中的身份外观和背景上下文，这使得可以在不进行再训练的情况下推广到不同的说话者。定量和定性实验都证实了我们的方法在真实感、嘴唇同步和视觉质量方面的优越性。有关代码、数据和请求的预训练模型，请参阅项目页面：https://zhanghm1995.github.io/GSmoothFace. et.al.|[2312.07385](http://arxiv.org/abs/2312.07385)|null|
|**2023-12-12**|**On the Potential of an Independent Avatar to Augment Metaverse User Socialization Time**|我们提出了一种计算建模方法，旨在捕捉如何通过在元宇宙中使用独立和自主版本的数字表示来虚拟增加元宇宙用户的可用社交时间容量的细节。我们设想了一个以元宇宙为中心的传统化身概念的扩展：当用户不直接控制化身时，化身也可以被编程为独立操作，从而将其变成基于代理的数字人类表示。这样，用户可以虚拟地委托维持现有联系人所需的化身社交时间，以便最终维持空闲的非化身介导的社交时间，该空闲的社交时间可以潜在地投资于额外的社交活动。我们使用社会科学中选定的概念对环境进行建模，并确定特征变量：自我网络、社会存在和社会线索。然后，我们将用户的非化身中介空闲时间最大化问题公式化为线性优化。最后，我们分析了问题的可行区域，并对化身介导的交互的不同参数值可以实现的空闲时间提出了一些初步见解。 et.al.|[2312.07077](http://arxiv.org/abs/2312.07077)|null|
|**2023-12-11**|**Development of the Lifelike Head Unit for a Humanoid Cybernetic Avatar `Yui' and Its Operation Interface**|在以化身为媒介的交流中，面对面对话者通过化身感知操作员的存在和情绪是至关重要的。尽管类似人类的机器人已经被开发成通过外表和动作来传达存在感，但很少有研究优先考虑使用机器人作为化身来加深操作员和对话者的沟通体验。为了解决这一差距，我们推出了“控制论化身Yui”，它具有28个自由度的人形头部单元，能够表达凝视、面部情绪和与言语相关的口腔动作。通过头戴式显示器（HMD）中的眼睛跟踪单元和Yui双眼的自由度，操作员可以自然地控制化身的凝视。此外，Yui耳朵里嵌入的麦克风可以让操作员在三维空间听到周围的声音，使他们能够仅根据听觉信息辨别通话方向。HMD的面部跟踪单元将化身的面部动作与操作员的面部动作同步。这种身临其境的界面，再加上Yui人性化的外表，实现了实时的情感传递和沟通，增强了双方的存在感。我们的实验展示了Yui的面部表情能力，并通过远程操作试验验证了该系统的功效，这表明化身技术有潜在的进步。 et.al.|[2312.06310](http://arxiv.org/abs/2312.06310)|null|
|**2023-12-10**|**ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering**|真实感和可控的人类化身的实时渲染是计算机视觉和图形学的基石。虽然神经隐式渲染的最新进展为数字化身解锁了前所未有的真实感，但实时性能大多仅用于静态场景。为了解决这一问题，我们提出了ASH，这是一种用于实时动态人类真实感渲染的可动画高斯飞溅方法。我们将穿着衣服的人参数化为可动画化的3D高斯，可以有效地将其泼洒到图像空间中以生成最终渲染。然而，在3D空间中天真地学习高斯参数在计算方面带来了严峻的挑战。相反，我们将高斯算子附加到可变形的角色模型上，并在2D纹理空间中学习它们的参数，这允许利用高效的2D卷积架构，该架构可以轻松地根据所需的高斯算子数量进行缩放。我们在姿势可控的化身上用竞争方法对ASH进行了基准测试，证明我们的方法在很大程度上优于现有的实时方法，并显示出与离线方法相当甚至更好的结果。 et.al.|[2312.05941](http://arxiv.org/abs/2312.05941)|null|
|**2023-12-09**|**FT2TF: First-Person Statement Text-To-Talking Face Generation**|会说话的人脸生成在计算机视觉社区中非常流行，其应用包括AR/VR、电话会议、数字助理和化身。传统的方法主要是音频驱动的方法，必须处理音频存储和处理不可避免的资源密集性。为了应对这一挑战，我们提出了FT2TF-第一人称陈述文本到会说话的人脸生成，这是一种由第一人称陈述文字驱动的新型一阶段端到端的会说话人脸生成管道。此外，FT2TF通过改变相应的输入文本来实现对面部表情的精确操纵。与之前的工作不同，我们的模型在推理过程中只利用视觉和文本信息，而没有任何其他来源（例如音频/地标/姿势）。在LRS2和LRS3数据集上进行了广泛的实验，并报告了多维评估指标的结果。定量和定性结果都表明，FT2TF优于现有的相关方法，达到了最先进的水平。这一成就突出了我们的模型能力，将第一人称陈述和动态人脸生成连接起来，为未来的工作提供了深刻的指导。 et.al.|[2312.05430](http://arxiv.org/abs/2312.05430)|null|
|**2023-12-08**|**360° Volumetric Portrait Avatar**|我们提出了360体积人像（3VP）化身，这是一种仅基于单眼视频输入重建人类受试者360照片逼真人像化身的新方法。最先进的单目化身重建方法依赖于稳定的面部表现捕捉。然而，基于3DMM的面部跟踪的普遍使用有其局限性；侧视图很难被捕获，而且它失败了，尤其是对于后视图，因为缺少面部标志或人类解析掩码等所需输入。这导致了只覆盖前半球的不完整的化身重建。与此相反，我们提出了一种基于模板的躯干、头部和面部表情跟踪，使我们能够从各个方面覆盖人类受试者的外观。因此，给定在单个相机前旋转的对象的序列，我们基于神经辐射场训练神经体积表示。构建这种表示的一个关键挑战是外观变化的建模，尤其是在口腔区域（即嘴唇和牙齿）。因此，我们提出了一种基于变形场的混合基础，它允许我们在不同的外观状态之间进行插值。我们根据捕获的真实世界数据评估我们的方法，并与最先进的单目重建方法进行比较。与此相反，我们的方法是第一种重建整个360度化身的单目技术。 et.al.|[2312.05311](http://arxiv.org/abs/2312.05311)|null|
|**2023-12-08**|**Disentangled Clothed Avatar Generation from Text Descriptions**|在本文中，我们介绍了一种新颖的文本到化身的生成方法，该方法分别生成人体和衣服，并允许在生成的化身上生成高质量的动画。虽然文本到化身生成的最新进展已经从文本提示中产生了不同的人类化身，但这些方法通常将所有元素——衣服、头发和身体——组合成一个单独的3D表示。这种纠缠的方法给编辑或动画等下游任务带来了挑战。为了克服这些限制，我们在SMPL模型的基础上提出了一种新的解纠缠的3D化身表示，称为序列偏移SMPL（SO-SMPL）。SO-SMPL用两个独立的网格表示人体和衣服，但将它们与偏移相关联，以确保身体和衣服之间的物理对齐。然后，我们设计了一个基于分数蒸馏采样（SDS）的蒸馏框架，从文本提示中生成所提出的SO-SMPL表示。与现有的文本到化身方法相比，我们的方法不仅实现了更高的文本和几何质量以及更好的与文本提示的语义对齐，而且显著提高了角色动画、虚拟试穿和化身编辑的视觉质量。我们的项目页面位于https://shanemankiw.github.io/SO-SMPL/. et.al.|[2312.05295](http://arxiv.org/abs/2312.05295)|null|
|**2023-12-08**|**IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing**|我们提出了IntrnsicAvatar，这是一种仅从单眼视频中恢复穿着衣服的人类化身的内在特性的新方法，包括几何形状、反照率、材料和环境照明。基于人类的神经渲染的最新进展使得仅从单眼视频中就可以对穿着衣服的人类进行高质量的几何结构和外观重建。然而，这些方法将反照率、材料和环境照明等固有特性烘焙成单个纠缠的神经表示。另一方面，只有少数作品解决了从单眼视频中估计穿着衣服的人的几何形状和解开其外观特性的问题。由于通过学习的MLP近似二次着色效果，它们通常实现有限的质量和解纠缠。在这项工作中，我们建议通过蒙特卡罗射线追踪来显式地对二次着色效果进行建模。我们将穿着衣服的人的渲染过程建模为体积散射过程，并将光线追踪与身体关节结合起来。我们的方法可以从单个单眼视频中恢复穿着衣服的人的高质量几何结构、反照率、材料和照明特性，而无需使用地面实况材料进行监督预训练。此外，由于我们明确地对体积散射过程和光线跟踪进行了建模，因此我们的模型自然地推广到新颖的姿势，从而能够在新颖的照明条件下对重建的化身进行动画制作。 et.al.|[2312.05210](http://arxiv.org/abs/2312.05210)|null|
|**2023-12-08**|**Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video**|3D化身生成的最新进展擅长于照片真实感模型的多视图监督。然而，尽管具有更广泛的适用性，但单眼对应物在质量上落后。我们建议ReCaLab缩小这一差距。ReCaLab是一个完全可微分的管道，只需从一个RGB视频中学习高保真3D人体化身。对姿势条件可变形NeRF进行了优化，以在体积上以标准T姿势表示人类受试者。然后利用规范表示来使用2D-3D对应有效地关联视点不可知的纹理。这使得能够单独生成反照率和阴影，它们共同构成RGB预测。该设计允许通过文本提示控制人体姿势、体型、纹理和照明的中间结果。图像条件扩散模型从而有助于使3D化身的外观和姿势动画化，以创建具有先前看不见的人类运动的视频序列。大量实验表明，ReCaLab在图像合成任务的图像质量方面优于以前的单目方法。ReCaLab甚至优于多视图方法，这些方法利用高达19倍的同步视频来完成新颖的姿势渲染任务。此外，自然语言为3D人类化身的创造性操作提供了直观的用户界面。 et.al.|[2312.04784](http://arxiv.org/abs/2312.04784)|null|
|**2023-12-07**|**MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar**|从单眼人像视频序列重建的照片逼真的头部化身的动画化能力代表了弥合虚拟世界和现实世界之间差距的关键一步。这项正在进行的研究利用了头部化身技术的最新进展，包括显式3D可变形网格（3DMM）、点云和神经隐式表示。然而，基于3DMM的方法受到其固定拓扑的约束，基于点的方法由于涉及大量的点而遭受沉重的训练负担，最后一种方法在变形灵活性和渲染效率方面受到限制。为了应对这些挑战，我们提出了MonoGaussianAvatar（基于单目高斯点的头部化身），这是一种利用3D高斯点表示与高斯变形场相结合的新方法，从单目人像视频中学习明确的头部化身。我们用高斯点定义我们的头部化身，高斯点的特征是形状可适应，从而实现灵活的拓扑结构。这些点表现出高斯变形场与人的目标姿势和表情对齐的运动，有助于有效变形。此外，高斯点具有可控的形状、大小、颜色和不透明度，并与高斯飞溅相结合，从而实现高效的训练和渲染。实验证明了我们的方法的优越性能，在以前的方法中取得了最先进的结果。 et.al.|[2312.04558](http://arxiv.org/abs/2312.04558)|null|
|**2023-12-07**|**FitDiff: Robust monocular 3D facial shape and reflectance estimation using Diffusion Models**|三维人脸重建的显著进展已经产生了高细节和逼真的人脸表示。最近，扩散模型实现了比GANs更好的性能，从而彻底改变了生成方法的能力。在这项工作中，我们提出了FitDiff，一个基于扩散的三维人脸化身生成模型。该模型利用从“野外”2D面部图像中提取的身份嵌入，准确地生成可重新点亮的面部化身。我们的多模式漫射模型同时输出面部反射率图（漫射和镜面反照率和法线）和形状，显示出强大的泛化能力。它只在公共面部数据集的注释子集上进行训练，并与3D重建相结合。我们通过使用感知和人脸识别损失来引导反向扩散过程，重新审视典型的3D人脸拟合方法。FitDiff是第一个以人脸识别嵌入为条件的LDM，它重建了可重新照明的人类化身，可以像在普通渲染引擎中一样使用，只从不受约束的面部图像开始，并实现了最先进的性能。 et.al.|[2312.04465](http://arxiv.org/abs/2312.04465)|null|
|**2023-12-19**|**Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing**|我们提出了一种新的框架，用于生成逼真的3D人头，并随后以显著的灵活性操纵和重新放置它们。所提出的方法利用了3D人头的隐式函数表示，采用了锚定在参数人脸模型上的3D高斯。为了增强表示能力和编码空间信息，我们在每个高斯中嵌入了一个轻量级的三平面有效载荷，而不是直接存储颜色和不透明度。此外，我们通过3DMM在2D UV空间中参数化高斯，从而能够有效利用扩散模型生成3D头部化身。我们的方法通过对面部特征和表情进行精细编辑，有助于创建多样化、逼真的3D人头。大量的实验证明了我们方法的有效性。 et.al.|[2312.03763](http://arxiv.org/abs/2312.03763)|null|
|**2023-12-06**|**Relightable Gaussian Codec Avatars**|重新照明的保真度受几何图形和外观表示的限制。对于几何体，网格和体积方法都很难对复杂的结构（如3D头发几何体）进行建模。就外观而言，现有的重新照明模型的保真度有限，而且往往太慢，无法在高分辨率连续环境中实时渲染。在这项工作中，我们提出了可信赖高斯编解码器化身，这是一种构建高保真度可信赖头部化身的方法，可以对其进行动画处理以生成新颖的表情。我们基于3D高斯的几何模型可以捕捉动态人脸序列上的头发和毛孔等3D一致的亚毫米细节。为了以统一的方式支持人类头部的不同材料，如眼睛、皮肤和头发，我们提出了一种基于可学习辐射转移的新型可重新照明外观模型。与漫射组件的全局照明感知球面谐波一起，我们使用球面高斯实现了空间全频反射的实时重新照明。该外观模型可以在点光和连续光照下有效地重新点亮。我们通过引入可重新照明的显式眼睛模型，进一步提高了眼睛反射的保真度，并实现了显式凝视控制。我们的方法在不影响实时性能的情况下优于现有方法。我们还展示了在线消费VR耳机上化身的实时重新照明，展示了我们化身的效率和保真度。 et.al.|[2312.03704](http://arxiv.org/abs/2312.03704)|null|
|**2023-12-06**|**Artist-Friendly Relightable and Animatable Neural Heads**|创建照片逼真数字化身的一种越来越常见的方法是通过使用体积神经场。当在一组多视图图像上训练时，原始神经辐射场（NeRF）允许静态头部进行令人印象深刻的新颖视图合成，后续方法表明，这些神经表示可以扩展到动态化身。最近，新的变体也超过了神经表示中烘焙照明的常见缺点，表明静态神经化身可以在任何环境中重新发光。在这项工作中，我们同时解决了运动和照明问题，为可重新照明和可动画化的神经头提出了一种新方法。我们的方法建立在一种已验证的动态化身方法的基础上，该方法基于体积基元的混合，结合最近提出的用于可重新照明神经场的轻量级硬件设置，并包括一种新的架构，该架构允许重新照明动态神经化身在任何环境中执行看不见的表情，即使是在近场照明和视点的情况下。 et.al.|[2312.03420](http://arxiv.org/abs/2312.03420)|null|
|**2023-12-05**|**Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians**|创建高保真3D头部化身一直是研究热点，但在轻量级稀疏视图设置下仍然存在巨大挑战。在本文中，我们提出了以可控的3D高斯表示的高斯头部化身，用于高保真度的头部化身建模。我们优化了中性的3D高斯和完全学习的基于MLP的变形场，以捕捉复杂的表达式。这两个部分相辅相成，因此我们的方法可以对细粒度的动态细节进行建模，同时确保表达式的准确性。此外，为了训练过程的稳定性和收敛性，我们设计了一种基于隐式SDF和深度行进四面体的精心设计的几何引导初始化策略。实验表明，我们的方法优于其他最先进的稀疏视图方法，即使在夸张的表达式下，也能在2K分辨率下实现超高保真渲染质量。 et.al.|[2312.03029](http://arxiv.org/abs/2312.03029)|**[link](https://github.com/yuelangx/gaussian-head-avatar)**|
|**2023-12-05**|**MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures**|在这个时代，大型语言模型和文本到图像模型的成功可以归功于大型数据集的驱动力。然而，在3D视觉领域，尽管在Ob厌恶和MVImgNet等大规模合成和真实捕获对象数据上训练的模型已经取得了显著进展，但在以人为中心的任务领域尚未观察到类似水平的进展，部分原因是缺乏大规模的人类数据集。由于在获取大规模高质量3D人体数据方面存在重大挑战，高保真3D人体捕捉的现有数据集仍然是中等规模的。为了弥补这一差距，我们提出了MVHumanNet，这是一个包含4500个人类身份的多视图人类动作序列的数据集。我们工作的主要重点是使用多视图人体捕捉系统收集具有大量不同身份和日常服装的人体数据，这有助于轻松扩展数据收集。我们的数据集包含9000套日常服装、60000个运动序列和6.45亿帧，具有广泛的注释，包括人体面具、相机参数、2D和3D关键点、SMPL/SMPLX参数以及相应的文本描述。为了探索MVHumanNet在各种2D和3D视觉任务中的潜力，我们对视图一致性动作识别、人类NeRF重建、文本驱动的视图无约束人体图像生成以及2D视图无约束人类图像和3D化身生成进行了试点研究。大量实验证明了MVHumanNet提供的规模所带来的性能改进和有效应用。作为目前规模最大的3D人体数据集，我们希望MVHumanNet数据的发布和注释将促进3D人体中心任务领域的进一步创新。 et.al.|[2312.02963](http://arxiv.org/abs/2312.02963)|null|
|**2023-12-05**|**HeadGaS: Real-Time Animatable Head Avatars via 3D Gaussian Splatting**|在过去的几年里，3D头部动画的质量和运行时间都有了重大改进，尤其是在可微分渲染和神经辐射领域的进步。实时渲染是现实世界应用程序非常理想的目标。我们提出了HeadGaS，这是第一个使用3D高斯飞溅（3DGS）进行3D头部重建和动画的模型。在本文中，我们介绍了一种混合模型，该模型扩展了3DGS的显式表示，并以可学习的潜在特征为基础，可以将其与参数头部模型的低维参数线性混合，以获得与表情相关的最终颜色和不透明度值。我们证明，HeadGaS在实时推理帧速率方面提供了最先进的结果，它比基线高出约2dB，同时将渲染速度加快了10倍以上。 et.al.|[2312.02902](http://arxiv.org/abs/2312.02902)|null|
|**2023-12-05**|**Perspectives from Naive Participants and Experienced Social Science Researchers on Addressing Embodiment in a Virtual Cyberball Task**|我们描述了一个沉浸式虚拟Cyberball任务的设计，其中包括化身定制，以及用户对此设计的反馈。我们首先创建了一个化身定制模板的原型，并将其添加到Unity3D游戏引擎中构建的Cyberball原型中。然后，我们与15名Cyberball利益相关者进行了深入的用户测试和反馈会议：5名天真的参与者对Cyberball没有任何了解，10名经验丰富的研究人员对使用Cyberball范式有着丰富的经验。我们报告了两个小组对以下设计见解的不同观点；与极简主义相比，为直观使用、包容性和现实体验而设计。参与者的回答揭示了系统设计问题如何在定制化身时导致或延续负面体验。它们还展示了在虚拟现实的设计过程中考虑多个利益相关者的反馈的价值，为设计未来的网络球原型和社会科学研究交互系统提供了更全面的视角。 et.al.|[2312.02897](http://arxiv.org/abs/2312.02897)|null|
|**2023-12-05**|**Neural Sign Actors: A diffusion model for 3D sign language production from text**|手语（SL）是聋人和重听群体的主要交流方式。深度学习的出现帮助了许多SL识别和翻译方法，取得了显著的效果。然而，手语生成（SLP）对计算机视觉界提出了挑战，因为生成的动作必须真实且具有精确的语义。大多数SLP方法依赖于2D数据，从而阻碍了它们达到必要的真实感水平的能力。在这项工作中，我们提出了一种基于扩散的SLP模型，该模型在4D签名化身及其相应文本转录物的策划的大规模数据集上进行训练。所提出的方法可以使用在SMPL-X身体骨架上定义的新颖且解剖学上知情的图神经网络上形成的扩散过程，从无约束的话语域生成3D化身的动态序列。通过一系列定量和定性实验，我们表明所提出的方法大大优于以前的SLP方法。我们认为，这项工作为实现逼真的神经符号化身迈出了重要而必要的一步，弥合了聋人和听力社区之间的沟通差距。代码、方法和生成的数据将公开。 et.al.|[2312.02702](http://arxiv.org/abs/2312.02702)|null|
|**2023-12-15**|**InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars**|虽然高保真度和效率是创建数字头部化身的核心，但最近依赖于2D或3D生成模型的方法往往会受到形状失真、表情不准确和身份闪烁等限制。此外，现有的单次反演技术不能完全利用多个输入图像进行详细的特征提取。我们提出了一种新的框架\textbf{Incremental 3D GAN Inversion}，该框架使用一种旨在提高多帧保真度的算法来增强化身重建性能，从而提高与帧数成比例的重建质量。我们的方法引入了一种独特的可动画化3D GAN先验，并对其进行了两个关键的修改，以增强表达的可控性，同时还引入了一个创新的神经纹理编码器，该编码器基于UV参数化对纹理特征空间进行分类。与传统技术不同，我们的架构强调像素对齐的图像到图像转换，减少了学习观察和规范空间之间对应关系的需要。此外，我们结合了基于ConvGRU的递归网络，用于多帧的时间数据聚合，增强几何结构和纹理细节重建。所提出的范例在单镜头和少镜头化身动画任务上展示了最先进的性能。 et.al.|[2312.02222](http://arxiv.org/abs/2312.02222)|null|
|**2023-12-03**|**FlashAvatar: High-Fidelity Digital Avatar Rendering at 300FPS**|我们提出了FlashAvatar，这是一种新颖且轻量级的3D可动画化化身表示，可以在几分钟内从短单目视频序列中重建数字化身，并在消费级GPU上以300FPS的速度渲染高保真照片逼真图像。为了实现这一点，我们在参数人脸模型的表面中保持均匀的3D高斯场，并学习额外的空间偏移来对非表面区域和细微的人脸细节进行建模。虽然充分利用几何先验可以捕捉高频面部细节并保留夸张的表情，但适当的初始化可以帮助减少高斯数，从而实现超快的渲染速度。大量实验结果表明，FlashAvatar在视觉质量和个性化细节方面优于现有作品，渲染速度几乎快了一个数量级。项目页面：https://ustc3dv.github.io/FlashAvatar/ et.al.|[2312.02214](http://arxiv.org/abs/2312.02214)|null|
|**2023-12-06**|**AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing**|支持用户交互编辑的可编辑三维感知生成近年来得到了快速发展。然而，现有的可编辑3D GANs要么无法实现高精度的局部编辑，要么面临巨大的计算成本。我们提出了AttriHuman-3D，一个可编辑的三维人体生成模型，它解决了上述属性分解和索引问题。该模型的核心思想是在具有六个特征平面的整体属性空间中生成所有属性（如人体、头发、衣服等），然后用不同的属性索引对其进行分解和操作。为了从生成的特征平面中精确提取不同属性的特征，我们提出了一种新的属性索引方法和正交投影正则化方法来增强解纠缠。我们还引入了超潜训练策略和属性特定采样策略，以避免风格纠缠和鉴别器的误导性惩罚。我们的方法允许用户交互式地编辑生成的3D人体化身中的选定属性，同时保持其他属性不变。定性和定量实验都表明，我们的模型在不同属性之间提供了强大的解纠缠，允许细粒度的图像编辑，并生成高质量的3D人类化身。 et.al.|[2312.02209](http://arxiv.org/abs/2312.02209)|null|
|**2023-12-04**|**GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians**|我们展示了GaussianAvatar，这是一种从单个视频中创建具有动态3D外观的逼真人类化身的有效方法。我们首先引入可动画化的3D高斯，以明确地表示各种姿势和服装风格的人类。这种明确的和可设置动画的表示可以更有效且一致地融合来自2D观察的3D外观。我们的表示进一步增强了动态特性，以支持姿势相关的外观建模，其中设计了动态外观网络和可优化的特征张量，以学习运动到外观的映射。此外，通过利用可微分运动条件，我们的方法能够在化身建模期间对运动和外观进行联合优化，这有助于解决单眼设置中长期存在的运动估计不准确的问题。GaussianAvatar的功效在公共数据集和我们收集的数据集上都得到了验证，证明了其在外观质量和渲染效率方面的卓越性能。 et.al.|[2312.02134](http://arxiv.org/abs/2312.02134)|null|
|**2023-12-04**|**Can we truly transfer an actor's genuine happiness to avatars? An investigation into virtual, real, posed and spontaneous faces**|一个眼神胜过千言万语是一句流行语。为什么一个简单的眼神就足以描绘我们对某事或某人的感受？这个问题的背后是心理学领域关于社会认知的理论基础和心理学家保罗·埃克曼的研究。面部表情作为一种非语言交流形式，是人与人之间传递情感的主要方式。面部肌肉的一组动作和表情将个体的一些情绪状态传达给观察者，这是许多领域研究的目标。我们的研究旨在评估Ekman在真实人脸、姿势人脸和自发人脸以及将真实人脸转换为计算机图形学人脸所产生的虚拟人脸数据集中的动作单元。此外，我们还对特定的电影角色进行了案例研究，如《绿巨人》和《天才》。我们打算发现真实数据集和CG数据集之间的面部表情、姿势脸和自然脸之间的差异和相似之处，并考虑视频中演员的性别。这项调查可以帮助教育、健康、娱乐、游戏、安全甚至法律事务中的几个知识领域，无论是使用真人还是虚拟人。我们的结果表明，无论性别如何，姿态数据集的AU强度都大于自发数据集。此外，当真实人脸转换为CG时，AU6和AU12的强度平滑度分别高达80%和45%。 et.al.|[2312.02128](http://arxiv.org/abs/2312.02128)|null|
|**2023-12-04**|**GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians**|我们介绍了GaussianAvatars，这是一种创建照片级真实感头部头像的新方法，在表情、姿势和视点方面都是完全可控的。其核心思想是基于3D高斯飞溅的动态3D表示，这些飞溅被装配到参数化的可变形人脸模型中。这种组合有助于照片真实感渲染，同时允许通过底层参数模型进行精确的动画控制，例如通过来自驱动序列的表达式传递或通过手动更改可变形模型参数。我们通过三角形的局部坐标系对每个splat进行参数化，并对显式位移偏移进行优化，以获得更准确的几何表示。在化身重建过程中，我们以端到端的方式联合优化可变形模型参数和高斯splat参数。我们在几个具有挑战性的场景中展示了我们的照片级真实感化身的动画功能。例如，我们展示了驾驶视频的重演，其中我们的方法显著优于现有作品。 et.al.|[2312.02069](http://arxiv.org/abs/2312.02069)|null|
|**2024-01-21**|**GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation**|为给定的主题构建生动的3D头部化身并在其上实现一系列动画是有价值但具有挑战性的。本文介绍了GaussianHead，它用各向异性的3D Gaussians对动作人头进行建模。在我们的框架中，分别构建了运动变形场和多分辨率三平面来处理头部的动态几何和复杂纹理。值得注意的是，我们对每个高斯强加了一个排他性的推导方案，该方案通过一组用于位置变换的可学习参数生成其多个对偶。通过这种设计，我们可以紧凑而准确地对高斯人的外观信息进行编码，即使是那些将头部的特定部件与复杂结构相匹配的信息。此外，还采用了新添加的高斯算子的继承推导策略，以促进训练加速。大量实验表明，我们的方法可以产生高保真度渲染，在重建、跨身份再现和新的视图合成任务方面优于最先进的方法。我们的代码位于：https://github.com/chiehwangs/gaussian-head. et.al.|[2312.01632](http://arxiv.org/abs/2312.01632)|**[link](https://github.com/chiehwangs/gaussian-head)**|
|**2023-11-30**|**Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data**|现有的单镜头4D头部合成方法通常在3DMM重建的帮助下从单眼视频中学习，但后者同样具有挑战性，这限制了它们进行合理的4D头部合成。我们提出了一种通过大规模合成数据学习一次4D头部合成的方法。关键是首先通过对抗性学习从单眼图像中学习部分4D生成模型，将不同身份和全动作的多视角图像合成为训练数据；然后利用基于变换器的可动画三平面重建器来学习使用合成数据的4D头部重建。提出了一种新的学习策略，通过分解三维重建和再现的学习过程来增强对真实图像的可推广性。实验证明了我们优于现有技术。 et.al.|[2311.18729](http://arxiv.org/abs/2311.18729)|null|
|**2023-11-30**|**DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars**|DiffusionAvatars合成了一个人的高保真3D头像，提供了对姿势和表情的直观控制。我们提出了一种基于扩散的神经渲染器，它利用通用的2D先验来生成引人注目的人脸图像。为了粗略地指导表情和头部姿势，我们从目标视点渲染了一个神经参数头部模型（NPHM），该模型充当了人的代理几何体。此外，为了增强复杂面部表情的建模，我们通过交叉注意力将DiffusionAvatars直接设置在从NPHM获得的表情代码上。最后，为了合成不同观点和表达的一致表面细节，我们通过在NPHM的规范空间中的TriPlane查找，将可学习的空间特征装配到头部表面。我们在RGB视频和相应的人的跟踪NPHM网格上训练DiffusionAvatars，并在自我重演和动画场景中测试获得的化身。我们的实验表明，DiffusionAvatars为一个人的新颖姿势和表情生成了时间一致且具有视觉吸引力的视频，优于现有方法。 et.al.|[2311.18635](http://arxiv.org/abs/2311.18635)|null|
|**2023-11-29**|**AvatarStudio: High-fidelity and Animatable 3D Avatar Creation from Text**|我们研究仅从文本描述创建高保真度和可动画化的3D化身的问题。现有的文本到化身的方法要么局限于不能被动画化的静态化身，要么难以生成具有良好质量和精确姿势控制的可动画化化身。为了解决这些限制，我们提出了AvatarStudio，这是一个从粗到细的生成模型，用于为可动画化的人类化身生成显式纹理3D网格。具体而言，AvatarStudio从基于低分辨率NeRF的粗略生成表示开始，然后将SMPL引导的关节运动合并到显式网格表示中，以支持化身动画和高分辨率渲染。为了确保生成的化身的视图一致性和姿势可控性，我们引入了一个以DensePose为条件的二维扩散模型，用于分数蒸馏采样监督。通过有效利用铰接网格表示和DensePose条件扩散模型之间的协同作用，AvatarStudio可以从准备好动画的文本中创建高质量的化身，显著优于以前的方法。此外，它适用于许多应用，例如，多模式化身动画和风格引导的化身创建。有关更多结果，请参阅我们的项目页面：http://jeff95.me/projects/avatarstudio.html et.al.|[2311.17917](http://arxiv.org/abs/2311.17917)|null|
|**2023-11-29**|**HUGS: Human Gaussian Splats**|神经渲染的最新进展已经将训练和渲染时间提高了几个数量级。虽然这些方法展示了最先进的质量和速度，但它们是为静态场景的摄影测量而设计的，不能很好地推广到环境中自由移动的人类。在这项工作中，我们介绍了人类高斯飞溅（HUGS），它表示可动画化的人类以及使用3D高斯飞溅（3DGS）的场景。我们的方法只拍摄一个具有少量（50-100）帧的单眼视频，它会自动学习在30分钟内解开静态场景和完全可动画化的人类化身。我们利用SMPL身体模型来初始化人类高斯。为了捕捉SMPL未建模的细节（如布料、头发），我们允许3D高斯与人体模型偏离。为动画人类使用3D高斯带来了新的挑战，包括在表达高斯时产生的人工制品。我们建议联合优化线性混合蒙皮权重，以协调动画过程中各个高斯人的运动。我们的方法实现了人类的新颖姿势合成和人类和场景的新颖视角合成。我们以60 FPS的渲染速度实现了最先进的渲染质量，同时比之前的工作快了约100倍。我们的代码将在此处公布：https://github.com/apple/ml-hugs et.al.|[2311.17910](http://arxiv.org/abs/2311.17910)|null|
|**2023-12-24**|**Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation**|在人机交互应用中，生成生动而情绪化的3D协同语音手势对于虚拟化身动画至关重要。虽然现有的方法能够根据单个情绪标签生成手势，但它们忽略了具有情绪转换的长手势序列建模在真实场景中更实用。此外，缺乏具有情绪转换语音和相应的3D人类手势的大规模可用数据集也限制了该任务的解决。为了实现这一目标，我们首先结合了ChatGPT-4和音频修复方法来构建高保真的情感过渡人类语音。考虑到获得与动态修复的情绪转换音频相对应的真实3D姿势注释是极其困难的，我们提出了一种新的弱监督训练策略来鼓励权威姿势转换。具体来说，为了增强过渡手势与不同情绪手势的协调性，我们将两个不同情绪手势序列之间的时间关联表示建模为风格引导，并将其融入过渡生成中。我们进一步设计了一种基于可学习的过渡手势混合情绪标签的情绪混合机制，该机制提供弱监督。最后，我们提出了一个关键帧采样器，以提供长序列中有效的初始姿势提示，使我们能够生成不同的姿势。大量实验表明，我们的方法优于通过在我们新定义的情绪转换任务和数据集上调整单一情绪条件对应物而构建的最先进的模型。我们的代码和数据集将在项目页面上发布：https://xingqunqi-lab.github.io/Emo-Transition/. et.al.|[2311.17532](http://arxiv.org/abs/2311.17532)|null|
|**2023-12-04**|**AgentAvatar: Disentangling Planning, Driving and Rendering for Photorealistic Avatar Agents**|在这项研究中，我们的目标是创建交互式化身代理，该代理可以从视觉和行为角度自主规划和逼真地动画化细微的面部动作。给定关于环境和代理概况的高级输入，我们的框架利用LLM来生成化身代理面部运动的一系列详细文本描述。然后，我们的任务无关驱动引擎将这些描述处理为运动标记序列，随后将其转换为连续的运动嵌入，这些运动嵌入由我们的独立基于神经的渲染器进一步消耗，以生成最终的照片真实感化身动画。这些简化的过程使我们的框架能够适应各种非语言化身交互，包括单元和二元交互。我们的广泛研究，包括在新编译的和现有的数据集上进行的实验，其中包括两种类型的代理——一种能够与环境进行一元交互，另一种设计用于二元对话——验证了我们方法的有效性和多功能性。据我们所知，我们通过将LLM和神经渲染相结合，实现了化身代理的广义非语言预测和照片逼真渲染，迈出了一大步。 et.al.|[2311.17465](http://arxiv.org/abs/2311.17465)|null|
|**2023-11-30**|**Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation**|我们研究了创建一个可以从动画角色的单个图像实时控制的角色模型的问题。这个问题的解决方案将大大降低创建化身、计算机游戏和其他交互式应用程序的成本。Talking Head Anime 3（THA3）是一个开源项目，试图直接解决这个问题。它采用（1）动画角色上身的图像和（2）45维姿势矢量作为输入，并输出采用指定姿势的同一角色的新图像。可能的动作范围对于个人化身和某些类型的游戏角色来说足够有表现力。然而，该系统太慢，无法在普通PC上实时生成动画，并且其图像质量可以提高。在本文中，我们从两个方面改进THA3。首先，我们提出了新的构成网络架构，该架构基于U-Nets旋转角色的头部和身体，并在现代生成模型中广泛使用。新的架构始终产生比THA3基线更好的图像质量。尽管如此，它们也会使整个系统慢得多：生成一帧需要150毫秒。其次，我们提出了一种技术，将系统提取到一个小型网络（小于2MB）中，该网络可以使用消费者游戏GPU实时生成512x512个动画帧（低于30FPS），同时保持接近整个系统的图像质量。这一改进使整个系统在实时应用中具有实用性。 et.al.|[2311.17409](http://arxiv.org/abs/2311.17409)|null|
|**2023-11-28**|**Human Gaussian Splatting: Real-time Rendering of Animatable Avatars**|这项工作解决了从多视图视频中学习的真实感人体化身的实时渲染问题。虽然建模和渲染虚拟人的经典方法通常使用纹理网格，但最近的研究已经开发出了神经体表示，可以实现令人印象深刻的视觉质量。然而，这些模型很难实时渲染，并且当使用与训练观察不同的身体姿势为角色设置动画时，它们的质量会降低。我们提出了第一个基于3D高斯散射的可动画人体模型，该模型最近成为神经辐射场的一种非常有效的替代方案。我们的身体由规范空间中的一组高斯基元表示，这些基元以粗到细的方法变形，该方法结合了前向蒙皮和局部非刚性细化。我们描述了如何从多视图观察中以端到端的方式学习我们的人类高斯飞溅（\OURS）模型，并将其与最先进的服装身体新姿势合成方法进行比较。我们的方法提供了比THuman4数据集上最先进的PSNR 1.5dbB更好的PSNR，同时能够以20fps或更高的速度渲染。 et.al.|[2311.17113](http://arxiv.org/abs/2311.17113)|null|
|**2023-11-28**|**The Empathic Metaverse: An Assistive Bioresponsive Platform For Emotional Experience Sharing**|Metaverse有望成为一个未来的平台，重新定义沟通、社交和互动的意义。然而，重要的是，我们要考虑避免我们今天使用的社交媒体平台的陷阱；网络欺凌、缺乏透明度和社会整体错误的心理模式。在本文中，我们提出了移情元宇宙，这是一个优先考虑情感共享以获得帮助的虚拟平台。它旨在培养亲社会行为，无论是利己主义还是利他主义，以便我们未来的社会能够更好地相互关爱，相互帮助。为了实现这一点，我们建议该平台具有生物响应性；它对个体的生理和认知状态做出反应和适应，并通过精心设计的化身、环境和互动来反映这一点。我们从三个研究方向探讨了这一概念：生物反应化身、中介通信和辅助工具。 et.al.|[2311.16610](http://arxiv.org/abs/2311.16610)|null|
|**2023-11-29**|**Animatable 3D Gaussian: Fast and High-Quality Reconstruction of Multiple Human Avatars**|神经辐射场能够重建高质量的可驾驶人类化身，但训练和渲染成本高昂。为了减少消耗，我们提出了可动画化的3D高斯，它从输入图像和姿势中学习人类化身。我们通过在规范空间中建模一组蒙皮的3D高斯和相应的骨架，并根据输入的姿势将3D高斯变形到姿势空间，将3D高斯扩展到动态人类场景。我们引入了哈希编码的形状和外观来加快训练，并提出了与时间相关的环境遮挡，以在包含复杂运动和动态阴影的场景中实现高质量的重建。在新的视图合成和新的姿态合成任务上，我们的方法在训练时间、渲染速度和重建质量方面都优于现有方法。我们的方法可以很容易地扩展到多人场景，并且只需25秒的训练就可以在10人的场景中获得可比的新颖视图合成结果。 et.al.|[2311.16482](http://arxiv.org/abs/2311.16482)|**[link](https://github.com/jimmyYliu/Animatable-3D-Gaussian)**|
|**2023-11-27**|**Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling**|从RGB视频中建模可动画化的人类化身是一个长期存在且具有挑战性的问题。最近的工作通常采用基于MLP的神经辐射场（NeRF）来表示3D人类，但纯MLP仍然很难回归依赖于姿势的服装细节。为此，我们介绍了可动画化高斯，这是一种新的化身表示，它利用强大的2D细胞神经网络和3D高斯飞溅来创建高保真化身。为了将3D高斯与可动画化的化身相关联，我们从输入视频中学习一个参数模板，然后在两个前后规范高斯图上对模板进行参数化，其中每个像素代表一个3D高斯。学习到的模板适用于为连衣裙等宽松衣服建模的穿着服装。这种模板引导的2D参数化使我们能够使用强大的基于StyleGAN的CNN来学习用于建模详细动态外观的姿态相关高斯图。此外，我们还引入了一种姿态投影策略，以便在给定新颖姿态的情况下更好地进行泛化。总体而言，我们的方法可以创建具有动态、逼真和通用外观的逼真化身。实验表明，我们的方法优于其他最先进的方法。密码https://github.com/lizhe00/AnimatableGaussians et.al.|[2311.16096](http://arxiv.org/abs/2311.16096)|**[link](https://github.com/lizhe00/animatablegaussians)**|
|**2023-11-27**|**HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images**|至于人类化身重建，当代技术通常需要获取昂贵的数据，并且难以从少量的随意图像中获得令人满意的结果。在本文中，我们从几个镜头的无约束相册中研究这项任务。由于有限的数据量和动态的关节姿势，从这样的数据源重建人类化身是具有挑战性的。为了处理动态数据，我们将蒙皮机制与深度行进四面体（DMET）集成，以形成可驱动的四面体表示，该四面体表示驱动DMET生成的任意网格拓扑，用于自适应无约束图像。为了有效地从少炮数据中挖掘指导性信息，我们设计了一种少炮参考和少炮制导的两阶段优化方法。前者侧重于将化身身份与参考图像对齐，而后者旨在为看不见的区域生成看似合理的外观。总的来说，我们的框架名为HaveFun，可以进行化身重建、渲染和动画制作。在我们开发的基准上进行的大量实验表明，HaveFun在重建人体和手部方面表现出显著优越的性能。项目网站：https://seanchenxy.github.io/HaveFunWeb/. et.al.|[2311.15672](http://arxiv.org/abs/2311.15672)|null|
|**2023-11-26**|**Should I use metaverse or not? An investigation of university students behavioral intention to use MetaEducation technology**|元宇宙是一种新兴的技术趋势，它结合了虚拟现实和增强现实，为用户提供了一个完全数字化的环境，在这个环境中，他们可以通过数字化身呈现虚拟身份，并像在现实世界中一样与他人互动。它的应用范围广泛，如经济（进入加密货币领域）、金融、社会生活、工作环境、医疗保健、房地产和教育。在新冠肺炎和后新冠肺炎时代，大学迅速采用电子学习技术，为学生提供在线访问学习内容和平台的机会，使之前对整合此类技术或准备机构基础设施的考虑几乎过时。有鉴于此，本研究借鉴技术接受模型（TAM），提出了一个分析大学生在教育中使用元宇宙技术的接受度和意图的框架。本研究旨在调查学生在教育中使用元宇宙技术（以下简称元教育）的意愿与所选TAM结构之间的关系，包括态度、感知有用性、感知易用性、元宇宙技术在教育中的自我效能和主观规范。值得注意的是，自我效能感和主观规范对态度和感知有用性有积极影响，而感知易用性与态度或感知有用性没有很强的相关性。作者假设，该研究的结构之间的弱关联可能归因于对元教育及其潜在利益的有限了解。有必要对该研究提出的模型进行进一步的调查和分析，以全面了解高等教育领域接受和使用元教育技术的复杂动态 et.al.|[2311.15251](http://arxiv.org/abs/2311.15251)|null|
|**2023-11-26**|**GAIA: Zero-shot Talking Avatar Generation**|零镜头会说话的化身生成旨在从语音和单个肖像图像合成自然的会说话的视频。以前的方法依赖于特定领域的启发式方法，如基于扭曲的运动表示和3D变形模型，这限制了生成的化身的自然度和多样性。在这项工作中，我们介绍了GAIA（化身的生成AI），它消除了说话化身生成中的领域先验。鉴于语音仅驱动化身的运动，而化身的外观和背景在整个视频中通常保持不变，我们将我们的方法分为两个阶段：1）将每个帧分解为运动和外观表示；2） 生成以所述语音和参考肖像图像为条件的运动序列。我们收集了一个大规模的高质量谈话化身数据集，并在其上用不同的尺度（高达2B个参数）训练模型。实验结果验证了GAIA的优越性、可扩展性和灵活性：1）所得到的模型在自然度、多样性、唇同步质量和视觉质量方面优于以前的基线模型；2） 该框架是可扩展的，因为较大的模型产生更好的结果；3） 它是通用的，并支持不同的应用程序，如可控讲话化身生成和文本指示化身生成。 et.al.|[2311.15230](http://arxiv.org/abs/2311.15230)|null|
|**2023-12-04**|**A numerical study on the influence of geometry on the rupture risk of abdominal aortic aneurysms**|腹主动脉瘤（AAAs）是由于动脉壁弱化而引起的腹主动脉局部扩张。本工作使用血液动力学和AAA壁力学模拟，研究了最大横径与腹部高度之比（DHr）对AAA破裂风险的影响。我们考虑了两种理想化的AAA几何形状，即DHr比AAA2高的AAA1。两个本构模型，即牛顿模型和Carreau Yasuda模型已用于将血液建模为不可压缩流体。此外，为了描述AAA墙的行为，已经使用了三个本构模型，即线弹性、Saint-Venant Kirchhoff弹性和一个称为Raghavan-Vorp弹性的唯象有限应变模型。使用solids4Foam（一个建立在有限体积框架上的开源软件包）对AAA生物力学进行了数值模拟。血液动力学参数研究表明，与AAA2相比，AAA1具有较低的时间平均壁剪切应力（TAWSS）和较高的振荡剪切指数（OSI）。因此，AAA1对血栓沉积的易感性增加，因此AAA破裂的风险更高。此外，AAA1的峰值壁应力比AAA2高约8%。结论是DHr越高，破裂风险越大。 et.al.|[2311.14147](http://arxiv.org/abs/2311.14147)|null|
|**2023-11-22**|**GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar**|数字人，尤其是3D面部化身，在过去几年里引起了很多关注，因为它们是AR或VR中的沉浸式远程呈现等几个应用的支柱。尽管取得了进展，但从商品硬件重建的面部化身是不完整的，并且遗漏了头部侧面和后部的部分，严重限制了化身的可用性。先前工作中的这种限制源于他们对面跟踪的要求，而这对于纵断面图和后视图来说是失败的。为了解决这个问题，我们建议从图像中学习特定于个人的可动画化身，而不需要假设可以访问精确的面部表情跟踪。在我们方法的核心，我们利用3D感知生成模型，该模型经过训练，可以从训练数据中再现面部表情的分布。为了训练这个外观模型，我们只假设有一组具有相应相机参数的2D图像。为了控制模型，我们学习了从3DMM面部表情参数到生成模型的潜在空间的映射。这种映射可以通过对外观模型的潜在空间进行采样并从归一化的正面视图重建面部参数来学习，其中面部表情估计表现良好。利用该方案，我们将三维外观重建和动画控制解耦，以实现图像合成的高保真度。在一系列实验中，我们将我们提出的技术与最先进的单目方法进行了比较，并显示出优越的质量，同时不需要对训练数据进行表达跟踪。 et.al.|[2311.13655](http://arxiv.org/abs/2311.13655)|null|
|**2023-11-22**|**XAGen: 3D Expressive Human Avatars Generation**|3D感知GAN模型的最新进展使得能够生成逼真且可控的人体图像。然而，现有的方法侧重于对主要身体关节的控制，而忽略了对表情属性的操作，如面部表情、下巴姿势、手部姿势等。在这项工作中，我们提出了XAGen，这是第一个能够对身体、面部和手部进行表情控制的人类化身的3D生成模型。为了增强面部和手部等小规模区域的保真度，我们设计了一种多尺度、多部分的3D表示，对精细细节进行建模。基于这种表示，我们提出了一种多部分绘制技术，该技术可以解开身体、面部和手的合成，以简化模型训练并提高几何质量。此外，我们设计了多部分鉴别器，根据其外观和细粒度控制能力来评估生成的化身的质量。实验表明，XAGen在真实感、多样性和表达控制能力方面超越了最先进的方法。代码和数据将在https://showlab.github.io/xagen. et.al.|[2311.13574](http://arxiv.org/abs/2311.13574)|null|
|**2023-11-20**|**DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation**|数字化身的真实感对于实现具有自我表达和定制功能的远程呈现应用程序至关重要。这种写实主义的一个关键方面源于真实的体型和服装的物理准确性。虽然物理模拟可以为穿着衣服的人产生高质量、逼真的运动，但它们需要精确估计体型和高质量的服装资产，以及布料模拟的相关物理参数。然而，手动创建这些资产并校准其参数是劳动密集型的，需要专业知识。为了解决这一差距，我们提出了DiffAvatar，这是一种使用可微分模拟进行身体和服装协同优化的新方法。通过将物理模拟集成到优化循环中，并考虑到布料的复杂非线性行为及其与身体的复杂相互作用，我们的框架恢复了身体和服装的几何形状，并以物理上合理的方式提取了重要的材料参数。我们的实验表明，我们的方法生成了逼真的服装和体型，可以很容易地用于下游应用。 et.al.|[2311.12194](http://arxiv.org/abs/2311.12194)|null|
|**2023-11-20**|**Semantic-Preserved Point-based Human Avatar**|为了在AR/VR和数字娱乐中实现逼真的体验，我们提出了第一个基于点的人类化身模型，该模型体现了数字人类的整个表现范围。我们使用两个MLP来对依赖于姿势的变形和线性蒙皮（LBS）权重进行建模。外观的表示依赖于解码器和附加到每个点的特征。与其他隐式方法相比，定向点表示不仅提供了一种更直观的方式来建模人类化身动画，而且显著减少了训练和推理时间。此外，我们提出了一种新的方法来将语义信息从SMPL-X模型转移到点，这使得能够更好地理解人体运动。通过利用点的语义信息，我们可以通过在不同的主题之间交换相同类别的点来促进虚拟试穿和人类化身的合成。实验结果证明了我们提出的方法的有效性。 et.al.|[2311.11614](http://arxiv.org/abs/2311.11614)|**[link](https://github.com/l1346792580123/spa)**|
|**2023-11-15**|**Exploring T-Duality for Self-Dual Fields**|我们在森的形式主义中研究了T对偶的化身，以获得不同维度的自对偶场强度。当在圆上紧致时，这种形式主义被证明可以自然地适应IIA/IIB型理论之间的T-对偶关系，而不需要像通常那样手动施加自对偶约束。我们还继续研究二维目标时空的这种形式主义，并将其作为世界表理论进行研究。特别是，我们表明，森的行为提供了一种基于自然世界观的扭曲和不对称扭曲弦的理解。最后，我们证明了Sen形式中描述的左手性玻色子和右手性玻色子的 $\mathrm｛T｝\bar｛\mathrm{T｝｝$ 变形理论具有通过场论T对偶与最近研究的量子力学的可积变形相关的标度极限。 et.al.|[2311.09153](http://arxiv.org/abs/2311.09153)|null|
|**2023-11-14**|**Drivable 3D Gaussian Avatars**|我们提出了可驱动的三维高斯化身（D3GA），这是第一个用高斯飞溅渲染的人体三维可控模型。当前的照片真实感可驾驶化身需要在训练期间精确的3D配准，或者在测试期间密集的输入图像，或者两者兼而有之。基于神经辐射场的辐射场对于遥现应用来说也往往慢得令人望而却步。这项工作使用最近提出的3D高斯散射（3DGS）技术，以实时帧速率渲染真实的人类，使用密集校准的多视图视频作为输入。为了使这些基本体变形，我们偏离了常用的线性混合蒙皮（LBS）的点变形方法，使用了经典的体积变形方法：笼变形。考虑到它们较小的尺寸，我们用关节角度和关键点来驱动这些变形，这更适合于通信应用。当使用相同的训练和测试数据时，我们在九名不同体型、衣服和动作的受试者身上进行的实验比最先进的方法获得了更高质量的结果。 et.al.|[2311.08581](http://arxiv.org/abs/2311.08581)|null|
|**2023-11-13**|**ARWalker: A Virtual Walking Companion Application**|包括增强现实（AR）在内的扩展现实（XR）技术在过去几年中引起了人们的极大关注，并已被应用于教育、医疗保健和制造等多个领域。在本文中，我们旨在通过开发ARWalker来探索AR在生物力学和人类运动领域的应用，ARWalker是一种以虚拟步行伙伴（化身）为特征的AR应用。研究参与者与虚拟同伴密切同步行走，虚拟同伴的步态表现出年轻健康成年人步态的特点。因此，研究参与者可以将步态训练成化身的步态，从而恢复步态的健康特性，降低跌倒的风险。ARWalker尤其可以帮助老年人和患有疾病的人，因为他们步态病态，因此更容易摔倒。我们实现了ARWalker的原型，并在Microsoft Hololens 2耳机上运行时评估了其系统性能。 et.al.|[2311.07502](http://arxiv.org/abs/2311.07502)|null|
|**2023-11-11**|**CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer**|重构个性化的可动画化头部化身在AR/VR领域具有重要意义。用于实现3D变形模型（3DMM）的显式面部控制的现有方法通常依赖于单个对象的多视图图像或视频，这使得重建过程复杂。此外，传统的渲染管道非常耗时，限制了实时动画的可能性。在本文中，我们介绍了CVTHead，这是一种使用基于点的神经渲染从单个参考图像生成可控神经头化身的新方法。CVTHead将网格的稀疏顶点视为点集，并使用所提出的顶点特征变换器来学习每个顶点的局部特征描述符。这使得能够对所有顶点之间的长程依赖关系进行建模。VoxCeleb数据集的实验结果表明，CVTHead的性能与最先进的基于图形的方法相当。此外，它能够高效地渲染具有各种表情、头部姿势和相机视图的新型人头。这些属性可以使用3DMM的系数进行显式控制，有助于在实时场景中实现多功能和逼真的动画。 et.al.|[2311.06443](http://arxiv.org/abs/2311.06443)|**[link](https://github.com/howiema/cvthead)**|
|**2023-11-28**|**BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis**|从视频中合成逼真的4D人头头像对于VR/AR、远程呈现和视频游戏应用至关重要。尽管现有的基于神经辐射场（NeRF）的方法实现了高保真度的结果，但计算费用限制了它们在实时应用中的使用。为了克服这一限制，我们引入了BakedAvatar，这是一种用于实时神经头部化身合成的新表示，可部署在标准多边形光栅化管道中。我们的方法从学习的头部等值面中提取可变形的多层网格，并计算与表情、姿势和视图相关的外观，这些外观可以烘焙到静态纹理中，以实现高效的光栅化。因此，我们提出了一种用于神经头化身合成的三阶段流水线，包括学习连续变形、流形和辐射场，提取分层网格和纹理，以及通过差分光栅化微调纹理细节。实验结果表明，我们的表示生成的合成结果质量与其他最先进的方法相当，同时显著减少了所需的推理时间。我们进一步展示了单眼视频中的各种头部化身合成结果，包括视图合成、面部再现、表情编辑和姿势编辑，所有这些都以交互式帧速率进行。 et.al.|[2311.05521](http://arxiv.org/abs/2311.05521)|**[link](https://github.com/buaavrcg/BakedAvatar)**|
|**2023-11-15**|**Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data**|监督深度学习技术的最新进展已经证明了仅从面部视频远程测量人类生理生命体征（如光体积描记器、心率）的可能性。然而，这些方法的性能在很大程度上取决于真实标记数据的可用性和多样性。然而，用高质量的标签收集大规模的真实世界数据通常具有挑战性和资源密集型，这也引发了存储个人生物测量数据时的隐私问题。引入了具有照片逼真的合成化身的基于合成视频的数据集（例如，SCAMPS\cite{mcduf2022scamps}），以缓解这些问题，同时提供高质量的合成数据。然而，合成数据和真实世界数据之间存在显著差距，这阻碍了在这些合成数据集上训练的神经模型的泛化。在本文中，我们提出了几种将真实世界的噪声添加到合成生理信号和相应的面部视频中的措施。我们试验了单独和组合的增强方法，并在三个公共的真实世界数据集上评估了我们的框架。我们的结果表明，我们能够将平均MAE从6.9降低到2.0。 et.al.|[2311.05371](http://arxiv.org/abs/2311.05371)|null|
|**2023-11-09**|**Exploring and Analyzing the Effect of Avatar's Realism on Anxiety of English as Second Language (ESL) Speakers**|虚拟化身的出现为远程会议、教育等提供了创新机会。我们的研究调查了以英语为母语的人在互动过程中使用的化身的真实感如何影响以英语为第二语言（ESL）的人的焦虑水平。ESL参与者通过卡通化的头像、逼真的头像或实际的视频流与以英语为母语的人进行对话。我们测量了说ESL的人自我报告的焦虑和他们的焦虑生理指标。我们的研究结果表明，使用卡通头像或直接视频与母语人士互动可以降低ESL参与者的焦虑水平。然而，与酷似人类的化身的互动加剧了这些焦虑。这些见解对虚拟化身的设计和应用至关重要，尤其是在解决跨文化沟通障碍和增强用户体验方面。 et.al.|[2311.05126](http://arxiv.org/abs/2311.05126)|null|
|**2023-11-08**|**Clonemator: Composing Spatiotemporal Clones to Create Interactive Automators in Virtual Reality**|Clomator是一个虚拟现实（VR）系统，允许用户创建他们的化身克隆，并在空间和时间上对其进行配置，从而形成自动化器来完成复杂的任务。特别地，克隆可以（1）作为静态对象冻结在用户的身体姿势，（2）同步模仿用户的运动，以及（3）在一段时间后回放用户的动作序列。与缩放、位置重排、组选择和复制等传统技术相结合，Clomator使用户能够通过将复杂的任务分解为与克隆的一系列协作来迭代开发定制和可重复使用的解决方案。这绕过了实现专用交互技术或脚本，同时允许在VR应用程序中进行灵活的交互。我们通过几个例子展示了Clonemator的灵活性，并通过初步的用户研究验证了它的可用性和有效性。最后，我们讨论了Clomator在虚拟现实应用中的潜力，如游戏机制、空间交互技术和多机器人控制，并为未来的研究提供了我们的见解。 et.al.|[2311.04427](http://arxiv.org/abs/2311.04427)|null|
|**2023-11-07**|**Towards Garment Sewing Pattern Reconstruction from a Single Image**|服装缝纫图案代表了服装固有的休息形状，是时尚设计、虚拟试穿和数字化身等许多应用程序的核心。在这项工作中，我们探讨了从日常照片中恢复服装缝纫图案以增强这些应用的挑战性问题。为了解决这个问题，我们首先合成了一个名为SewFactory的多功能数据集，该数据集由大约1M张图像和基本缝纫图案组成，用于模型训练和定量评估。SewFactory涵盖了广泛的人体姿势、体型和缝纫图案，并由于所提出的人体纹理合成网络而具有逼真的外观。然后，我们提出了一种称为Sewformer的两级Transformer网络，它显著提高了缝纫模式的预测性能。大量实验表明，所提出的框架在恢复缝纫图案方面是有效的，并很好地推广到随意拍摄的人体照片中。代码、数据集和预训练模型可在以下位置获得：https://sewformer.github.io. et.al.|[2311.04218](http://arxiv.org/abs/2311.04218)|**[link](https://github.com/sail-sg/sewformer)**|
|**2023-11-03**|**SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data**|准确可靠的人体运动重建对于在虚拟现实（VR）和娱乐应用中创建全身化身的自然交互至关重要。随着Metaverse和社交应用程序越来越受欢迎，用户正在寻求具有成本效益的解决方案，以创建与商业动作捕捉系统制作的全身动画质量相当的全身动画。然而，为了提供负担得起的解决方案，重要的是尽量减少连接在受试者身体上的传感器的数量。不幸的是，从稀疏数据中重建全身姿态是一个严重不确定的问题。由于姿态的位置漂移和模糊性，一些使用IMU传感器的研究在重建姿态方面面临挑战。近年来，一些主流VR系统已经发布了提供位置和旋转信息的6自由度（6-DoF）跟踪设备。然而，大多数重建全身姿势的解决方案都依赖于传统的反向运动学（IK）解决方案，这些解决方案通常会产生不连续和不自然的姿势。在本文中，我们介绍了SparsePoser，这是一种新的基于深度学习的解决方案，用于从一组减少的六个跟踪设备中重建全身姿势。我们的系统集成了一个基于卷积的自动编码器，该编码器通过从运动捕捉数据中学习人体运动流形来合成高质量的连续人体姿势。然后，我们使用由多个轻量级前馈神经网络组成的学习IK组件，将手和脚调整到相应的跟踪器。我们在公开的运动捕捉数据集和实时现场演示上广泛评估了我们的方法。我们证明，我们的方法优于使用IMU传感器或6-DoF跟踪设备的最先进技术，可用于不同身体尺寸和比例的用户。 et.al.|[2311.02191](http://arxiv.org/abs/2311.02191)|**[link](https://github.com/UPC-ViRVIG/SparsePoser)**|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

