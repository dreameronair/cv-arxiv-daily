---
layout: default
---

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.31
> Usage instructions: [here](./docs/README.md#usage)

## 3D

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-27**|**Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting**|我们展示了将基于物理的固体和流体动画与3D高斯飞溅（3DGS）相结合的可行性，以在使用3DGS重建的虚拟场景中创建新的效果。利用底层表示中高斯飞溅和基于位置的动力学（PBD）的一致性，我们以内聚的方式管理渲染、视图合成以及固体和流体的动力学。与高斯着色器类似，我们使用添加的法线增强每个高斯内核，将内核的方向与曲面法线对齐，以优化PBD模拟。这种方法有效地消除了由固体中的旋转变形引起的尖锐噪声。它还允许我们集成基于物理的渲染，以增强流体上的动态曲面反射。因此，我们的框架能够真实地再现动态流体上的曲面高光，并促进场景对象和新视图中流体之间的交互。有关更多信息，请访问我们的项目页面\url{https://amysteriouscat.github.io/GaussianSplashing/}. et.al.|[2401.15318](http://arxiv.org/abs/2401.15318)|null|
|**2024-01-26**|**3D Reconstruction and New View Synthesis of Indoor Environments based on a Dual Neural Radiance Field**|同时实现室内环境的3D重建和新视图合成具有广泛的应用，但在技术上非常具有挑战性。基于隐式神经函数的现有技术方法可以获得极好的三维重建结果，但它们在新视图合成方面的性能可能不令人满意。神经辐射场（NeRF）的令人兴奋的发展彻底改变了新的视图合成，然而，基于NeRF的模型可能无法重建干净的几何表面。我们开发了一种双神经辐射场（Du-NeRF），以同时实现高质量的几何重建和视图渲染。Du-NeRF包含两个几何场，一个从SDF场导出以便于几何重建，另一个从密度场导出以促进新视图合成。Du NeRF的一个创新特征是，它将视图无关分量与密度场解耦，并将其用作标签来监督SDF场的学习过程。这减少了形状辐射模糊性，并使几何图形和颜色在学习过程中相互受益。大量实验表明，Du-NeRF可以显著提高室内环境下新视图合成和3D重建的性能，并且在构建包含不服从多视图颜色一致性的精细几何图形的区域时尤其有效。 et.al.|[2401.14726](http://arxiv.org/abs/2401.14726)|null|
|**2024-01-23**|**IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images**|尽管许多3D重建和新颖的视图合成方法允许从消费者相机轻松捕捉的多视图图像中真实地渲染场景，但它们在表示中烘焙照明，无法支持高级应用程序，如材质编辑、重新照明和虚拟对象插入。通过反向渲染重建基于物理的材料特性和照明有望实现此类应用。然而，大多数反向渲染技术都需要高动态范围（HDR）图像作为输入，这是大多数用户无法访问的设置。我们提出了一种方法，从多视图、低动态范围（LDR）图像中恢复场景的基于物理的材料特性和空间变化的HDR照明。我们在反向渲染管道中对LDR图像形成过程进行建模，并提出了一种新的材料、照明和相机响应模型的优化策略。与采用LDR或HDR输入的最先进的反向渲染方法相比，我们使用合成场景和真实场景来评估我们的方法。我们的方法优于以LDR图像作为输入的现有方法，并允许高度逼真的重新照明和对象插入。 et.al.|[2401.12977](http://arxiv.org/abs/2401.12977)|null|
|**2024-01-24**|**RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos**|我们介绍了一种新的在野外捕获的RGB-D对象数据集，称为WildRGB-D。与大多数现有的仅带有RGB捕获的以对象为中心的真实世界数据集不同，深度通道的直接捕获允许更好的3D注释和更广泛的下游应用。WildRGB-D包括大型类别级RGB-D对象视频，这些视频是用iPhone 360度环绕对象拍摄的。它包含大约8500个记录对象和近20000个RGB-D视频，涉及46个常见对象类别。这些视频是在三种设置的不同杂乱背景下拍摄的，以覆盖尽可能多的真实世界场景：（i）一个视频中的单个对象；（ii）一个视频中的多个对象；以及（iii）在一个视频中具有静止手的对象。该数据集由对象遮罩、真实世界比例的相机姿态和RGBD视频中重建的聚合点云进行注释。我们用WildRGB-D对四个任务进行了基准测试，包括新颖的视图合成、相机姿态估计、物体6d姿态估计和物体表面重建。我们的实验表明，RGB-D对象的大规模捕获为推进3D对象学习提供了巨大的潜力。我们的项目页面是https://wildrgbd.github.io/. et.al.|[2401.12592](http://arxiv.org/abs/2401.12592)|null|
|**2024-01-23**|**Methods and strategies for improving the novel view synthesis quality of neural radiation field**|神经辐射场（NeRF）技术可以从2D图像中学习场景的3D隐式模型，并合成逼真的新视图图像。该技术得到了业界的广泛关注，具有良好的应用前景。针对NeRF图像渲染质量需要提高的问题，近三年来，许多研究人员提出了各种方法来提高渲染质量。对最新的相关论文进行了分类和综述，分析了质量改进背后的技术原理，并讨论了质量改进方法的未来发展方向。这项研究可以帮助研究人员快速了解该领域技术的现状和发展脉络，有助于激发更高效算法的发展，促进NeRF技术在相关领域的应用。 et.al.|[2401.12451](http://arxiv.org/abs/2401.12451)|null|
|**2024-01-22**|**HG3-NeRF: Hierarchical Geometric, Semantic, and Photometric Guided Neural Radiance Fields for Sparse View Inputs**|神经辐射场（NeRF）作为一种通过从离散观测中学习场景表示的新型视图合成范式，已经引起了人们的极大关注。然而，当面对稀疏视图输入时，NeRF表现出明显的性能退化，从而限制了其进一步的适用性。在这项工作中，我们介绍了层次几何、语义和光度引导的NeRF（HG3-NeRF），这是一种新的方法，可以解决上述限制，并增强不同视图中几何、语义内容和外观的一致性。我们提出了分层几何制导（HGG），将运动结构的附加（SfM），即稀疏深度先验，纳入场景表示中。与直接深度监督不同，HGG从局部几何区域到全局几何区域对体积点进行采样，减轻了深度先验中固有偏差引起的偏差。此外，我们从不同分辨率的图像中观察到的语义一致性的显著变化中获得了灵感，并提出了层次语义引导（HSG）来学习粗到细的语义内容，该内容对应于粗到细场景表示。实验结果表明，HG3-NeRF可以在不同的标准基准上优于其他最先进的方法，并实现稀疏视图输入的高保真度合成结果。 et.al.|[2401.11711](http://arxiv.org/abs/2401.11711)|null|
|**2024-01-18**|**Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions**|隐式神经表示（INRs）的许多变体，其中神经网络被训练为信号的连续表示，对于下游任务具有巨大的实用性，包括新颖的视图合成、视频压缩和图像超分辨率。不幸的是，对这些网络的内部运作方式的研究严重不足。我们的工作，即解释隐式神经画布（XINC），是一个统一的框架，用于通过检查每个神经元对每个输出像素的贡献强度来解释INRs的特性。我们将这些贡献图的集合称为隐式神经画布，并使用这一概念来证明我们研究的INR学会了以令人惊讶的方式“观察”它们所代表的帧。例如，INR往往具有高度分布的表示。虽然缺乏高级对象语义，但它们对颜色和边缘有很大的偏见，而且几乎完全是空间不可知的。我们通过研究视频INR中对象在时间上的表现方式得出了我们的结论，使用聚类来可视化跨层和架构的相似神经元，并表明这是由运动主导的。这些见解证明了我们的分析框架的普遍有用性。我们的项目页面位于https://namithap10.github.io/xinc. et.al.|[2401.10217](http://arxiv.org/abs/2401.10217)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Objects With Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting**|从照片中重建对象并将其虚拟地放置在新环境中超出了标准的新颖视图合成任务，因为对象的外观不仅要适应新颖的视点，还要适应新的照明条件，而且反向渲染方法的评估依赖于新颖的视图合成数据或用于定量分析的简单合成数据集。这项工作提供了一个真实世界的数据集，用于测量重新照明对象的重建和渲染。为此，我们捕获了多个环境中相同对象的环境照明和地面实况图像，从而可以从一个环境中拍摄的图像中重建对象，并量化看不见的照明环境的渲染视图的质量。此外，我们介绍了一个由现成方法组成的简单基线，并在重新照明任务中测试了几种最先进的方法，表明新的视图合成不是衡量性能的可靠指标。代码和数据集可在https://github.com/isl-org/objects-with-lighting . et.al.|[2401.09126](http://arxiv.org/abs/2401.09126)|**[link](https://github.com/isl-org/objects-with-lighting)**|
|**2024-01-17**|**ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization**|在给定一组2D图像的情况下，神经辐射场（NeRF）在新视图合成（NVS）中表现出显著的性能。然而，NeRF训练需要每个输入视图的精确相机姿势，通常通过运动结构（SfM）管道获得。最近的作品试图放松这种限制，但它们仍然经常依赖于可以改进的体面的初始姿势。在这里，我们旨在消除姿势初始化的要求。我们提出了增量置信（ICON），这是一种从2D视频帧中训练NeRF的优化过程。ICON仅假设相机运动平滑，以估计姿势的初始猜测。此外，ICON引入了“置信度”：一种用于动态重加权梯度的模型质量自适应度量。ICON依赖于高置信度姿势来学习NeRF，并依赖于高置信度3D结构（由NeRF编码）来学习姿势。我们表明，与使用SfM姿势的方法相比，ICON在没有预先初始化姿势的情况下，在CO3D和HO3D中都实现了卓越的性能。 et.al.|[2401.08937](http://arxiv.org/abs/2401.08937)|null|
|**2024-01-16**|**Fast Dynamic 3D Object Generation from a Single-view Video**|由于缺乏4D标记的数据，从单视图视频生成动态三维（3D）对象是具有挑战性的。现有方法通过传输现成的图像生成模型（如分数蒸馏采样）来扩展文本到3D管道，但由于需要通过大型预训练模型反向传播信息有限的监督信号，这些方法的扩展速度慢且成本高（例如，每个对象150分钟）。为了解决这一限制，我们提出了一种高效的视频到4D对象生成框架，称为Efficient4D。它在不同的相机视图下生成高质量的时空一致图像，然后将其用作标记数据，直接训练具有显式点云几何结构的新型4D高斯飞溅模型，实现在连续相机轨迹下的实时渲染。对合成视频和真实视频的广泛实验表明，与现有技术的替代方案相比，Efficient4D的速度显著提高了10倍，同时保持了相同水平的创新视图合成质量。例如，Efficient4D只需14分钟即可对动态对象进行建模。 et.al.|[2401.08742](http://arxiv.org/abs/2401.08742)|null|
|**2024-01-18**|**ProvNeRF: Modeling per Point Provenance in NeRFs as a Stochastic Process**|神经辐射场（NeRFs）在各种应用中越来越受欢迎。然而，它们在稀疏视图设置中面临挑战，缺乏来自体积渲染的足够约束。在具有多种应用的经典计算机视觉中，从稀疏和无约束的相机重建和理解3D场景是一个长期存在的问题。虽然最近的工作在稀疏、无约束的视图场景中探索了NeRF，但他们的重点主要是增强重建和新颖的视图合成。我们的方法从更广泛的角度出发，提出了一个问题：“从哪里看到了每个点？”——这决定了我们对它的理解和重建程度。换句话说，我们的目标是在稀疏、无约束的视图下确定每个3D点及其相关信息的起源或出处。我们介绍了ProvNeRF，这是一个模型，通过合并每个点的来源，为每个点的可能源位置建模，丰富了传统的NeRF表示。我们通过扩展随机过程的隐式最大似然估计（IMLE）来实现这一点。值得注意的是，我们的方法与任何预先训练的NeRF模型和相关的训练相机姿势兼容。我们证明，与最先进的方法相比，逐点源建模提供了几个优势，包括不确定性估计、基于标准的视图选择和改进的新视图合成。请访问我们的项目页面https://provnerf.github.io et.al.|[2401.08140](http://arxiv.org/abs/2401.08140)|null|
|**2024-01-11**|**TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering**|基于点的辐射场渲染在新颖的视图合成中表现出了令人印象深刻的结果，提供了令人信服的渲染质量和计算效率的结合。然而，这一领域的最新方法也并非没有缺点。3D高斯飞溅[Kerbl和Kopanas等人2023]在渲染高度详细的场景时，由于模糊和模糊的伪影，会遇到困难。另一方面ADOP[R\“uckert et al.2022]可以容纳更清晰的图像，但神经重建网络会降低性能，它会处理时间不稳定性，并且无法有效地解决点云中的大间隙。在本文中，我们提出了TRIPS（Trilinear point Splatting），一种结合了Gaussian Splatting和ADOP思想的方法。我们的新技术背后的基本概念包括将点光栅化为屏幕空间图像金字塔，金字塔层的选择由投影点的大小决定。这种方法允许使用单个三线性写入来渲染任意大的点。然后使用轻量级神经网络来重建包括超出飞溅分辨率的细节的无孔图像。重要的是，我们的渲染管道是完全可微分的，允许点大小和位置的自动优化。我们的评估表明，TRIPS在渲染质量方面超过了现有的最先进的方法，同时在现成的硬件上保持了每秒60帧的实时帧速率。这种性能扩展到具有挑战性的场景，例如具有复杂几何形状、广阔景观和自动曝光镜头的场景。 et.al.|[2401.06003](http://arxiv.org/abs/2401.06003)|null|
|**2024-01-10**|**Diffusion Priors for Dynamic View Synthesis from Monocular Videos**|动态新颖视图合成旨在捕捉视频中视觉内容的时间演变。现有的方法很难区分运动和结构，特别是在相机姿态与对象运动相比未知或受约束的情况下。此外，仅使用来自参考图像的信息，对在给定视频中被遮挡或部分观察到的看不见的区域产生幻觉是极具挑战性的。为了解决这些问题，我们首先使用定制技术在视频帧上微调预训练的RGB-D扩散模型。随后，我们将微调模型中的知识提取为包括动态和静态神经辐射场（NeRF）分量的4D表示。所提出的流水线在保持场景同一性的同时实现了几何一致性。我们进行了深入的实验，以定性和定量地评估所提出方法的有效性。我们的结果证明了我们的方法在具有挑战性的情况下的稳健性和实用性，进一步推进了动态新视图合成。 et.al.|[2401.05583](http://arxiv.org/abs/2401.05583)|null|
|**2024-01-09**|**Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation**|生成扩散模型的最新进展已经实现了从单个输入图像或文本提示生成3D资产的先前不可行的能力。在这项工作中，我们的目标是提高这些模型的质量和功能，以完成创建可控、照片真实感的人类化身的任务。我们通过将3D可变形模型集成到最先进的多视角一致扩散方法中来实现这一点。我们证明了生成管道在关节式3D模型上的精确调节增强了基线模型在从单个图像合成新视图任务中的性能。更重要的是，这种集成有助于将面部表情和身体姿势控制无缝准确地结合到生成过程中。据我们所知，我们提出的框架是第一个扩散模型，能够从看不见的物体的单个图像中创建完全3D一致、可动画化和照片真实感的人类化身；大量的定量和定性评估证明了我们的方法在新视角和新表情合成任务上优于现有的最先进的化身创建模型。 et.al.|[2401.04728](http://arxiv.org/abs/2401.04728)|null|
|**2024-01-07**|**See360: Novel Panoramic View Interpolation**|我们介绍了See360，它是一种使用潜在空间视点估计进行360全景插值的通用且高效的框架。大多数现有的视图渲染方法只关注室内或合成三维环境，并渲染小对象的新视图。相反，我们建议将以相机为中心的视图合成作为2D仿射变换来处理，而不使用点云或深度图，这使得能够实现有效的360？全景场景探索。给定一对参考图像，See360模型通过提出的新颖的多尺度仿射变换器（MSAT）来学习渲染新颖的视图，从而实现从粗到细的特征渲染。我们还提出了一种条件潜在空间自动编码器（C-LAE）来实现任意角度的视图插值。为了展示我们方法的多功能性，我们引入了四个训练数据集，即UrbanCity360、Archinterior360、HungHom360和Lab360，它们是从室内和室外环境中收集的，用于真实和合成渲染。实验结果表明，该方法具有足够的通用性，可以实现四个数据集任意视图的实时绘制。此外，我们的See360模型可以应用于野外的视图合成：只需很短的额外训练时间（约10分钟），并且能够渲染未知的真实世界场景。See360的卓越性能为以相机为中心的视图渲染和360全景视图插值开辟了一个很有前途的方向。 et.al.|[2401.03431](http://arxiv.org/abs/2401.03431)|**[link](https://github.com/Holmes-Alan/See360)**|
|**2024-01-06**|**RustNeRF: Robust Neural Radiance Field with Low-Quality Images**|最近在神经辐射场（NeRF）方面的工作利用了多视图三维一致性，在三维场景建模和高保真新颖视图合成方面取得了令人印象深刻的结果。然而，也有局限性。首先，现有方法假设有足够的高质量图像可用于训练NeRF模型，忽略了真实世界的图像退化。其次，由于不同视图之间未建模的不一致性，以前的方法在训练集中难以解决模糊性问题。在这项工作中，我们为真实世界的高质量NeRF提供了RustNeRF。为了提高NeRF在真实世界输入下的鲁棒性，我们训练了一个包含真实世界退化建模的3D感知预处理网络。我们提出了一种新的隐式多视图引导来解决图像退化和恢复过程中的信息丢失问题。大量实验证明了RustNeRF在实际退化情况下优于现有方法。代码将被发布。 et.al.|[2401.03257](http://arxiv.org/abs/2401.03257)|null|
|**2024-01-02**|**Street Gaussians for Modeling Dynamic Urban Scenes**|本文旨在解决从单目视频中建模动态城市街道场景的问题。最近的方法扩展了NeRF，将跟踪车辆姿态纳入车辆动画，实现了动态城市街道场景的照片逼真视图合成。然而，它们的显著局限性在于训练和渲染速度慢，再加上履带车辆姿态对高精度的迫切需求。我们介绍了Street Gaussians，一种新的明确的场景表示，它解决了所有这些限制。具体地说，动态城市街道被表示为一组点云，这些点云配备有语义logits和3D Gaussians，每一个都与前景车辆或背景相关联。为了对前景对象车辆的动力学进行建模，使用可优化的跟踪姿态以及动态外观的动态球面谐波模型对每个对象点云进行优化。显式表示允许容易地合成目标车辆和背景，这反过来又允许在半小时的训练内以133 FPS（1066 $\times$ 1600分辨率）进行场景编辑操作和渲染。所提出的方法在多个具有挑战性的基准上进行了评估，包括KITTI和Waymo Open数据集。实验表明，在所有数据集上，所提出的方法始终优于最先进的方法。此外，尽管仅依赖于现成跟踪器的姿态，但所提出的表示提供的性能与使用精确的地面实况姿态所实现的性能不相上下。代码位于https://zju3dv.github.io/street_gaussians/. et.al.|[2401.01339](http://arxiv.org/abs/2401.01339)|null|
|**2024-01-01**|**Deblurring 3D Gaussian Splatting**|最近对辐射场的研究为具有照片级真实感渲染质量的新颖视图合成铺平了坚实的道路。然而，它们通常使用神经网络和体积绘制，这两种方法的训练成本很高，并且由于绘制时间长，阻碍了它们在各种实时应用中的广泛使用。最近，人们提出了一种基于3D高斯散射的方法来对3D场景进行建模，并在实时渲染图像的同时实现了显著的视觉质量。然而，如果训练图像模糊，则渲染质量会严重下降。模糊通常是由于镜头散焦、物体运动和相机抖动而产生的，它不可避免地会干扰干净图像的获取。先前的几项研究试图使用神经场从模糊的输入图像中渲染干净清晰的图像。然而，这些工作中的大多数仅设计用于基于体积渲染的神经辐射场，并不直接适用于基于光栅化的3D高斯散射方法。因此，我们提出了一种新的实时去模糊框架，即去模糊3D高斯散点，使用小型多层感知器（MLP）来操纵每个3D高斯的协方差来对场景模糊度进行建模。虽然去模糊的3D高斯飞溅仍然可以享受实时渲染，但它可以从模糊的图像中重建精细和清晰的细节。在基准上进行了各种实验，结果表明了我们的去模糊方法的有效性。定性结果可在https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/ et.al.|[2401.00834](http://arxiv.org/abs/2401.00834)|null|
|**2024-01-01**|**Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using Sharpness Prior**|神经辐射场（NeRF）在基于神经渲染的新视图合成中表现出了显著的性能。然而，当输入图像是在不完美的条件下拍摄的时，NeRF会遭受严重的视觉质量下降，例如照明不良、散焦模糊和透镜像差。特别是，当通常使用相机拍摄图像时，散焦模糊在图像中非常常见。尽管最近很少有研究提出渲染相当高质量的清晰图像，但它们仍然面临许多关键挑战。特别地，这些方法采用了基于多层感知器（MLP）的NeRF，这需要大量的计算时间。为了克服这些缺点，本文提出了一种新的技术Sharp-NeRF——一种基于网格的NeRF，它可以在半小时的训练内从输入的模糊图像中渲染干净清晰的图像。为此，我们使用了几个基于网格的内核来准确地对场景的清晰度/模糊度进行建模。计算像素的清晰度水平以学习空间变化的模糊核。我们在由模糊图像组成的基准上进行了实验，并评估了完全参考和非参考指标。定性和定量的结果表明，我们的方法以生动的色彩和精细的细节呈现出尖锐的新颖观点，并且它比以前的作品具有更快的训练时间。我们的项目页面位于https://benhenryl.github.io/SharpNeRF/ et.al.|[2401.00825](http://arxiv.org/abs/2401.00825)|**[link](https://github.com/benhenryl/sharpnerf)**|

## MLLM

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-30**|**Effect of Weight Quantization on Learning Models by Typical Case Analysis**|本文研究了大规模数据分析模型中使用的量化方法及其超参数选择。最近数据分析规模的激增显著增加了计算资源需求。为了解决这一问题，量化模型权重已成为深度学习等数据分析应用中的一种普遍做法。量化对于在计算资源有限的设备上部署大型模型尤为重要。然而，量化超参数的选择，如权重量化的位数和值范围，仍然是一个未充分探索的领域。在这项研究中，我们采用统计物理学的典型案例分析，特别是复制方法，来探索超参数对简单学习模型量化的影响。我们的分析得出了三个关键发现：（i）不稳定的超参数相位，称为复制对称性破坏，发生在少量比特和大量化宽度的情况下；（ii）存在最小化误差的最优量化宽度；以及（iii）量化延迟了过参数化的开始，有助于减轻双下降现象所指示的过拟合。我们还发现非均匀量化可以提高稳定性。此外，我们还开发了一种近似的消息传递算法来验证我们的理论结果。 et.al.|[2401.17269](http://arxiv.org/abs/2401.17269)|null|
|**2024-01-30**|**LATENTPATCH: A Non-Parametric Approach for Face Generation and Editing**|本文介绍了LatentPatch，这是一种从只有少量图像的小数据集生成真实图像的新方法。我们使用一个只有几千个参数的轻量级模型。与传统的微调预训练的大规模生成模型的少镜头生成方法不同，我们的方法是通过序列特征匹配直接在潜在分布上计算的，并且可以通过设计来解释。为了避免基于变换器、递归网络或自关注的大型模型不适合小型数据集，我们的方法受到非参数纹理合成和风格传递模型的启发，并确保从源分布中对生成的图像特征进行采样。我们扩展了以前的单图像模型，以处理少数图像，并证明我们的方法可以生成逼真的图像，以及实现条件采样和图像编辑。我们在人脸数据集上进行了实验，并表明我们的简单模型是有效的和通用的。 et.al.|[2401.16830](http://arxiv.org/abs/2401.16830)|null|
|**2024-01-30**|**Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models**|在不断发展的在线传播格局中，缓和仇恨言论（HS）是一个复杂的挑战，数字内容的多模式性质使其更加复杂。这项全面的调查深入研究了HS调节的最新进展，突出了大型语言模型（LLM）和大型多模式模型（LMM）的新兴作用。我们的探索始于对当前文献的彻底分析，揭示了传播HS中文本、视觉和听觉元素之间的微妙相互作用。我们发现了整合这些模式的显著趋势，主要是由于HS传播的复杂性和微妙性。重点放在LLM和LMM推动的进展上，它们已经开始重新定义检测和调节能力的边界。我们确定了研究中存在的差距，特别是在语言和文化代表性不足的背景下，以及处理资源匮乏环境的解决方案的必要性。该调查以前瞻性的视角结束，概述了未来研究的潜在途径，包括探索新的人工智能方法、适度的人工智能伦理治理，以及开发更细致、更具情境意识的系统。这篇全面的综述旨在促进进一步的研究，并促进合作，在数字时代采用更复杂、更负责任、更以人为本的HS节制方法。\脚注｛\textcolor｛red｝｛警告：本文包含令人反感的例子。 et.al.|[2401.16727](http://arxiv.org/abs/2401.16727)|null|
|**2024-01-29**|**InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model**|我们介绍了InternetLM-XComposer2，这是一种在自由形式文本图像合成和理解方面表现出色的尖端视觉语言模型。该模型超越了传统的视觉语言理解，从轮廓、详细的文本规范和参考图像等不同输入中熟练地制作交错的文本图像内容，实现了高度可定制的内容创建。InternLM-XComposer2提出了一种部分LoRA（PLoRA）方法，该方法将额外的LoRA参数专门应用于图像标记，以保持预先训练的语言知识的完整性，在精确的视觉理解和具有文学天赋的文本合成之间取得平衡。实验结果表明，基于InternetLM2-7B的InternetLM-XComposer2在生成高质量的长文本多模式内容方面具有优势，并且在各种基准测试中具有卓越的视觉语言理解性能，不仅显著优于现有的多模式模型，而且在某些评估中与GPT-4V和Gemini Pro相匹配甚至超过。这突出了它在多模式理解领域的非凡熟练程度。具有7B参数的InternetLM-XComposer2型号系列可在https://github.com/InternLM/InternLM-XComposer. et.al.|[2401.16420](http://arxiv.org/abs/2401.16420)|**[link](https://github.com/internlm/internlm-xcomposer)**|
|**2024-01-29**|**CO2: Efficient Distributed Training with Full Communication-Computation Overlap**|大型语言模型的根本成功取决于大规模分布式训练技术的有效实现。尽管如此，构建一个具有高速通信互连性的庞大、高性能集群的成本高得令人望而却步，而且只有知名实体才能访问。在这项工作中，我们的目标是降低这一障碍，并在带宽有限的集群中实现大规模训练的民主化。我们提出了一种称为CO2的新方法，该方法将本地更新和异步通信引入分布式数据并行训练，从而促进了CO2通信与计算的完全重叠。即使在受非常有限的通信带宽限制的广泛的多节点集群上，CO2也能够获得高可扩展性。我们进一步提出了过时间隙惩罚和外部动量修剪技术以及CO2，以增强其收敛性和训练稳定性。此外，CO2与成熟的ZeRO系列优化器无缝集成，通过大型模型训练减少模型状态的内存消耗。我们还提供了收敛性的数学证明，并建立了严格的上界。此外，我们通过一系列广泛的实践实验验证了我们的发现，这些实验涵盖了计算机视觉和自然语言处理领域的广泛任务。这些实验用于证明当在包括多达128个A100 GPU的配置中部署时，CO2在收敛性、通用性和可扩展性方面的能力。无论是在具有800Gbps RDMA或80Gbps TCP/IP节点间连接的集群上，结果都强调了CO2的卓越容量，可以极大地提高可扩展性。 et.al.|[2401.16265](http://arxiv.org/abs/2401.16265)|null|
|**2024-01-27**|**A Survey on Data Augmentation in Large Model Era**|包括大型语言和扩散模型在内的大型模型在接近人类水平的智力方面表现出了非凡的前景，引起了学术界和工业界的极大兴趣。然而，这些大型模型的训练需要大量高质量的数据，随着这些模型的不断更新，现有的高质量数据库可能很快就会耗尽。这一挑战推动了对数据增强方法的研究激增。利用大型模型，这些数据增强技术的性能优于传统方法。本文从全面的角度对大型模型驱动的数据扩充方法进行了详尽的综述。我们首先将相关研究分类为三大类：图像增强、文本增强和配对数据增强。接下来，我们将深入研究与基于模型的大型数据扩充相关的各种数据后处理技术。然后，我们的讨论扩展到包括自然语言处理、计算机视觉和音频信号处理中这些数据增强方法的一系列应用。我们继续评估基于模型的大型数据增强在不同场景中的成功和局限性。在结束我们的审查时，我们强调了数据增强领域未来探索的潜在挑战和途径。我们的目标是为研究人员提供关键见解，最终为更复杂的大型模型的发展做出贡献。我们一贯将相关开源材料保存在：https://github.com/MLGroup-JLU/LLM-data-aug-survey. et.al.|[2401.15422](http://arxiv.org/abs/2401.15422)|null|
|**2024-01-26**|**F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods**|大型语言模型（LLM）以其前所未有的性能而备受关注，导致越来越多的研究评估LLM。然而，这些评估基准仅限于评估遵循指令的能力，而忽略了培训前阶段出现的基本能力。以往的主观评价方法主要是根据API模型进行评分。然而，在缺乏参考文献的情况下，大型模型辨别细微差异的能力有限。为了弥补这一差距，我们提出了F-Eval，这是一个双语评估基准，用于评估基本能力，包括表达能力、常识能力和逻辑能力。F-Eval中的任务包括多选客观任务、开放式客观任务、基于参考的主观任务和无参考的主观工作。对于无参考的主观任务，我们设计了新的评估方法，作为API模型评分的替代方案。我们对13个高级LLM进行了评估。结果表明，与其他评估者相比，我们的评估方法显示出更高的相关系数和更大的差异。此外，我们还讨论了不同模型大小、维度和归一化方法的影响。我们预计F-Eval将促进LLM基本能力的研究。 et.al.|[2401.14869](http://arxiv.org/abs/2401.14869)|**[link](https://github.com/juliasun623/f-eval)**|
|**2024-01-26**|**Leveraging Large Models for Crafting Narrative Visualization: A Survey**|叙事可视化有效地将数据转化为引人入胜的故事，使复杂的信息可供广大受众访问。大型模型对叙事可视化至关重要，通过其处理自然语言查询和答案、生成连贯叙事和增强视觉交流的卓越能力，从本质上促进了这一过程。受先前叙事可视化工作和大型模型最新进展的启发，我们综合了大型模型在叙事可视化各个阶段的潜在任务和机会。在我们的研究中，我们调查了79篇论文，探讨了大型模型在叙事可视化自动化创建中的作用。我们提出了一个综合的管道，利用大型模型来制作叙事可视化，将回顾的文献分为四个基本阶段：数据、叙事、可视化和呈现。此外，我们确定了九个特定任务，其中大型模型应用于这些阶段。这项研究描绘了LM4NV过程中的挑战和机遇，为未来的研究提供了深刻的方向，并为该领域的学者提供了宝贵的指导。 et.al.|[2401.14010](http://arxiv.org/abs/2401.14010)|null|
|**2024-01-23**|**Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study**|在大模型时代，解码的自回归性质往往导致延迟成为一个重要的瓶颈。我们提出了一种非自回归LM融合ASR系统，该系统有效地利用了加速器硬件的并行化能力。我们的方法在每段评分模式中结合了通用语音模型（USM）和PaLM 2语言模型，在FLEURS和YouTube字幕上，所有语言的平均相对WER提高了10.8%和3.6%。此外，我们的综合消融研究分析了LLM大小、上下文长度、词汇大小、融合方法等关键参数。例如，我们探讨了LLM大小在128M到340B参数范围内对ASR性能的影响。这项研究为影响实际大规模LM融合语音识别系统有效性的因素提供了有价值的见解。 et.al.|[2401.12789](http://arxiv.org/abs/2401.12789)|null|
|**2024-01-23**|**Full-Stack Optimization for CAM-Only DNN Inference**|在过去的几年里，神经网络的准确性在各个领域都有了很大的提高。然而，它们不断增加的复杂性导致了冯·诺依曼系统中令人望而却步的高能量需求和延迟。最近已经提出了几种内存计算（CIM）系统来克服这一问题，但涉及大型模型的准确性、硬件可靠性和可扩展性的权衡仍然是一个挑战。此外，对于一些CIM设计，激活运动仍然需要相当大的时间和能量。本文探讨了三元权重神经网络和使用跑道存储器（RTM）实现的关联处理器（AP）的算法优化的组合。我们提出了一种新的编译流程，通过降低AP的算术强度来优化AP上的卷积。通过利用基于RTM的AP的优势，这种方法大大减少了存储器内的数据传输，同时解决了准确性、能效和可靠性问题。具体而言，与内存加速器中的纵横制相比，我们的解决方案将ImageNet上的ResNet-18推理的能效提高了7.5倍，同时保持了软件的准确性。 et.al.|[2401.12630](http://arxiv.org/abs/2401.12630)|null|
|**2024-01-17**|**Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native**|在本文中，我们研究了大型生成人工智能模型和云原生计算架构的交叉点。最近的大型机型，如ChatGPT，虽然其功能具有革命性，但面临着成本和高端GPU需求不断上升等挑战。通过对大模型即服务（LMaaS）和云数据库即服务（DBaaS）进行类比，我们描述了一种利用云原生技术（例如，多租户和无服务器计算）和高级机器学习运行时（例如，批量LoRA推理）的人工智能原生计算范式。这些共同努力旨在优化商品销售成本（COGS）并提高资源可及性。合并这两个领域的旅程才刚刚开始，我们希望刺激该领域未来的研究和发展。 et.al.|[2401.12230](http://arxiv.org/abs/2401.12230)|null|
|**2024-01-22**|**A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network**|多尺度卷积神经网络（CNN）在解决各种视觉问题方面表现出了非凡的能力。然而，融合不同尺度的特征总是导致模型尺寸大，阻碍了多尺度细胞神经网络在RGB-D显著性检测中的应用。在本文中，我们提出了一种定制的特征融合模块，称为显著性增强特征融合（SEFF），用于RGB-D显著性检测。SEFF利用相邻尺度的显著性图来增强融合所需的特征，从而产生更具代表性的融合特征。我们的多尺度RGB-D显著性检测器使用SEFF并处理具有三个不同尺度的图像。SEFF用于融合RGB和深度图像的特征，以及不同尺度的解码器的特征。在五个基准数据集上的大量实验已经证明了我们的方法优于十个SOTA显著性检测器。 et.al.|[2401.11914](http://arxiv.org/abs/2401.11914)|null|
|**2024-01-22**|**SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese**|我们介绍了SuperCLUE-Math6（SC-Math6），这是一个新的基准数据集，用于评估汉语模型的数学推理能力。SC-Math6是GSM8K数据集的升级中文版，具有更强的难度、多样性和应用范围。它由2000多个数学单词问题组成，需要多步骤推理并提供自然语言解决方案。我们提出了一种创新方案，根据不同推理步骤问题的性能来量化大型模型的推理能力。在12个具有代表性的中国模型上进行的实验表明，推理水平分层清晰，GPT-4等顶级模型表现出优异的性能。SC-Math6填补了中国数学推理基准的空白，为提高汉语模型的智能性提供了一个全面的测试平台。 et.al.|[2401.11819](http://arxiv.org/abs/2401.11819)|null|
|**2024-01-22**|**P2DT: Mitigating Forgetting in task-incremental Learning with progressive prompt Decision Transformer**|灾难性遗忘对管理由大型模型控制的智能代理提出了重大挑战，当这些代理面临新任务时，会导致性能下降。在我们的工作中，我们提出了一种新的解决方案——渐进式即时决策转换器（P2DT）。该方法通过在新任务训练期间动态附加决策令牌来增强基于转换器的模型，从而促进特定于任务的策略。我们的方法可以缓解持续和离线强化学习场景中的遗忘。此外，P2DT利用了通过传统强化学习从所有任务中收集的轨迹，并在训练过程中生成新的任务特定令牌，从而保留了先前研究的知识。初步结果表明，我们的模型有效地缓解了灾难性遗忘，并能很好地适应不断增加的任务环境。 et.al.|[2401.11666](http://arxiv.org/abs/2401.11666)|null|
|**2024-01-17**|**Generative Denoise Distillation: Simple Stochastic Noises Induce Efficient Knowledge Transfer for Dense Prediction**|知识提炼是将知识从更强大的大模型（教师）转移到更简单的模型（学生）的过程。目前的许多方法都涉及学生直接模仿老师的知识。然而，通过这些流行的方法，在学习的表示中仍然存在冗余，这些方法倾向于不加区别地学习每个空间位置的特征。为了从教师那里获得更紧凑的表示（概念特征），受人类认知的启发，我们提出了一种创新的方法，称为生成去噪蒸馏（GDD），其中将随机噪声添加到学生的概念特征中，以将其嵌入浅层网络生成的实例特征中。然后，将生成的实例特征与来自老师的实例知识对齐。我们对对象检测、实例分割和语义分割进行了广泛的实验，以证明我们的方法的通用性和有效性。值得注意的是，GDD在上述任务中实现了最先进的性能。我们通过增强基于ResNet-18的PspNet和DeepLabV3，在语义分割方面取得了实质性的改进，mIoU得分分别为74.67和77.69，超过了他们之前在20个类别的Cityscapes数据集上的69.85和73.20分。源代码位于https://github.com/ZhgLiu/GDD. et.al.|[2401.08332](http://arxiv.org/abs/2401.08332)|null|
|**2024-01-16**|**Multitask Learning in Minimally Invasive Surgical Vision: A Review**|微创手术（MIS）已经彻底改变了许多程序，并减少了患者的恢复时间和受伤风险。然而，MIS给外科团队带来了额外的复杂性和负担。数据驱动的手术视觉算法被认为是开发具有改进自主性的未来MIS系统的关键构建块。机器学习和计算机视觉的最新进展已成功应用于分析从MIS获得的视频，有望缓解MIS视频中的挑战。手术场景和动作理解包括多个相关任务，当单独解决时，这些任务可能会占用大量记忆，效率低下，并且无法捕捉任务关系。多任务学习（MTL）是一种利用来自多个相关任务的信息来提高性能和帮助泛化的学习范式，非常适合对MIS数据进行细粒度和高级理解。这篇综述概述了当前最先进的MTL系统，这些系统利用了从MIS获得的视频。除了列出已发表的方法外，我们还讨论了这些MTL系统的优点和局限性。此外，本文还分析了MTL在MIS中各个应用领域的文献，包括那些具有大型模型的文献，强调了显著的趋势、新的研究方向和发展。 et.al.|[2401.08256](http://arxiv.org/abs/2401.08256)|null|
|**2024-01-16**|**A Survey of Resource-efficient LLM and Multimodal Foundation Models**|大型基础模型，包括大型语言模型（LLM）、视觉转换器（ViT）、扩散和基于LLM的多模式模型，正在彻底改变从训练到部署的整个机器学习生命周期。然而，这些型号在多功能性和性能方面的巨大进步在硬件资源方面付出了巨大的代价。为了以可扩展和环境可持续的方式支持这些大型模型的发展，人们相当重视制定资源节约型战略。这项调查深入探讨了此类研究的关键重要性，考察了算法和系统两个方面。它提供了从现有文献中收集的全面分析和有价值的见解，涵盖了从尖端模型架构和训练/服务算法到实际系统设计和实现的广泛主题。这项调查的目标是全面了解当前的方法是如何应对大型基础模型带来的资源挑战的，并有可能激励未来在这一领域取得突破。 et.al.|[2401.08092](http://arxiv.org/abs/2401.08092)|**[link](https://github.com/ubiquitouslearning/efficient_foundation_model_survey)**|
|**2024-01-21**|**PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar Nonlinear Conservation Laws**|我们能为广泛的PDE相关科学学习任务建立一个单一的大型模型吗？这个模型能在没有任何微调的情况下推广到新的偏微分方程，甚至是新形式的偏微分函数吗？上下文中操作员学习和相应的上下文中操作员网络（ICON）模型代表了对这些问题的初步探索。ICON关于第一个问题的能力已经在前面进行了演示。在本文中，我们提出了一种使用ICON解决PDE问题的详细方法，并展示了单个ICON模型如何在适当设计的数据提示下，以不同的步长对不同的方程进行正向和反向预测。我们为第二个问题提供了积极的证据，即ICON可以很好地推广到一些具有新形式的偏微分方程，而不需要任何微调。这可以通过对一维标量非线性守恒定律的研究来证明，这是一个具有时间演化的偏微分方程族。我们还展示了如何通过将函数和方程转换到ICON的能力范围来拓宽ICON模型可以解决的问题范围。我们认为，本文的进展是朝着在上下文操作员学习框架下训练PDE相关任务的基础模型的目标迈出的重要一步。 et.al.|[2401.07364](http://arxiv.org/abs/2401.07364)|null|
|**2024-01-11**|**HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis**|单程序多数据并行（SPMD）最近被用于训练大型深度神经网络（DNN）。很少有研究探讨其在异构集群中的适用性，以充分利用可用资源进行大型模型学习。本文介绍了OurSystem，这是一个旨在加快异构集群上SPMD DNN训练的自动化系统。\OurSystem联合优化了张量分片策略、异构设备的分片率以及张量交换的通信方法，以实现SPMD并行性的优化分布式训练。我们新颖地将模型划分公式化为程序合成问题，在该问题中，我们在语义上类似于为单个设备设计的程序的分布式指令集上从头开始生成分布式程序，并使用基于a*的搜索算法系统地探索解空间。我们通过将其公式化为线性规划问题来推导最优张量分片率。此外，\OurSystem探索了异构集群中的张量通信优化，并将其集成为节目合成过程的一部分，用于自动选择最佳集体通信原语并应用充分因子广播技术。在具有代表性的工作负载上进行的大量实验表明，\OurSystem在异构集群上实现了高达2.41x的加速。 et.al.|[2401.05965](http://arxiv.org/abs/2401.05965)|**[link](https://github.com/alibaba/hap)**|
|**2024-01-11**|**MatSAM: Efficient Materials Microstructure Extraction via Visual Large Model**|准确有效地提取材料微观图像中的微观结构在探索结构-性能关系和优化工艺参数方面发挥着关键作用。基于深度学习的图像分割技术依赖于手动注释，耗时耗力，难以满足模型可移植性和泛化的要求。分段任意模型（SAM）是一种具有强大的深度特征表示和零镜头泛化能力的大型视觉模型，为图像分割提供了新的解决方案。然而，在没有人为注释的情况下直接应用SAM来分割材料微观图像中的微观结构并不能达到预期的结果，因为难以使其原生的即时工程适应材料微观图像的关键微观结构的密集和分散特征。在本文中，我们提出了一种基于SAM的通用高效的微观结构提取解决方案MatSAM。根据材料微观结构的分布和形状，设计了一种新的基于点的提示生成策略。它为不同的微观图像生成提示，融合感兴趣区域（ROI）关键点和网格关键点的提示，并集成后处理方法对材料微观结构进行定量表征。对于包括晶界和相在内的常见微观结构，MatSAM实现了优于传统方法的分割性能，甚至优于对光学显微镜（OM）和扫描电子显微镜（SEM）成像的18种材料微观结构进行评估的监督学习方法。我们相信，MatSAM可以显著降低材料微观结构定量表征的成本，并加快新材料的设计。 et.al.|[2401.05638](http://arxiv.org/abs/2401.05638)|null|

## Diffusion

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-30**|**OmniSCV: An Omnidirectional Synthetic Image Generator for Computer Vision**|全方位和360度图像在工业和消费社会中越来越普遍，引起了全方位计算机视觉的关注。它们的宽视场允许仅从图像中收集大量关于环境的信息。然而，这些图像的失真需要开发特定的算法来进行处理和解释。此外，大量的图像对于基于学习的计算机视觉算法的正确训练是必不可少的。在本文中，我们提出了一种用于生成具有语义和深度信息的全向图像数据集的工具。这些图像是从在虚幻引擎4的真实虚拟环境中通过接口插件获取的一组捕获中合成的。我们收集了各种著名的投影模型，如等矩形和圆柱形全景图、不同的鱼眼透镜、折反射系统和经验模型。此外，在我们的工具中，我们将真实感非中心投影系统包括为非中心全景和非中心折反射系统。据我们所知，这是文献中第一个报道的生成真实感非中心图像的工具。此外，由于全向图像是虚拟制作的，我们提供了关于语义和深度的像素信息，以及相机校准参数的完美知识。这允许创建具有像素精度的地面实况信息，用于训练学习算法和测试3D视觉方法。为了验证所提出的工具，测试了不同的计算机视觉算法，如从屈光和折反射中心图像中提取线，使用等矩形全景进行3D布局恢复和SLAM，以及从非中心全景进行3D重建。 et.al.|[2401.17061](http://arxiv.org/abs/2401.17061)|**[link](https://github.com/sbrunoberenguel/omniscv)**|
|**2024-01-30**|**Repositioning the Subject within Image**|当前的图像操作主要集中在静态操作上，例如替换图像中的特定区域或更改其整体样式。在本文中，我们介绍了一种创新的动态操作任务，主题重新定位。该任务涉及将用户指定的对象重新定位到所需位置，同时保持图像的保真度。我们的研究表明，主题重新定位的基本子任务，包括填补重新定位的主题留下的空白，重建主题的模糊部分，并将主题与周围区域相一致，可以有效地重新表述为一个统一的、及时指导的修复任务。因此，我们可以使用单个扩散生成模型来处理这些子任务，使用通过我们提出的任务反演技术学习的各种任务提示。此外，我们集成了预处理和后处理技术，以进一步提高受试者重新定位的质量。这些要素共同构成了我们的细分市场gEnerate和bLEnd（SEELE）框架。为了评估SEELE在受试者重新定位方面的有效性，我们组装了一个名为ReS的真实世界受试者再定位数据集。我们在ReS上的结果证明了重新定位图像生成的质量。 et.al.|[2401.16861](http://arxiv.org/abs/2401.16861)|null|
|**2024-01-30**|**X-ray Image Generation as a Method of Performance Prediction for Real-Time Inspection: a Case Study**|X射线成像可以有效地用于工业产品的高通量在线检测。然而，设计一个满足工业要求并实现高精度的系统是一个具有挑战性的问题。许多系统设置的效果是特定于应用程序的，并且很难提前预测。因此，通常使用经验规则和视觉观察来配置系统。所得到的系统的性能通过广泛的实验测试来表征。我们建议使用计算方法用对应于相同实验设置的生成图像来代替真实测量。通过这种方法，可以观察实验设置对大量数据的影响，并比传统方法更快地预测系统性能。我们认为，对于准确的性能预测来说，图像生成器的高精度可能是不必要的。我们提出了一种使用POD曲线来表征发电模型质量的定量方法。所提出的方法可以适用于各种应用，我们在家禽检查问题上进行了演示。我们展示了如何使用校准的图像生成模型来定量评估X射线曝光时间对检查系统性能的影响。 et.al.|[2401.16847](http://arxiv.org/abs/2401.16847)|null|
|**2024-01-29**|**Bridging Generative and Discriminative Models for Unified Visual Perception with Diffusion Priors**|扩散模型在图像生成中的非凡能力促使人们努力将其应用扩展到生成任务之外。然而，一个持续的挑战是缺乏一种统一的方法来将扩散模型应用于具有不同语义粒度要求的视觉感知任务。我们的目的是建立一个统一的视觉感知框架，利用生成模型和判别模型之间的潜在协同作用。在本文中，我们提出了Vermouth，这是一个简单而有效的框架，包括一个预先训练的稳定扩散（SD）模型，该模型包含丰富的生成先验，一个能够集成层次表示的统一头部（U-head），以及一个提供判别先验的自适应专家。全面的调查揭示了Vermouth的潜在特征，例如在不同的时间步长和不同的U-net阶段，隐藏在潜在变量中的感知粒度不同。我们强调，没有必要结合重量级或复杂的解码器来将扩散模型转换为有效的表示学习器。针对定制的判别模型进行的广泛比较评估展示了我们的方法在基于零镜头草图的图像检索（ZS-SBIR）、少镜头分类和开放词汇语义分割任务上的有效性。这些有希望的结果证明了扩散模型作为强大的学习者的潜力，确立了它们在提供信息丰富和稳健的视觉表征方面的意义。 et.al.|[2401.16459](http://arxiv.org/abs/2401.16459)|null|
|**2024-01-29**|**Spatial-Aware Latent Initialization for Controllable Image Generation**|最近，文本到图像的扩散模型已经证明了以文本输入为条件生成高质量图像的令人印象深刻的能力。然而，这些模型很难准确地遵循关于空间布局信息的文本指令。虽然之前的研究主要集中在将交叉注意力图与布局条件对齐，但它们忽略了初始化噪声对布局指导的影响。为了实现更好的布局控制，我们建议在去噪过程中利用空间感知的初始化噪声。具体而言，我们发现具有有限反转步骤的反转参考图像包含关于对象位置的宝贵空间意识，从而在生成的图像中产生类似的布局。基于这一观察结果，我们开发了一个开放的词汇框架，为每个布局条件定制空间感知的初始化噪声。在不修改除初始化噪声之外的其他模块的情况下，我们的方法可以作为即插即用模块无缝集成到其他无训练布局指导框架中。我们在可用的稳定扩散模型和COCO数据集上对我们的方法进行了定量和定性评估。配备了空间感知的潜在初始化，我们的方法在保留高质量内容的同时显著提高了布局指导的有效性。 et.al.|[2401.16157](http://arxiv.org/abs/2401.16157)|null|
|**2024-01-29**|**Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You**|文本到图像生成模型最近在图像质量、灵活性和文本对齐方面取得了惊人的成果，因此被应用于数量快速增长的应用中。通过提高多语言能力，一个更大的社区现在可以使用这种技术。然而，正如我们将要展示的那样，多语言模型与单语模型同样存在（性别）偏见。此外，人们自然期望这些模型能在不同语言之间提供相似的结果，但事实并非如此，而且语言之间存在重要差异。因此，我们提出了一个新的基准MAGBIG，旨在促进无性别偏见的多语言模型研究。我们研究了多语言T2I模型是否利用MAGBIG放大了性别偏见。为此，我们使用多语言提示来请求某一职业或特征的人的肖像图像（使用形容词）。我们的研究结果表明，模型不仅偏离了每个性别产生的可能性应该相等的规范假设，而且不同语言之间也存在很大差异。此外，我们研究了即时工程策略，即使用间接、中性的配方，作为这些偏见的可能补救措施。不幸的是，它们只在有限的程度上起作用，并导致更差的文本到图像对齐。因此，这项工作需要对图像生成器中不同语言的不同表示进行更多的研究。 et.al.|[2401.16092](http://arxiv.org/abs/2401.16092)|null|
|**2024-01-29**|**Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling**|我们介绍了Motion-I2V，这是一种用于一致可控的图像到视频生成（I2V）的新框架。与之前直接学习复杂的图像到视频映射的方法不同，Motion-I2V通过显式运动建模将I2V分解为两个阶段。对于第一阶段，我们提出了一种基于扩散的运动场预测器，该预测器专注于推导参考图像像素的轨迹。对于第二阶段，我们提出了运动增强的时间注意力来增强视频潜在扩散模型中的有限一维时间注意力。该模块可以在第一阶段预测轨迹的引导下，有效地将参考图像的特征传播到合成帧。与现有方法相比，即使在存在大的运动和视点变化的情况下，Motion-I2V也可以生成更一致的视频。通过为第一阶段训练稀疏轨迹ControlNet，Motion-I2V可以支持用户使用稀疏轨迹和区域注释精确控制运动轨迹和运动区域。这提供了I2V过程的更多可控性，而不仅仅依赖于文本指令。此外，Motion-I2V的第二阶段自然支持零镜头视频到视频的转换。定性和定量比较都证明了Motion-I2V在一致和可控的图像到视频生成方面优于现有方法。 et.al.|[2401.15977](http://arxiv.org/abs/2401.15977)|null|
|**2024-01-29**|**Diffusion Facial Forgery Detection**|检测扩散生成的图像最近已发展成为一个新兴的研究领域。现有的基于扩散的数据集主要集中于一般图像生成。然而，到目前为止，面部伪造的研究还较少，这会带来更严重的社会风险。为了解决这一差距，本文介绍了DiFF，这是一个专门用于人脸聚焦扩散生成图像的综合数据集。DiFF包括在四种条件下使用十三种不同的生成方法合成的500000多幅图像。特别是，该数据集利用了30000个精心收集的文本和视觉提示，确保了图像的合成具有高保真度和语义一致性。我们通过人体测试和几种有代表性的伪造检测方法在DiFF数据集上进行了广泛的实验。结果表明，人类观察者和自动检测器的二进制检测精度通常低于30%，这揭示了检测扩散生成的面部伪造品的挑战。此外，我们提出了一种边缘图正则化方法，以有效地增强现有检测器的泛化能力。 et.al.|[2401.15859](http://arxiv.org/abs/2401.15859)|null|
|**2024-01-29**|**2L3: Lifting Imperfect Generated 2D Images into Accurate 3D**|从单个图像重建3D对象是一个有趣但具有挑战性的问题。一个有前途的解决方案是利用多视图（MV）3D重建将生成的MV图像融合成一致的3D对象。然而，生成的图像通常存在照明不一致、几何体错位和视图稀疏的问题，导致重建质量较差。为了解决这些问题，我们提出了一种新的3D重建框架，该框架利用固有分解引导、瞬态单先验引导和视图增强来分别解决这三个问题。具体来说，我们首先利用阴影信息从生成的图像中解耦，以减少不一致照明的影响；然后，我们引入了具有视点相关瞬态编码的单声道先验来增强重构的法线；最后，我们设计了一种视图增强融合策略，最大限度地减少生成的稀疏视图中的像素级损失和增强的随机视图中的语义损失，从而获得视图一致的几何结构和详细的纹理。因此，我们的方法能够集成预先训练的MV图像生成器和基于神经网络的体积有符号距离函数（SDF）表示，用于单个图像到3D对象的重建。我们在各种数据集上评估了我们的框架，并证明了其在定量和定性评估中的卓越性能，这意味着3D对象重建方面取得了重大进展。与最新的同步梦想家方法相比，我们将倒角距离误差降低了约36%，PSNR提高了约30%。 et.al.|[2401.15841](http://arxiv.org/abs/2401.15841)|null|
|**2024-01-28**|**Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding**|随着大规模文本图像生成模型在文本图像生成领域取得了显著进展，人们提出了许多微调方法。然而，这些模型往往难以处理新颖的对象，尤其是一次性场景。我们提出的方法旨在以对象驱动的方式解决可推广性和保真度的挑战，仅使用单个输入图像和感兴趣的特定对象区域。为了提高可推广性并缓解过度拟合，在我们的范式中，在微调扩散模型之前，基于对象的外观及其类别初始化原型嵌入。在微调过程中，我们提出了一个类特征正则化，以保留对象类的先验知识。为了进一步提高保真度，我们引入了特定于对象的损失，它也可以用于植入多个对象。总体而言，我们提出的用于植入新对象的对象驱动方法可以与现有概念无缝集成，并且具有高保真度和通用性。我们的方法优于现有的几种方法。代码将被发布。 et.al.|[2401.15708](http://arxiv.org/abs/2401.15708)|null|
|**2024-01-30**|**Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation**|尽管在生成高质量图像的文本到图像模型方面取得了重大进展，但在复杂的文本提示背景下，这些方法仍然难以确保文本提示相对于图像的可控性，尤其是在保留对象属性和关系方面。在本文中，我们提出了CompAgent，这是一种无需训练的合成文本到图像生成方法，以大型语言模型（LLM）代理为核心。CompAgent的基本思想是以分而治之的方法论为前提的。给定包含多个概念（包括对象、属性和关系）的复杂文本提示，LLM代理首先对其进行分解，这需要提取单个对象及其相关属性，并预测连贯的场景布局。然后可以独立地征服这些单独的物体。随后，代理通过分析文本进行推理，计划并使用工具来组成这些孤立的对象。验证和人类反馈机制最终被纳入我们的代理中，以进一步纠正潜在的属性错误并细化生成的图像。在LLM代理的指导下，我们提出了一个无调整的多概念定制模型和布局到图像生成模型作为概念合成的工具，并提出了一种局部图像编辑方法作为与代理交互进行验证的工具。场景布局控制这些工具之间的图像生成过程，以防止多个对象之间的混淆。大量实验证明了我们的合成文本到图像生成方法的优越性：CompAgent在T2I CompBench（开放世界合成T2I生成的综合基准）上实现了10%以上的改进。对各种相关任务的扩展也说明了CompAgent在潜在应用中的灵活性。 et.al.|[2401.15688](http://arxiv.org/abs/2401.15688)|null|
|**2024-01-28**|**IntentTuner: An Interactive Framework for Integrating Human Intents in Fine-tuning Text-to-Image Generative Models**|微调有助于文本到图像生成模型适应新颖的概念（例如，风格和肖像），使用户能够打造创造性的定制内容。最近在微调方面的努力侧重于减少训练数据和减轻计算过载，但忽略了与用户意图的一致性，特别是在多模式训练数据的手动管理和面向意图的评估方面。根据一项针对微调从业者理解用户意图的形成性研究，我们提出了IntentTuner，这是一个交互式框架，在微调工作流程的每个阶段智能地结合了人类意图。IntentTuner使用户能够通过图像示例和文本描述阐明训练意图，并自动将其转换为有效的数据增强策略。此外，IntentTuner引入了新的度量来测量用户意图一致性，允许对模型训练进行意图感知监测和评估。应用程序示例和用户研究表明，与通用基线工具相比，IntentTuner简化了微调，减少了认知努力，并产生了更好的模型。 et.al.|[2401.15559](http://arxiv.org/abs/2401.15559)|null|
|**2024-01-27**|**GEM: Boost Simple Network for Glass Surface Segmentation via Segment Anything Model and Data Synthesis**|由于玻璃区域的透明度和反射特性的模糊性，检测玻璃区域是一项具有挑战性的任务。这些透明眼镜共享透射的任意背景场景和反射物体的视觉外观，因此没有固定的图案。最近的视觉基础模型基于大量数据进行训练，在图像感知和图像生成方面表现出惊人的性能。为了更高精度地分割玻璃表面，我们充分利用了两个视觉基础模型：分割任何东西（SAM）和稳定扩散。具体来说，我们设计了一个名为GEM的简单玻璃表面分割器，它只由SAM主干、简单特征金字塔、辨别查询选择模块和掩码解码器组成。辨别查询选择可以自适应地识别玻璃表面特征，并将其分配为掩码解码器中的初始化查询。我们还提出了一个合成但逼真的大规模玻璃表面检测数据集，称为S-GSD，通过四种不同尺度的扩散模型，包含原始真实数据大小的1x、5x、10x和20x。该数据集是迁移学习的一个可行来源。合成数据的规模对迁移学习有积极影响，而随着数据量的增加，这种改善将逐渐饱和。大量实验表明，GEM在GSD-S验证集上达到了最先进的水平（IoU+2.1%）。代码和数据集可在：https://github.com/isbrycee/GEM-Glass-Segmentor. et.al.|[2401.15282](http://arxiv.org/abs/2401.15282)|null|
|**2024-01-25**|**Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote Sensing Imagery**|云层的存在严重损害了光学遥感图像的质量和有效性。然而，现有的基于深度学习（DL）的去云（CR）技术在准确重建图像的原始视觉真实性和详细语义内容方面遇到了困难。为了应对这一挑战，这项工作建议在数据和方法方面进行改进。在数据端，建立了一个0.5米空间分辨率的超分辨率基准，名为中大除云（CUHK-CR）。该基准包含了丰富的详细纹理和多样化的云覆盖范围，为设计和评估CR模型奠定了坚实的基础。从方法论的角度来看，提出了一种新的基于扩散的CR框架，称为扩散增强（DE）来执行渐进的纹理细节恢复，该框架在提高推理精度的同时减轻了训练难度。此外，开发了权重分配（WA）网络来动态调整特征融合的权重，从而进一步提高性能，特别是在超分辨率图像生成的情况下。此外，应用从粗到细的训练策略来有效地加快训练收敛，同时降低处理超分辨率图像所需的计算复杂度。在新建立的CUHK-CR和现有数据集（如RICE）上的大量实验证实，所提出的DE框架在感知质量和信号保真度方面都优于现有的基于DL的方法。 et.al.|[2401.15105](http://arxiv.org/abs/2401.15105)|null|
|**2024-01-26**|**Annotated Hands for Generative Models**|诸如GANs和扩散模型之类的生成模型已经展示了令人印象深刻的图像生成能力。尽管取得了这些成功，但这些系统在用手创建图像方面却出奇地差。我们为生成模型提出了一种新的训练框架，该框架大大提高了此类系统创建手部图像的能力。我们的方法是用三个额外的通道来增强训练图像，这些通道为图像中的手提供注释。这些注释提供了附加结构，该附加结构诱使生成模型产生更高质量的手部图像。我们在两个不同的生成模型上演示了这种方法：生成对抗性网络和扩散模型。我们在一个新的手部图像合成数据集和包含手的真实照片上展示了我们的方法。我们通过使用现成的手部检测器对手指关节识别进行更高的置信度来测量生成的手部的改进质量。 et.al.|[2401.15075](http://arxiv.org/abs/2401.15075)|**[link](https://github.com/YY-GX/Annotated-Hands-Dataset)**|
|**2024-01-26**|**Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with Large Vision-Language Model Support**|文本到图像模型的最新进展显著增强了图像生成能力，但开源模型在双语或中文支持方面仍存在显著差距。为了满足这一需求，我们提出了Taiyi Diffusion XL，这是一种新的中英文双语文本到图像模型，它是通过双语连续预训练过程扩展CLIP和Stable Diffusion XL。这种方法包括通过将最常用的汉字集成到CLIP的标记器和嵌入层中，再加上绝对位置编码扩展，来有效地扩展词汇。此外，我们通过大视觉语言模型丰富文本提示，从而获得更好的图像字幕，并具有更高的视觉质量。这些增强随后应用于下游的文本到图像模型。我们的实证结果表明，所开发的CLIP模型在双语图像文本检索方面表现出色。此外，泰翼Diffusion XL的双语图像生成能力超过了以前的型号。这项研究导致了Taiyi Diffusion XL模型的开发和开源，代表了图像生成领域的显著进步，尤其是在中文应用领域。这一贡献是朝着解决多模式研究中更多样化的语言支持需求迈出的一步。模型和演示可在\href上公开获取{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}｛此https URL｝，促进该领域的进一步研究和合作。 et.al.|[2401.14688](http://arxiv.org/abs/2401.14688)|null|
|**2024-01-25**|**Deconstructing Denoising Diffusion Models for Self-Supervised Learning**|在这项研究中，我们检验了最初用于图像生成的去噪扩散模型（DDM）的表示学习能力。我们的理念是解构DDM，逐渐将其转化为经典的去噪自动编码器（DAE）。这一解构过程使我们能够探索现代DDM的各个组成部分如何影响自我监督的表征学习。我们观察到，只有极少数现代组件对学习好的表征至关重要，而其他许多组件则不重要。我们的研究最终得出了一种高度简化的方法，在很大程度上类似于经典的DAE。我们希望我们的研究将在现代自我监督学习领域内重新激发人们对经典方法家族的兴趣。 et.al.|[2401.14404](http://arxiv.org/abs/2401.14404)|null|
|**2024-01-25**|**UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models**|在当代设计实践中，计算机视觉和生成人工智能（genAI）的融合代表着向更具互动性和包容性的过程的变革性转变。这些技术提供了图像分析和生成的新维度，这在城市景观重建的背景下尤其重要。本文提出了一种封装在原型应用程序中的新工作流程，旨在利用先进的图像分割和扩散模型之间的协同作用，实现城市设计的综合方法。我们的方法包括用于详细图像分割的OneFormer模型和通过ControlNet实现的用于从文本描述生成图像的稳定扩散XL（SDXL）扩散模型。验证结果表明，原型应用程序具有高度的性能，在对象检测和文本到图像生成方面都显示出显著的准确性。通过对各类城市景观特征的迭代评估，交叉点优于并集（IoU）和CLIP得分证明了这一点。初步测试包括将UrbanGenAI作为一种教育工具，增强设计教学法中的学习体验，并作为一种促进社区驱动的城市规划的参与性工具。早期研究结果表明，UrbanGenAI不仅推进了城市景观重建的技术前沿，还提供了显著的教学和参与式规划效益。UrbanGenAI正在进行的开发旨在进一步验证其在更广泛背景下的有效性，并集成实时反馈机制和3D建模能力等附加功能。关键词：生成人工智能；全景图像分割；扩散模型；城市景观设计；设计教育学；共同设计 et.al.|[2401.14379](http://arxiv.org/abs/2401.14379)|null|
|**2024-01-26**|**Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs**|生成模型的进步引发了人们对在遵守特定结构准则的同时生成图像的极大兴趣。场景图到图像生成是生成与给定场景图一致的图像的一个这样的任务。然而，视觉场景的复杂性对基于场景图中的指定关系准确对齐对象提出了挑战。现有的方法通过首先预测场景布局并使用对抗性训练从这些布局生成图像来完成这项任务。在这项工作中，我们介绍了一种从场景图生成图像的新方法，该方法无需预测中间布局。我们利用预先训练的文本到图像扩散模型和CLIP指导将图形知识转化为图像。为此，我们首先预训练我们的图编码器，使用基于GAN的训练将图特征与相应图像的CLIP特征对齐。此外，我们将图特征与给定场景图中存在的对象标签的CLIP嵌入相融合，以创建图一致的CLIP引导的条件信号。在条件输入中，对象嵌入提供图像的粗略结构，而图特征提供基于对象之间关系的结构对齐。最后，我们用具有重建和CLIP对准损失的图一致性条件信号来微调预训练的扩散模型。详细的实验表明，我们的方法在COCO材料和Visual Genome数据集的标准基准上优于现有方法。 et.al.|[2401.14111](http://arxiv.org/abs/2401.14111)|null|
|**2024-01-30**|**CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion**|大规模的文本到图像生成模型已经取得了令人印象深刻的进展，展示了它们合成大量高质量图像的能力。然而，将这些模型用于艺术图像编辑带来了两个重大挑战。首先，用户很难精心制作文本提示，详细说明输入图像的视觉元素。其次，流行的模型在特定区域进行修改时，经常会破坏整体艺术风格，使艺术作品的连贯性和美学统一性变得复杂。为了克服这些障碍，我们构建了创新的统一框架CreativeSynth，该框架基于扩散模型，能够在艺术图像生成领域协调多模式输入和多任务。通过将多模式特征与定制的注意力机制相结合，CreativeSynth通过反转和实时风格转移，促进了将真实世界的语义内容导入艺术领域。这允许对图像样式和内容进行精确操作，同时保持原始模型参数的完整性。严格的定性和定量评估强调，CreativeSynth擅长增强艺术图像的保真度，并保留其固有的美学本质。通过弥合生成模型和艺术技巧之间的差距，CreativeSynth成为一个自定义的数字调色板。 et.al.|[2401.14066](http://arxiv.org/abs/2401.14066)|null|

## avatar

| Publish Date | Title | Abstract | PDF | Code |
|:---------|:-----------------------|:---------------------------------|:------|:------|
|**2024-01-29**|**DressCode: Autoregressively Sewing and Generating Garments from Text Guidance**|服装在人类外表中的重要作用突显了服装数字化对数字人类创造的重要性。3D内容创作的最新进展对数字人类创作至关重要。尽管如此，基于文本指导的服装生成仍处于萌芽阶段。我们介绍了一个文本驱动的3D服装生成框架DressCode，旨在为新手民主化设计，并在时尚设计、虚拟试穿和数字人类创作方面提供巨大潜力。对于我们的框架，我们首先介绍了SewingGPT，这是一种基于GPT的架构，将交叉注意力与文本条件嵌入相结合，以生成具有文本指导的缝合模式。我们还为高质量、基于瓦片的PBR纹理生成量身定制了预训练的稳定扩散。通过利用大型语言模型，我们的框架通过自然语言交互生成CG友好型服装。我们的方法还便于图案完成和纹理编辑，通过用户友好的交互简化了设计师的流程。通过与其他最先进的方法进行全面的评估和比较，我们的方法展示了最佳的质量和与输入提示的一致性。用户研究进一步验证了我们的高质量渲染结果，突出了其在生产环境中的实用性和潜力。 et.al.|[2401.16465](http://arxiv.org/abs/2401.16465)|null|
|**2024-01-29**|**Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis**|在生物特征安全成为现代身份验证系统基石的时代，确保这些生物特征样本的真实性至关重要。活体检测，即区分真实和伪造生物特征样本的能力，是这一挑战的前沿。这项研究对活跃度检测模型进行了全面评估，特别关注它们在跨数据库场景中的性能，这是一种因其复杂性和现实世界相关性而臭名昭著的测试范式。我们的研究首先在单个数据集上仔细评估模型，揭示其性能指标的细微差别。深入研究了一半总错误率、错误接受率和错误拒绝率等指标，我们发现了对模型优势和劣势的宝贵见解。至关重要的是，我们对跨数据库测试的探索提供了一个独特的视角，突出了在一个数据集上训练和在另一个数据集中部署之间的鸿沟。与现有方法的比较分析，从卷积网络到更复杂的策略，丰富了我们对当前形势的理解。即使在最先进的模型之间，性能的差异也突显了这一领域的固有挑战。从本质上讲，这篇论文既是研究结果的宝库，也是对生物特征活体检测中更细致、数据多样和适应性更强的方法的明确呼吁。在真实性和欺骗性之间的动态舞蹈中，我们的工作为驾驭生物识别安全的发展节奏提供了蓝图。 et.al.|[2401.16232](http://arxiv.org/abs/2401.16232)|null|
|**2024-01-27**|**AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**|最近的社区在从稀疏的多视图视频构建照片逼真的可动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色呈现逼真的服装动态，因为它们主要依赖裸体模型进行人体建模，同时保留未建模的服装部分。这主要是因为宽松服装产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为4-8）生成宽松衣服中的可动画化人类化身的新方法。为了能够在这种情况下捕捉和学习宽松服装的外观，我们使用了从基于物理的模拟数据中获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕捉和渲染复杂的服装动力学。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动力学的新方法。为了使用粗略估计为看不见的衣服状态建立逼真的渲染，引入了一个以身体和衣服运动为条件的姿势驱动的可变形神经辐射场，提供了对这两个部分的显式控制。在测试时，可以从看不见的情况中捕捉新的服装姿势，这些姿势来自基于物理或神经网络的模拟器，以驱动看不见服装的动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松、动作各异的表演者。实验表明，我们的方法能够呈现出与身体高度偏离的自然服装动力学，并很好地推广到看不见的视图和姿势，超过了现有方法的性能。代码和数据将公开。 et.al.|[2401.15348](http://arxiv.org/abs/2401.15348)|null|
|**2024-01-09**|**DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation**|我们提出了DiffSHEG，这是一种基于扩散的方法，用于语音驱动的任意长度的整体三维表达和手势生成。虽然之前的工作专注于共同语音手势或单独生成表情，但同步表情和手势的联合生成仍然很少被探索。为了解决这一问题，我们基于扩散的协同语音运动生成转换器实现了从表情到手势的单向信息流，有助于改进联合表情手势分布的匹配。此外，我们还介绍了一种基于外绘的采样策略，用于扩散模型中的任意长序列生成，提供了灵活性和计算效率。我们的方法提供了一种实用的解决方案，可以产生由语音驱动的高质量同步表情和手势生成。在两个公共数据集上进行评估后，我们的方法在数量和质量上都达到了最先进的性能。此外，一项用户研究证实了DiffSHEG优于先前的方法。通过实现表达和同步运动的实时生成，DiffSHEG展示了其在数字人和具体代理开发中的各种应用潜力。 et.al.|[2401.04747](http://arxiv.org/abs/2401.04747)|null|
|**2024-01-11**|**Jump Cut Smoothing for Talking Heads**|跳跃式剪裁给观看体验带来了一种突然的、有时是不必要的变化。我们提出了一个新颖的框架来平滑这些跳跃剪辑，在谈话头部视频的背景下。我们利用视频中其他源帧中受试者的外观，将其与由DensePose关键点和面部标志驱动的中级表示融合。为了实现运动，我们在切割周围的结束帧之间插入关键点和地标。然后，我们使用来自关键点和源帧的图像翻译网络来合成像素。由于关键点可能包含错误，我们提出了一种跨模态注意力方案，以在每个关键点的多个选项中选择最合适的来源。通过利用这种中级表示，我们的方法可以获得比强大的视频插值基线更强的结果。我们在会说话的头部视频中演示了我们的方法，如剪切填充词、停顿，甚至随机剪切。我们的实验表明，即使在有挑战性的情况下，我们也可以实现无缝转换，其中会说话的头部在跳跃切割中旋转或剧烈移动。 et.al.|[2401.04718](http://arxiv.org/abs/2401.04718)|null|
|**2024-01-01**|**Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade**|我们介绍了一个图形感知的自动编码器集成框架，以及相关的形式主义和工具，旨在促进人文学科学术的深度学习。通过组成子体系结构以生成同构于人文领域的模型，我们保持了可解释性，同时为每个子体系结构选择提供功能签名，允许传统和计算研究人员在不干扰既定实践的情况下进行合作。我们展示了我们的方法在美国后大西洋奴隶贸易历史研究中的实际应用，并做出了一些具体的技术贡献：一种新的混合图卷积自动编码器机制，用于常见图拓扑的批处理策略，以及用于特定用例的掩蔽技术。越来越多的20多项研究证明了该框架扩大不同领域参与的有效性，这些研究既有与人文主义者的合作，也有机器学习文献中的既定任务，涵盖了各种领域和数据模式。我们对几种不同的体系结构选择进行了性能比较，最后列出了这项研究即将采取的下一步行动。 et.al.|[2401.00824](http://arxiv.org/abs/2401.00824)|null|
|**2023-12-23**|**Human101: Training 100+FPS Human Gaussians in 100s from 1 View**|从单视角视频中重构人体在虚拟现实领域发挥着关键作用。一种流行的应用场景需要快速重建高保真3D数字人，同时确保实时渲染和交互。现有的方法往往难以满足这两个要求。在本文中，我们介绍了Human101，这是一种新颖的框架，通过在100秒内训练3D高斯人并以100+FPS进行渲染，能够从单视图视频中生成高保真动态3D人体重建。我们的方法利用了3D高斯飞溅的优势，它提供了3D人类的明确而有效的表示。与之前基于NeRF的管道不同，Human101巧妙地应用了以人为中心的前向高斯动画方法来变形3D高斯的参数，从而提高了渲染速度（即，以令人印象深刻的60+FPS渲染1024个分辨率的图像，以100+FPS渲染512个分辨率的图片）。实验结果表明，我们的方法大大超过了当前的方法，每秒帧数激增了10倍，并提供了相当或卓越的渲染质量。代码和演示将在上发布https://github.com/longxiang-ai/Human101. et.al.|[2312.15258](http://arxiv.org/abs/2312.15258)|**[link](https://github.com/longxiang-ai/human101)**|
|**2023-12-23**|**Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models**|扩散模型以文本提示为条件，生成具有复杂细节的逼真图像。但是，当涉及到手、牙齿等人类特征时，大多数预先训练的模型都无法生成准确的图像。我们假设，可以通过注释良好的高质量数据来克服扩散模型的这种无能。在本文中，我们专门研究了使用扩散模型改进手-物体交互图像的生成。我们收集了一个注释良好的手-物交互合成数据集，该数据集使用Prompt Propose-Verify框架进行策划，并在其上微调稳定的扩散模型。我们根据CLIPScore、ImageReward、Fedility和alignment等定性和定量指标对图像-文本数据集进行评估，并显示出比当前最先进的基准测试更好的性能。 et.al.|[2312.15247](http://arxiv.org/abs/2312.15247)|null|
|**2023-12-23**|**TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**|直接语音到语音翻译通过引入自监督学习中获得的离散单元来实现高质量的结果。这种方法避免了与模型级联相关的延迟和级联错误。然而，与音频语音相比，有声头翻译，即将视听语音（即有声头视频）从一种语言转换为另一种语言，仍然面临着几个挑战：（1）现有方法总是依赖于级联，通过音频和文本进行合成，导致延迟和级联错误。（2） 会说话的头翻译有一套有限的参考框架。如果生成的翻译超过了原始语音的长度，则需要通过重复帧来补充视频序列，从而导致不和谐的视频转换。在这项工作中，我们提出了一个有声头翻译模型\textbf{TransFace}，它可以直接将视听语音翻译成其他语言的视听语音。它由一个语音到单元的翻译模型和一个基于单元的视听语音合成器Unit2Lip组成，前者将音频语音转换为离散单元，后者并行地从离散单元重新合成同步的视听语音。此外，我们还引入了一个有界持续时间预测器，确保等轴测头的平移并防止重复的参考帧。实验表明，我们提出的Unit2Lip模型显著提高了同步性（原始和生成的音频语音在LSE-C上分别为1.601和0.982），并将LRS2上的推理速度提高了4.35倍。此外，TransFace在LRS3-T和100%等时翻译上的Es-En和Fr-En的BLEU得分分别为61.93和47.55。 et.al.|[2312.15197](http://arxiv.org/abs/2312.15197)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|
|**2023-12-22**|**Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images**|随着元宇宙慢慢成为现实，以及数字人类的快速发展，对人脸原则风格编辑管道的需求势必会增加。我们通过引入潜在2语义自动编码器（L2SAE）来满足这一需求，这是一种生成式自动编码器模型，有助于对人脸图像中几个感兴趣区域（ROI）的风格属性进行高度本地化编辑。L2SAE学习编码图像的结构和风格信息的单独潜在表示。因此，允许对所选择的ROI进行保留结构的样式编辑。编码的结构表示是具有减少的空间维度的多通道2D张量，其捕获局部和全局结构属性。样式表示是捕获全局样式属性的1D张量。在我们的框架中，我们对结构表示进行切片，以建立与不同ROI的强且不纠缠的对应关系。因此，所选ROI的样式编辑相当于（a）从切片结构表示生成的ROI掩模和（b）具有全局样式变化的解码图像的简单组合，该全局样式变化是从操纵的（使用高斯噪声）全局样式和不变的结构张量生成的。在没有额外人力监督的情况下进行风格编辑是对SOTA风格编辑管道的重大胜利，因为大多数现有作品都需要额外的人力（监督）后期培训，才能将语义归因于风格编辑。我们还取消了基于迭代优化的反演或在训练后确定可控的潜在方向，这需要额外的计算成本高昂的操作。我们在多个应用程序中为相同的应用程序提供定性和定量结果，例如使用从多个数据集采样的测试图像进行选择性风格编辑和交换。 et.al.|[2312.15037](http://arxiv.org/abs/2312.15037)|null|
|**2023-12-21**|**From Past to Future: Digital Methods Towards Artefact Analysis**|在过去的二十年里，数字人文改变了人文和社会科学的格局，实现了对广泛数据集的高级计算分析和解释。值得注意的是，东南亚，特别是新加坡最近的举措侧重于对历史数据进行分类和归档，如艺术品、文学作品，尤其是考古文物。这项研究通过在两个不同的人工制品数据集上应用统计方法，展示了数字人文的巨大潜力。具体而言，我们展示了对公元1千年中期东南亚大陆“旭日”铸币的自动模具研究结果，随后对从新加坡中部殖民前圣安德鲁大教堂遗址挖掘的13-14世纪陶器的2D图像使用了无监督统计方法。这项研究提供了一个比较评估，展示了基于统计的方法对不同考古材料的解释和分析以及数字人文整体的变革影响。 et.al.|[2312.13790](http://arxiv.org/abs/2312.13790)|null|
|**2023-12-18**|**Relightable Neural Actor with Intrinsic Decomposition and Pose Control**|在视觉和图形学中，创建一个可欣赏、可驾驶和逼真的数字人类化身是一个具有挑战性的重要问题。人类是高度立体化的，会产生依赖姿势的外观效果，如自我阴影和皱纹，皮肤和衣服需要复杂且空间变化的BRDF模型。虽然最近的人类重新照明方法可以从多视图视频中恢复看似合理的材料光分解，但它们不能推广到新颖的姿势，并且仍然存在视觉伪影。为了解决这一问题，我们提出了Relightable Neural Actor，这是第一种基于视频的方法，用于学习照片真实感的神经人体模型，该模型可以重新照明，允许外观编辑，并可以由任意骨骼姿势控制。重要的是，为了学习我们的人类化身，我们只需要在已知但静态的照明条件下对人类进行多视图记录。为了实现这一点，我们用可驱动的密度场来表示演员的几何体，该密度场对姿势相关的服装变形进行建模，并提供3D和UV空间之间的映射，其中对法线、可见性和材质进行编码。为了在现实世界场景中评估我们的方法，我们收集了一个新的数据集，其中包括在室内和室外不同光照条件下记录的四个参与者，为人类重新照明提供了第一个此类基准，并展示了最先进的新人类姿势的重新照明结果。 et.al.|[2312.11587](http://arxiv.org/abs/2312.11587)|null|
|**2023-12-18**|**VectorTalker: SVG Talking Face Generation with Progressive Vectorisation**|高保真度和高效的音频驱动谈话头生成一直是计算机图形学和计算机视觉领域的一个关键研究课题。在这项工作中，我们研究了基于矢量图像的音频驱动的谈话头生成。与现有作品中使用最广泛的光栅图像直接动画相比，矢量图像具有良好的可扩展性，可用于多种应用。基于矢量图像的会说话的头生成面临两个主要挑战：高质量的矢量图像重建（相对于源肖像图像）和生动的动画（相对于音频信号）。为了解决这些问题，我们提出了一种新的可扩展矢量图形重建和动画方法，称为VectorTalker。具体而言，对于高保真度重建，VectorTalker以从粗到细的方式分层重建矢量图像。对于生动的音频驱动的面部动画，我们建议使用面部标志作为中间运动表示，并提出一个有效的标志驱动的矢量图像变形模块。我们的方法可以在一个统一的框架内处理各种风格的肖像图像，包括日本漫画、卡通和照片真实感图像。我们进行了广泛的定量和定性评估，实验结果证明了VectorTalker在矢量图形重建和音频驱动动画方面的优势。 et.al.|[2312.11568](http://arxiv.org/abs/2312.11568)|null|
|**2023-12-18**|**AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis**|音频驱动的会说话的头部合成是一个很有前途的课题，在数字人、电影制作和虚拟现实等领域有着广泛的应用。与之前的研究相比，最近基于NeRF的方法在质量和保真度方面显示出优势。然而，当涉及到少镜头会说话的头部生成时，在一个身份只有几秒钟会说话的视频的实际场景中，出现了两个限制：1）它们要么没有基本模型，作为快速收敛的面部先验，要么在构建先验时忽略了音频的重要性；2） 它们大多忽略了不同面部区域与音频之间的相关性，例如，嘴与音频相关，而耳朵与音频无关。在本文中，我们提出了音频增强神经辐射场（AE NeRF）来解决上述问题，它可以用最少的镜头数据集生成新扬声器的逼真肖像。具体来说，我们在参考方案的特征融合阶段引入了音频感知聚合模块，其中权重由参考图像和目标图像之间音频的相似性决定。然后，提出了一种基于双NeRF框架的音频对齐人脸生成策略，分别对音频相关区域和音频无关区域进行建模。大量实验表明，即使在有限的训练集或训练迭代中，AE NeRF在图像保真度、音频嘴唇同步和泛化能力方面也超过了最先进的技术。 et.al.|[2312.10921](http://arxiv.org/abs/2312.10921)|null|
|**2023-12-28**|**MM-TTS: Multi-modal Prompt based Style Transfer for Expressive Text-to-Speech Synthesis**|文本到语音中的风格转换任务是指将风格信息转换为文本内容，以生成具有特定风格的相应语音的过程。然而，现有的大多数风格转移方法要么基于固定的情感标签，要么基于参考语音片段，无法实现灵活的风格转移。最近，一些方法采用文本描述来引导风格转移。在本文中，我们提出了一个更灵活的多模式和风格可控的TTS框架MM-TTS。它可以在统一的多模态提示空间中使用任何模态作为提示，包括参考语音、情感面部图像和文本描述，以控制系统中生成的语音的风格。建模这种多模态风格可控的TTS的挑战主要在于两个方面：1）将多模态信息对齐到统一的风格空间中，以允许在单个系统中输入任意模态作为风格提示，从而赋予生成提示风格相关语音的能力。为了解决这些问题，我们提出了一种对齐的多模态提示编码器，该编码器将不同的模态嵌入到统一的风格空间中，支持不同模态的风格转换。此外，我们提出了一种新的自适应风格转换方法，称为风格自适应卷积，以实现更好的风格表示。此外，我们还设计了一种基于整流流的细化器，以解决Mel频谱图过于平滑的问题，并生成高保真度的音频。由于没有公共的多模态TTS数据集，我们构建了一个名为MEAD-TTS的数据集，该数据集与表达性谈话头部领域有关。我们在MEAD-TTS数据集和域外数据集上的实验表明，基于多模态提示的MM-TTS可以获得令人满意的结果。 et.al.|[2312.10687](http://arxiv.org/abs/2312.10687)|null|
|**2023-12-15**|**DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models**|扩散模型在各种下游生成任务中取得了显著的成功，但在重要且具有挑战性的表达型会说话的头部生成中仍有待探索。在这项工作中，我们提出了一个DreamTalk框架来填补这一空白，该框架采用了细致的设计来释放扩散模型在生成富有表现力的谈话头方面的潜力。具体来说，DreamTalk由三个关键组成部分组成：去噪网络、风格感知嘴唇专家和风格预测器。基于扩散的去噪网络能够在不同的表情中一致地合成高质量的音频驱动的面部运动。为了提高嘴唇动作的表现力和准确性，我们引入了一位风格敏感的嘴唇专家，他可以在注意说话风格的同时指导嘴唇同步。为了消除对表达参考视频或文本的需要，利用额外的基于扩散的风格预测器来直接从音频预测目标表达。通过这种方式，DreamTalk可以利用强大的扩散模型有效地生成富有表情的人脸，并减少对昂贵风格参考的依赖。实验结果表明，DreamTalk能够生成具有不同说话风格的照片逼真的说话脸，并实现准确的嘴唇运动，超过了现有的最先进的同类产品。 et.al.|[2312.09767](http://arxiv.org/abs/2312.09767)|null|
|**2023-12-15**|**Riveter: Measuring Power and Social Dynamics Between Entities**|Riveer为分析文本语料库中与实体相关的动词含义提供了一个完整的、易于使用的管道。我们在包装中预先填充了情感、权力和代理的内涵框架，这些框架已证明对在广泛的语料库中捕捉社会现象（如性别偏见）是有用的。几十年来，词汇框架一直是计算社会科学、数字人文科学和自然语言处理的基础工具，促进了文本语料库的多方面分析。但是，研究以动词为中心的词汇特别需要自然语言处理技能，这降低了其他研究人员对其的可及性。通过组织语言处理管道，为语料库中的所有实体提供完整的词典评分和可视化，并为用户提供针对特定研究问题的功能，Riveer大大提高了动词词汇的可访问性，并可以促进未来的广泛研究。 et.al.|[2312.09536](http://arxiv.org/abs/2312.09536)|**[link](https://github.com/maartensap/riveter-nlp)**|
|**2023-12-15**|**3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting**|我们介绍了一种使用3D高斯飞溅（3DGS）从单眼视频创建可动画化人类化身的方法。现有的基于神经辐射场（NeRFs）的方法实现了高质量的新视图/新姿态图像合成，但通常需要数天的训练，并且在推理时非常慢。最近，该社区探索了快速网格结构，以有效训练穿着衣服的化身。尽管训练速度极快，但这些方法几乎无法实现约15 FPS的交互式渲染帧速率。在本文中，我们使用3D高斯飞溅并学习非刚性变形网络来重建可动画化的穿着衣服的人类化身，这些化身可以在30分钟内训练并以实时帧速率（50+FPS）渲染。考虑到我们表示的显式性质，我们进一步在高斯均值向量和协方差矩阵上引入尽可能等距的正则化，增强了我们的模型在高度清晰的看不见姿态上的泛化能力。实验结果表明，与最先进的方法相比，我们的方法在从单目输入创建可动画化身方面实现了相当甚至更好的性能，同时在训练和推理方面分别快了400倍和250倍。 et.al.|[2312.09228](http://arxiv.org/abs/2312.09228)|null|
|**2023-12-12**|**On the Potential of an Independent Avatar to Augment Metaverse User Socialization Time**|我们提出了一种计算建模方法，旨在捕捉如何通过在元宇宙中使用独立和自主版本的数字表示来虚拟增加元宇宙用户的可用社交时间容量的细节。我们设想了一个以元宇宙为中心的传统化身概念的扩展：当用户不直接控制化身时，化身也可以被编程为独立操作，从而将其变成基于代理的数字人类表示。这样，用户可以虚拟地委托维持现有联系人所需的化身社交时间，以便最终维持空闲的非化身介导的社交时间，该空闲的社交时间可以潜在地投资于额外的社交活动。我们使用社会科学中选定的概念对环境进行建模，并确定特征变量：自我网络、社会存在和社会线索。然后，我们将用户的非化身中介空闲时间最大化问题公式化为线性优化。最后，我们分析了问题的可行区域，并对化身介导的交互的不同参数值可以实现的空闲时间提出了一些初步见解。 et.al.|[2312.07077](http://arxiv.org/abs/2312.07077)|null|

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

