[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.02.01
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#avatar>avatar</a></li>
  </ol>
</details>

## avatar

|Publish Date|Title|Abstract|PDF|Code|
|---|---|-----------------------------------|---|---|
|**2024-01-29**|**Democratizing the Creation of Animatable Facial Avatars**|在高端视觉效果管道中，（通常）使用定制的（且昂贵的）光舞台系统来扫描演员，以获取各种表情的几何形状和纹理。为了实现民主化，我们提出了一种新的管道，用于获得几何体和纹理以及足够的表达信息，以在不使用灯光舞台或任何其他高端硬件（或手动清理）的情况下构建定制的特定于个人的动画装备。一个关键的新颖想法包括扭曲真实世界的图像以与模板化身的几何形状对齐，并随后将扭曲的图像投影到模板化身的纹理中；重要的是，这使我们能够利用真实世界中的烘焙照明/纹理信息来创建替代面部特征（并弥合领域差距），以便进行几何重建。我们的方法不仅可以用于获得中性的表达几何体和去光纹理，而且还可以用于在将化身导入动画系统后改进化身（注意，这种导入往往是有损的，同时也会产生各种特征的幻觉）。由于默认的动画装备将包含与特定个体的模板表达式不正确对应的模板表达式，因此我们使用Simon Says方法来捕捉各种表达式并构建特定于个人的动画装备（像它们一样移动）。我们前面提到的翘曲/投影方法具有足够高的效率来重建对应于每个表达式的几何结构。 et.al.|[2401.16534](http://arxiv.org/abs/2401.16534)|null|
|**2024-01-27**|**AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**|最近的社区在从稀疏的多视图视频构建照片逼真的可动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色呈现逼真的服装动态，因为它们主要依赖裸体模型进行人体建模，同时保留未建模的服装部分。这主要是因为宽松服装产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为4-8）生成宽松衣服中的可动画化人类化身的新方法。为了能够在这种情况下捕捉和学习宽松服装的外观，我们使用了从基于物理的模拟数据中获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕捉和渲染复杂的服装动力学。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动力学的新方法。为了使用粗略估计为看不见的衣服状态建立逼真的渲染，引入了一个以身体和衣服运动为条件的姿势驱动的可变形神经辐射场，提供了对这两个部分的显式控制。在测试时，可以从看不见的情况中捕捉新的服装姿势，这些姿势来自基于物理或神经网络的模拟器，以驱动看不见服装的动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松、动作各异的表演者。实验表明，我们的方法能够呈现出与身体高度偏离的自然服装动力学，并很好地推广到看不见的视图和姿势，超过了现有方法的性能。代码和数据将公开。 et.al.|[2401.15348](http://arxiv.org/abs/2401.15348)|null|
|**2024-01-10**|**A General-purpose AI Avatar in Healthcare**|机器学习和自然语言处理的最新进展导致人工智能（AI）作为医疗保健行业的一种宝贵工具得到了快速发展。使用大型语言模型（LLM）作为会话代理或聊天机器人有可能帮助医生诊断患者，检测疾病的早期症状，并为患者提供健康建议。本文重点关注聊天机器人在医疗保健中的作用，并探索使用化身使人工智能交互对患者更具吸引力。通过使用三类提示字典和提示改进机制，展示了通用AI化身应用程序的框架。建议采用两阶段方法来微调通用人工智能语言模型，并创建不同的人工智能化身，与用户讨论医疗问题。即时工程增强了聊天机器人的对话能力和个性特征，培养了与患者更人性化的互动。最终，在聊天机器人中注入个性可能会增加患者的参与度。未来的研究方向包括研究如何提高聊天机器人对上下文的理解，并通过对专业医疗数据集进行微调来确保其输出的准确性。 et.al.|[2401.12981](http://arxiv.org/abs/2401.12981)|null|
|**2024-01-23**|**GALA: Generating Animatable Layered Assets from a Single Scan**|我们提出了GALA，这是一个框架，它以单层衣服的3D人体网格为输入，并将其分解为完整的多层3D资产。然后可以将输出与其他资产组合，以创建具有任何姿势的新颖的穿着衣服的人类化身。现有的重建方法通常将穿着衣服的人类视为单层几何体，并忽略了人类与发型、衣服和配饰的固有组成性，从而限制了网格在下游应用中的效用。将单层网格分解为单独的层是一项具有挑战性的任务，因为它需要为严重遮挡的区域合成合理的几何结构和纹理。此外，即使分解成功，网格也不能在姿势和体型方面进行归一化，从而无法实现具有新身份和姿势的连贯合成。为了应对这些挑战，我们建议利用预训练的2D扩散模型的一般知识作为人类和其他资产的几何和外观先验。我们首先使用从多视图2D分割中提取的3D表面分割来分离输入网格。然后，我们使用一种新的姿态引导的分数蒸馏采样（SDS）损失来合成姿态空间和规范空间中不同层的缺失几何。一旦我们完成了高保真3D几何体的修复，我们还将相同的SDS损失应用于其纹理，以获得包括初始遮挡区域在内的完整外观。通过一系列分解步骤，我们在一个共享的规范空间中获得了多层3D资产，这些资产根据姿势和人体形状进行了归一化，从而支持轻松合成新的身份，并用新的姿势进行复活。我们的实验证明了与现有解决方案相比，我们的方法在分解、规范化和组合任务方面的有效性。 et.al.|[2401.12979](http://arxiv.org/abs/2401.12979)|null|
|**2024-01-30**|**PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting**|尽管取得了很大进展，但实现实时高保真度的头部化身动画仍然很困难，并且现有的方法必须在速度和质量之间进行权衡。基于3DMM的方法通常无法对眼镜和发型等非面部结构进行建模，而神经隐式模型则存在变形不灵活和渲染效率低下的问题。尽管3D高斯已经被证明具有很好的几何表示和辐射场重建能力，但在头部化身创建中应用3D高斯仍然是一个主要挑战，因为3D高斯很难对由姿势和表情变化引起的头部形状变化进行建模。在本文中，我们介绍了PSAvatar，这是一种新的可动画化头部化身创建框架，它利用离散几何图元创建参数可变形形状模型，并使用3D高斯进行精细细节表示和高保真渲染。参数变形形状模型是一种基于点的变形形状模型（PMSM），它使用点代替网格进行三维表示，以实现增强的表示灵活性。PMSM首先通过在表面和网格外采样将FLAME网格转换为点，以不仅能够重建表面状结构，而且能够重建复杂的几何形状，如眼镜和发型。通过以综合分析的方式将这些点与头部形状对齐，PMSM可以利用3D高斯进行精细的细节表示和外观建模，从而能够创建高保真化身。我们展示了PSAvatar可以重建各种主题的高保真头部化身，并且化身可以实时动画化（以512 $\times$512的分辨率$\ge$ 25fps）。 et.al.|[2401.12900](http://arxiv.org/abs/2401.12900)|**[link](https://github.com/pcl3dv/PSAvatar)**|
|**2024-01-26**|**New spectral-parameter dependent solutions of the Yang-Baxter equation**|杨-巴克斯特方程（YBE）在研究可积多体量子系统中起着至关重要的作用。许多已知的YBE解决方案提供了从量子自旋链到超导系统的各种例子。可解统计力学模型及其化身也基于YBE。因此，YBE的新解决方案可以用于构建新的有趣的1D量子或2D经典系统，并具有许多其他深远的应用。在这项工作中，我们试图在对应于两个量子位情况的最低维度上找到YBE的（几乎）穷举的解集。我们开发了一种算法，该算法有可能用于生成YBE的新的高维解。 et.al.|[2401.12710](http://arxiv.org/abs/2401.12710)|null|
|**2024-01-20**|**UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures**|3D化身生成的最新进展已经引起了人们的极大关注。这些突破旨在制作更逼真的可动画化化身，缩小虚拟体验和现实世界体验之间的差距。大多数现有的工作都采用了分数蒸馏采样（SDS）损失，结合可微分的渲染器和文本条件，来指导扩散模型生成3D化身。然而，SDS通常生成的结果过于平滑，面部细节很少，因此与祖先采样相比缺乏多样性。另一方面，其他作品从单个图像生成3D化身，其中不想要的光照效果、透视图和较差的图像质量的挑战使得它们难以可靠地重建具有对齐的完整纹理的3D面部网格。在本文中，我们提出了一种新的3D化身生成方法，称为UltrAvatar，该方法具有增强的几何逼真度和卓越的基于物理的渲染（PBR）纹理质量，而没有不需要的照明。为此，该方法提出了一种漫射颜色提取模型和真实性引导的纹理漫射模型。前者去除了不需要的照明效果，以显示真实的漫反射颜色，从而可以在各种照明条件下渲染生成的化身。后者遵循两种基于梯度的指导，用于生成PBR纹理，以更好地与3D网格几何体对齐，呈现不同的人脸身份特征和细节。我们证明了所提出的方法的有效性和稳健性，在实验中大大优于最先进的方法。 et.al.|[2401.11078](http://arxiv.org/abs/2401.11078)|null|
|**2024-01-19**|**Fast Registration of Photorealistic Avatars for VR Facial Animation**|虚拟现实（VR）展示了社交互动的前景，这种互动比其他媒体更具沉浸感。其中的关键是能够在佩戴VR耳机的情况下准确地为自己肖像的真实感化身制作动画。尽管在离线设置中可以将特定于个人的化身高质量地注册到头戴式摄像机（HMC）图像，但通用实时模型的性能显著降低。由于相机视角倾斜和模态差异，在线注册也具有挑战性。在这项工作中，我们首先表明，化身和头戴式耳机相机图像之间的域间隙是困难的主要来源之一，其中基于转换器的架构在域一致性数据上实现了高精度，但当重新引入域间隙时会降低。基于这一发现，我们开发了一种系统设计，将问题解耦为两个部分：1）一个接受域输入的迭代细化模块，以及2）一个基于表情和头部姿势的当前估计的通用化身引导的图像到图像风格传递模块。这两个模块相互加强，因为当显示接近真实的例子时，图像风格的转移变得更容易，而更好的域间隙去除有助于配准。我们的系统可以高效地产生高质量的结果，从而无需昂贵的离线注册来生成个性化标签。我们通过在商品耳机上进行的大量实验验证了我们的方法的准确性和效率，证明了与直接回归方法和离线注册相比的显著改进。 et.al.|[2401.11002](http://arxiv.org/abs/2401.11002)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Tri $^{2}$-plane: Volumetric Avatar Reconstruction with Feature Pyramid**|近年来，在利用神经体积绘制进行面部化身重建方面取得了相当大的成就。尽管取得了显著的进步，但从单眼视频中重建复杂而动态的头部运动仍然需要捕捉和恢复细粒度的细节。在这项工作中，我们提出了一种新的方法，命名为Tri$^2$-plane，用于单目照片逼真的体积头部化身重建。与现有的依赖于单个三平面变形场进行动态面部建模的工作不同，所提出的tri$^2$-平面利用了特征金字塔和三个上下横向连接三平面的原理来改进细节。它在多个尺度上采样和渲染面部细节，从整个面部过渡到特定的局部区域，然后过渡到更精细的子区域。此外，我们在训练中加入了一种基于相机的几何感知滑动窗口方法作为增强，它提高了规范空间之外的鲁棒性，特别提高了交叉身份生成能力。实验结果表明，Tri$^2$ -平面不仅超越了现有的方法，而且通过实验在定量指标和定性评估方面都取得了卓越的性能。 et.al.|[2401.09386](http://arxiv.org/abs/2401.09386)|**[link](https://github.com/songluchuan/tri2plane)**|
|**2024-01-20**|**Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**|一次拍摄3D会说话的肖像生成旨在从看不见的图像中重建3D化身，然后用参考视频或音频将其动画化，以生成会说话的人像视频。现有的方法无法同时实现准确的三维化身重建和稳定的人脸动画。此外，虽然现有的作品主要集中在合成头部，但生成自然的躯干和背景片段以获得逼真的说话肖像视频也是至关重要的。为了解决这些限制，我们提出了Real3D Potrait，该框架（1）通过从3D人脸生成模型中提取3D先验知识的大图像到平面模型提高了单次3D重建能力；（2） 利用高效的运动适配器促进精确的运动条件动画；（3） 使用头部-躯干背景超分辨率模型来合成具有自然躯干运动和可切换背景的逼真视频；以及（4）支持具有可推广的音频到运动模型的单镜头音频驱动的谈话面部生成。大量实验表明，与以前的方法相比，Real3D Portrait很好地概括了看不见的身份，并生成了更逼真的谈话肖像视频。视频样本和源代码可在https://real3dportrait.github.io . et.al.|[2401.08503](http://arxiv.org/abs/2401.08503)|null|
|**2024-01-13**|**EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation**|随着虚拟环境的不断发展，对沉浸式和情感化体验的需求也在增长。为了满足这一需求，我们引入了使用优化知识提取（EVOKE）实现情感的虚拟化身映射，这是一种轻量级的情感识别框架，旨在将情感识别无缝集成到虚拟环境中的3D化身中。我们的方法利用了知识提取，包括在公开的DEAP数据集上进行多标签分类，该数据集涵盖了效价、唤醒和支配作为主要情绪类别。值得注意的是，我们的蒸馏模型，一个只有两个卷积层、参数比教师模型少18倍的CNN，取得了有竞争力的结果，其准确率为87%，同时所需的计算资源要少得多。这种性能和可部署性之间的平衡使我们的框架成为虚拟环境系统的理想选择。此外，多标签分类结果被用于将情绪映射到定制设计的3D化身上。 et.al.|[2401.06957](http://arxiv.org/abs/2401.06957)|null|
|**2024-01-10**|**Analysis and Perspectives on the ANA Avatar XPRIZE Competition**|ANA化身XPRIZE是一项为期四年的竞赛，旨在开发一种机器人“化身”系统，使人类操作员能够在远程环境中感知、交流和行动，就好像身体存在一样。比赛有一个独特的要求，即评委在人机界面上进行不到一个小时的培训后，将操作化身，并根据客观和主观评分标准对化身系统进行评判。本文从技术、评判和组织的角度对竞争进行了统一的总结和分析。我们研究了远程机器人技术的使用以及参赛团队在其化身系统中追求的创新，并将这些技术的使用与评委的任务表现和主观调查评分相关联。它还总结了团队领导、评委和组织者对比赛执行和影响的看法，为远程机器人和远程呈现的未来发展提供信息。 et.al.|[2401.05290](http://arxiv.org/abs/2401.05290)|null|
|**2024-01-09**|**A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars**|本文的目的是开发一个将口语翻译成手语的功能系统，称为Spoken2Sign翻译。Spoken2Sign任务与传统的手语到口语（Sign2Spoken）翻译是正交和互补的。为了实现Spoken2Sign翻译，我们提出了一个简单的基线，包括三个步骤：1）使用现有的Sign2Spoken基准创建一个光泽视频词典；2） 为字典中的每个符号视频估计3D符号；3） 借助生成的gloss-3D符号字典，训练Spoken2Sign模型，该模型由Text2Gloss翻译器、符号连接器和渲染模块组成。翻译结果然后通过标志化身来显示。据我们所知，我们是第一个以3D符号的输出格式呈现Spoken2Sign任务的公司。除了Spoken2Sign翻译的能力外，我们还证明了我们的方法的两个副产品——三维关键点增强和多视图理解——可以帮助实现基于关键点的手语理解。代码和型号将在https://github.com/FangyunWei/SLRT et.al.|[2401.04730](http://arxiv.org/abs/2401.04730)|**[link](https://github.com/FangyunWei/SLRT)**|
|**2024-01-09**|**Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation**|生成扩散模型的最新进展已经实现了从单个输入图像或文本提示生成3D资产的先前不可行的能力。在这项工作中，我们的目标是提高这些模型的质量和功能，以完成创建可控、照片真实感的人类化身的任务。我们通过将3D可变形模型集成到最先进的多视角一致扩散方法中来实现这一点。我们证明了生成管道在关节式3D模型上的精确调节增强了基线模型在从单个图像合成新视图任务中的性能。更重要的是，这种集成有助于将面部表情和身体姿势控制无缝准确地结合到生成过程中。据我们所知，我们提出的框架是第一个扩散模型，能够从看不见的物体的单个图像中创建完全3D一致、可动画化和照片真实感的人类化身；大量的定量和定性评估证明了我们的方法在新视角和新表情合成任务上优于现有的最先进的化身创建模型。 et.al.|[2401.04728](http://arxiv.org/abs/2401.04728)|null|
|**2024-01-08**|**AKN_Regie: bridging digital and performing arts**|AvatarStaging框架包括在混合戏剧舞台上指挥化身，实现物理演员的物质性和由动作捕捉或特定动画播放器实时控制的化身的虚拟性之间的共同存在。它导致了AKN_Regie创作工具的实现，该工具使用蓝图视觉语言编程，作为虚幻引擎（UE）视频游戏引擎的插件。本文描述了AKN_Regie作为非程序员戏剧人的工具的主要功能。它以UE特有的蓝图可视化语言提供了其实现的见解。它详细介绍了该工具是如何随着其在大约十部戏剧作品中的使用而演变的。讨论了AKN_Regie的非编程观点Plugin Perspective和编程文化对其发展的适应Blueprint Perspective之间的循环过程。最后，建议从C++的角度来加强技术问题的文化挪用，弥合深深涉及人类物质性的表演艺术和邀请发现新世界的化身之间的差距。 et.al.|[2401.03761](http://arxiv.org/abs/2401.03761)|null|
|**2024-01-07**|**Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness**|当前的说话化身大多基于说话的音频和文本生成共同说话手势，而不考虑说话者的非说话动作。此外，先前关于协同语音手势生成的工作已经基于单个手势数据集设计了网络结构，这导致数据量有限、可推广性受损和说话者运动受限。为了解决这些问题，我们引入了FreeTalker，据我们所知，它是第一个生成自发（例如，共同发言手势）和非自发（例如在讲台上移动）说话者动作的框架。具体来说，我们训练了一个基于扩散的说话人运动生成模型，该模型利用来自各种运动数据集的异构数据，采用语音驱动手势和文本驱动运动的统一表示。在推理过程中，我们利用无分类器引导来高度控制剪辑中的风格。此外，为了在片段之间创建平滑的过渡，我们使用DoubleTake，这是一种利用生成先验并确保无缝运动混合的方法。大量实验表明，我们的方法可以产生自然可控的扬声器运动。我们的代码、模型和演示可在\url上获得{https://youngseng.github.io/FreeTalker/}. et.al.|[2401.03476](http://arxiv.org/abs/2401.03476)|null|
|**2024-01-07**|**Deformation of a planar ferromagnetic elastic ribbon**|虽然对纯弹性带进行了广泛的研究，但在本文中，我们探讨了磁化对平面铁磁弹性带变形的影响。我们通过推导与弯曲的平面铁磁弹性带相关的前导阶磁能来开始研究。磁能和弹性能之和就是带状物的总能量。我们通过取总能量的第一个变化来导出平衡方程。然后，我们系统地确定和分析这些平衡方程在各种规范边界条件下的解。我们还分析了平衡解的稳定性。将我们的发现与研究充分的欧拉弹性进行比较，可以深入了解磁性对弹性带变形行为的影响。我们的分析有助于更深入地理解平面铁磁结构的磁化和机械响应之间的相互作用，并为理论和实际应用提供有价值的见解。 et.al.|[2401.03447](http://arxiv.org/abs/2401.03447)|null|
|**2024-01-09**|**Amplifying robotics capacities with a human touch: An immersive low-latency panoramic remote system**|人工智能和机器人技术在过去十年中取得了显著进步，改变了各个领域的工作模式和机会。这些技术的应用将社会推向了一个人与机器共生的时代。为了促进人类与智能机器人之间的高效通信，我们提出了“阿凡达”系统，这是一个沉浸式低延迟全景人机交互平台。我们设计并测试了一个坚固的移动平台原型，该平台集成了边缘计算单元、全景视频捕获设备、动力电池、机械臂和网络通信设备。在良好的网络条件下，我们实现了延迟357ms的低延迟高清全景视觉体验。操作员可以利用VR耳机和控制器对机器人和设备进行实时沉浸式控制。该系统能够实现跨越校园、省份、国家甚至大洲（纽约到深圳）的远距离远程控制。此外，该系统结合了用于地图和轨迹记录的视觉SLAM技术，提供了自主导航功能。我们相信，这个直观的系统平台可以提高人机协作的效率和情景体验，随着相关技术的进一步进步，它将成为人工智能与人类高效共生合作的通用工具。 et.al.|[2401.03398](http://arxiv.org/abs/2401.03398)|null|
|**2024-01-06**|**Dress-Me-Up: A Dataset & Method for Self-Supervised 3D Garment Retargeting**|我们提出了一种新的自监督框架，用于将非参数化的3D服装重新定位到任意形状和姿势的3D人体化身上，从而实现3D虚拟试穿（VTON）。现有的自监督3D重定目标方法仅支持参数化和规范化服装，这些服装只能覆盖在参数化身体上，例如SMPL。为了便于非参数化的服装和身体，我们提出了一种新的方法，该方法引入了基于Isomap嵌入的服装和人体之间的对应匹配，以获得两个网格之间的粗略对齐。我们在自监督设置中执行粗对准的神经细化。此外，我们利用拉普拉斯细节积分方法来保留输入服装的固有细节。为了评估我们的3D非参数服装重定目标框架，我们提出了一个由255件具有真实噪声和拓扑变形的真实世界服装组成的数据集。该数据集包含15名不同受试者以5种不同姿势穿着的价值44美元的独特服装，这些服装是使用多视图RGBD捕捉设置捕捉的。与现有的最先进的方法相比，我们在非参数服装和人类化身上显示出卓越的重定目标质量，这是所提出的非参数3D服装重定目标数据集上的第一个基线。 et.al.|[2401.03108](http://arxiv.org/abs/2401.03108)|null|

<p align=right>(<a href=#updated-on-20240201>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

