[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.31
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#avatar>avatar</a></li>
  </ol>
</details>

## avatar

|Publish Date|Title|Abstract|PDF|Code|
|---|---|-----------------------------------|---|---|
|**2024-01-27**|**AniDress: Animatable Loose-Dressed Avatar from Sparse Views Using Garment Rigging Model**|最近的社区在从稀疏的多视图视频构建照片逼真的可动画化身方面取得了重大进展。然而，当前的工作流程很难为宽松的角色呈现逼真的服装动态，因为它们主要依赖裸体模型进行人体建模，同时保留未建模的服装部分。这主要是因为宽松服装产生的变形是高度非刚性的，捕捉这种变形通常需要密集的视图作为监督。在本文中，我们介绍了AniDress，这是一种使用非常稀疏的多视图视频（在我们的设置中为4-8）生成宽松衣服中的可动画化人类化身的新方法。为了能够在这种情况下捕捉和学习宽松服装的外观，我们使用了从基于物理的模拟数据中获得的基于虚拟骨骼的服装索具模型。这样的模型使我们能够通过一组低维骨骼变换来捕捉和渲染复杂的服装动力学。从技术上讲，我们开发了一种从稀疏多视图视频中估计时间相干服装动力学的新方法。为了使用粗略估计为看不见的衣服状态建立逼真的渲染，引入了一个以身体和衣服运动为条件的姿势驱动的可变形神经辐射场，提供了对这两个部分的显式控制。在测试时，可以从看不见的情况中捕捉新的服装姿势，这些姿势来自基于物理或神经网络的模拟器，以驱动看不见服装的动力学。为了评估我们的方法，我们创建了一个多视图数据集，捕捉穿着宽松、动作各异的表演者。实验表明，我们的方法能够呈现出与身体高度偏离的自然服装动力学，并很好地推广到看不见的视图和姿势，超过了现有方法的性能。代码和数据将公开。 et.al.|[2401.15348](http://arxiv.org/abs/2401.15348)|null|
|**2024-01-23**|**GALA: Generating Animatable Layered Assets from a Single Scan**|我们提出了GALA，这是一个框架，它以单层衣服的3D人体网格为输入，并将其分解为完整的多层3D资产。然后可以将输出与其他资产组合，以创建具有任何姿势的新颖的穿着衣服的人类化身。现有的重建方法通常将穿着衣服的人类视为单层几何体，并忽略了人类与发型、衣服和配饰的固有组成性，从而限制了网格在下游应用中的效用。将单层网格分解为单独的层是一项具有挑战性的任务，因为它需要为严重遮挡的区域合成合理的几何结构和纹理。此外，即使分解成功，网格也不能在姿势和体型方面进行归一化，从而无法实现具有新身份和姿势的连贯合成。为了应对这些挑战，我们建议利用预训练的2D扩散模型的一般知识作为人类和其他资产的几何和外观先验。我们首先使用从多视图2D分割中提取的3D表面分割来分离输入网格。然后，我们使用一种新的姿态引导的分数蒸馏采样（SDS）损失来合成姿态空间和规范空间中不同层的缺失几何。一旦我们完成了高保真3D几何体的修复，我们还将相同的SDS损失应用于其纹理，以获得包括初始遮挡区域在内的完整外观。通过一系列分解步骤，我们在一个共享的规范空间中获得了多层3D资产，这些资产根据姿势和人体形状进行了归一化，从而支持轻松合成新的身份，并用新的姿势进行复活。我们的实验证明了与现有解决方案相比，我们的方法在分解、规范化和组合任务方面的有效性。 et.al.|[2401.12979](http://arxiv.org/abs/2401.12979)|null|
|**2024-01-30**|**PSAvatar: A Point-based Morphable Shape Model for Real-Time Head Avatar Animation with 3D Gaussian Splatting**|尽管取得了很大进展，但实现实时高保真度的头部化身动画仍然很困难，并且现有的方法必须在速度和质量之间进行权衡。基于3DMM的方法通常无法对眼镜和发型等非面部结构进行建模，而神经隐式模型则存在变形不灵活和渲染效率低下的问题。尽管3D高斯已经被证明具有很好的几何表示和辐射场重建能力，但在头部化身创建中应用3D高斯仍然是一个主要挑战，因为3D高斯很难对由姿势和表情变化引起的头部形状变化进行建模。在本文中，我们介绍了PSAvatar，这是一种新的可动画化头部化身创建框架，它利用离散几何图元创建参数可变形形状模型，并使用3D高斯进行精细细节表示和高保真渲染。参数变形形状模型是一种基于点的变形形状模型（PMSM），它使用点代替网格进行三维表示，以实现增强的表示灵活性。PMSM首先通过在表面和网格外采样将FLAME网格转换为点，以不仅能够重建表面状结构，而且能够重建复杂的几何形状，如眼镜和发型。通过以综合分析的方式将这些点与头部形状对齐，PMSM可以利用3D高斯进行精细的细节表示和外观建模，从而能够创建高保真化身。我们展示了PSAvatar可以重建各种主题的高保真头部化身，并且化身可以实时动画化（以512 $\times$512的分辨率$\ge$ 25fps）。 et.al.|[2401.12900](http://arxiv.org/abs/2401.12900)|**[link](https://github.com/pcl3dv/PSAvatar)**|
|**2024-01-26**|**New spectral-parameter dependent solutions of the Yang-Baxter equation**|杨-巴克斯特方程（YBE）在研究可积多体量子系统中起着至关重要的作用。许多已知的YBE解决方案提供了从量子自旋链到超导系统的各种例子。可解统计力学模型及其化身也基于YBE。因此，YBE的新解决方案可以用于构建新的有趣的1D量子或2D经典系统，并具有许多其他深远的应用。在这项工作中，我们试图在对应于两个量子位情况的最低维度上找到YBE的（几乎）穷举的解集。我们开发了一种算法，该算法有可能用于生成YBE的新的高维解。 et.al.|[2401.12710](http://arxiv.org/abs/2401.12710)|null|
|**2024-01-20**|**UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures**|3D化身生成的最新进展已经引起了人们的极大关注。这些突破旨在制作更逼真的可动画化化身，缩小虚拟体验和现实世界体验之间的差距。大多数现有的工作都采用了分数蒸馏采样（SDS）损失，结合可微分的渲染器和文本条件，来指导扩散模型生成3D化身。然而，SDS通常生成的结果过于平滑，面部细节很少，因此与祖先采样相比缺乏多样性。另一方面，其他作品从单个图像生成3D化身，其中不想要的光照效果、透视图和较差的图像质量的挑战使得它们难以可靠地重建具有对齐的完整纹理的3D面部网格。在本文中，我们提出了一种新的3D化身生成方法，称为UltrAvatar，该方法具有增强的几何逼真度和卓越的基于物理的渲染（PBR）纹理质量，而没有不需要的照明。为此，该方法提出了一种漫射颜色提取模型和真实性引导的纹理漫射模型。前者去除了不需要的照明效果，以显示真实的漫反射颜色，从而可以在各种照明条件下渲染生成的化身。后者遵循两种基于梯度的指导，用于生成PBR纹理，以更好地与3D网格几何体对齐，呈现不同的人脸身份特征和细节。我们证明了所提出的方法的有效性和稳健性，在实验中大大优于最先进的方法。 et.al.|[2401.11078](http://arxiv.org/abs/2401.11078)|null|
|**2024-01-19**|**Fast Registration of Photorealistic Avatars for VR Facial Animation**|虚拟现实（VR）展示了社交互动的前景，这种互动比其他媒体更具沉浸感。其中的关键是能够在佩戴VR耳机的情况下准确地为自己肖像的真实感化身制作动画。尽管在离线设置中可以将特定于个人的化身高质量地注册到头戴式摄像机（HMC）图像，但通用实时模型的性能显著降低。由于相机视角倾斜和模态差异，在线注册也具有挑战性。在这项工作中，我们首先表明，化身和头戴式耳机相机图像之间的域间隙是困难的主要来源之一，其中基于转换器的架构在域一致性数据上实现了高精度，但当重新引入域间隙时会降低。基于这一发现，我们开发了一种系统设计，将问题解耦为两个部分：1）一个接受域输入的迭代细化模块，以及2）一个基于表情和头部姿势的当前估计的通用化身引导的图像到图像风格传递模块。这两个模块相互加强，因为当显示接近真实的例子时，图像风格的转移变得更容易，而更好的域间隙去除有助于配准。我们的系统可以高效地产生高质量的结果，从而无需昂贵的离线注册来生成个性化标签。我们通过在商品耳机上进行的大量实验验证了我们的方法的准确性和效率，证明了与直接回归方法和离线注册相比的显著改进。 et.al.|[2401.11002](http://arxiv.org/abs/2401.11002)|null|
|**2024-01-18**|**GPAvatar: Generalizable and Precise Head Avatar from Image(s)**|头部化身重建对于虚拟现实、在线会议、游戏和电影行业的应用至关重要，在计算机视觉界引起了极大的关注。该领域的基本目标是忠实地再现头部化身，并精确地控制表情和姿势。现有的方法分为基于2D的扭曲、基于网格和神经渲染方法，在保持多视图一致性、结合非面部信息和推广到新身份方面存在挑战。在本文中，我们提出了一个名为GPAvatar的框架，该框架可以在单个前向通道中从一个或多个图像重建3D头部化身。这项工作的关键思想是引入一个由点云驱动的动态基于点的表情场，以精确有效地捕捉表情。此外，我们在三平面规范场中使用多三平面注意力（MTA）融合模块来利用来自多个输入图像的信息。所提出的方法实现了忠实的身份重建、精确的表达控制和多视图一致性，在自由视点渲染和新颖视图合成方面显示了良好的效果。 et.al.|[2401.10215](http://arxiv.org/abs/2401.10215)|**[link](https://github.com/xg-chu/gpavatar)**|
|**2024-01-17**|**Tri $^{2}$-plane: Volumetric Avatar Reconstruction with Feature Pyramid**|近年来，在利用神经体积绘制进行面部化身重建方面取得了相当大的成就。尽管取得了显著的进步，但从单眼视频中重建复杂而动态的头部运动仍然需要捕捉和恢复细粒度的细节。在这项工作中，我们提出了一种新的方法，命名为Tri$^2$-plane，用于单目照片逼真的体积头部化身重建。与现有的依赖于单个三平面变形场进行动态面部建模的工作不同，所提出的tri$^2$-平面利用了特征金字塔和三个上下横向连接三平面的原理来改进细节。它在多个尺度上采样和渲染面部细节，从整个面部过渡到特定的局部区域，然后过渡到更精细的子区域。此外，我们在训练中加入了一种基于相机的几何感知滑动窗口方法作为增强，它提高了规范空间之外的鲁棒性，特别提高了交叉身份生成能力。实验结果表明，Tri$^2$ -平面不仅超越了现有的方法，而且通过实验在定量指标和定性评估方面都取得了卓越的性能。 et.al.|[2401.09386](http://arxiv.org/abs/2401.09386)|**[link](https://github.com/songluchuan/tri2plane)**|
|**2024-01-20**|**Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**|一次拍摄3D会说话的肖像生成旨在从看不见的图像中重建3D化身，然后用参考视频或音频将其动画化，以生成会说话的人像视频。现有的方法无法同时实现准确的三维化身重建和稳定的人脸动画。此外，虽然现有的作品主要集中在合成头部，但生成自然的躯干和背景片段以获得逼真的说话肖像视频也是至关重要的。为了解决这些限制，我们提出了Real3D Potrait，该框架（1）通过从3D人脸生成模型中提取3D先验知识的大图像到平面模型提高了单次3D重建能力；（2） 利用高效的运动适配器促进精确的运动条件动画；（3） 使用头部-躯干背景超分辨率模型来合成具有自然躯干运动和可切换背景的逼真视频；以及（4）支持具有可推广的音频到运动模型的单镜头音频驱动的谈话面部生成。大量实验表明，与以前的方法相比，Real3D Portrait很好地概括了看不见的身份，并生成了更逼真的谈话肖像视频。视频样本和源代码可在https://real3dportrait.github.io . et.al.|[2401.08503](http://arxiv.org/abs/2401.08503)|null|
|**2024-01-13**|**EVOKE: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation**|随着虚拟环境的不断发展，对沉浸式和情感化体验的需求也在增长。为了满足这一需求，我们引入了使用优化知识提取（EVOKE）实现情感的虚拟化身映射，这是一种轻量级的情感识别框架，旨在将情感识别无缝集成到虚拟环境中的3D化身中。我们的方法利用了知识提取，包括在公开的DEAP数据集上进行多标签分类，该数据集涵盖了效价、唤醒和支配作为主要情绪类别。值得注意的是，我们的蒸馏模型，一个只有两个卷积层、参数比教师模型少18倍的CNN，取得了有竞争力的结果，其准确率为87%，同时所需的计算资源要少得多。这种性能和可部署性之间的平衡使我们的框架成为虚拟环境系统的理想选择。此外，多标签分类结果被用于将情绪映射到定制设计的3D化身上。 et.al.|[2401.06957](http://arxiv.org/abs/2401.06957)|null|

<p align=right>(<a href=#updated-on-20240131>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

