[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.01.10
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#3d>3D</a></li>
    <li><a href=#mllm>MLLM</a></li>
    <li><a href=#diffusion>Diffusion</a></li>
    <li><a href=#avatar>avatar</a></li>
  </ol>
</details>

## 3D

|Publish Date|Title|Abstract|PDF|Code|
|---|---|-----------------------------------|---|---|
|**2024-01-07**|**See360: Novel Panoramic View Interpolation**|我们介绍了See360，它是一种使用潜在空间视点估计进行360全景插值的通用且高效的框架。大多数现有的视图渲染方法只关注室内或合成三维环境，并渲染小对象的新视图。相反，我们建议将以相机为中心的视图合成作为2D仿射变换来处理，而不使用点云或深度图，这使得能够实现有效的360？全景场景探索。给定一对参考图像，See360模型通过提出的新颖的多尺度仿射变换器（MSAT）来学习渲染新颖的视图，从而实现从粗到细的特征渲染。我们还提出了一种条件潜在空间自动编码器（C-LAE）来实现任意角度的视图插值。为了展示我们方法的多功能性，我们引入了四个训练数据集，即UrbanCity360、Archinterior360、HungHom360和Lab360，它们是从室内和室外环境中收集的，用于真实和合成渲染。实验结果表明，该方法具有足够的通用性，可以实现四个数据集任意视图的实时绘制。此外，我们的See360模型可以应用于野外的视图合成：只需很短的额外训练时间（约10分钟），并且能够渲染未知的真实世界场景。See360的卓越性能为以相机为中心的视图渲染和360全景视图插值开辟了一个很有前途的方向。 et.al.|[2401.03431](http://arxiv.org/abs/2401.03431)|**[link](https://github.com/Holmes-Alan/See360)**|
|**2024-01-06**|**RustNeRF: Robust Neural Radiance Field with Low-Quality Images**|最近在神经辐射场（NeRF）方面的工作利用了多视图三维一致性，在三维场景建模和高保真新颖视图合成方面取得了令人印象深刻的结果。然而，也有局限性。首先，现有方法假设有足够的高质量图像可用于训练NeRF模型，忽略了真实世界的图像退化。其次，由于不同视图之间未建模的不一致性，以前的方法在训练集中难以解决模糊性问题。在这项工作中，我们为真实世界的高质量NeRF提供了RustNeRF。为了提高NeRF在真实世界输入下的鲁棒性，我们训练了一个包含真实世界退化建模的3D感知预处理网络。我们提出了一种新的隐式多视图引导来解决图像退化和恢复过程中的信息丢失问题。大量实验证明了RustNeRF在实际退化情况下优于现有方法。代码将被发布。 et.al.|[2401.03257](http://arxiv.org/abs/2401.03257)|null|
|**2024-01-02**|**Street Gaussians for Modeling Dynamic Urban Scenes**|本文旨在解决从单目视频中建模动态城市街道场景的问题。最近的方法扩展了NeRF，将跟踪车辆姿态纳入车辆动画，实现了动态城市街道场景的照片逼真视图合成。然而，它们的显著局限性在于训练和渲染速度慢，再加上履带车辆姿态对高精度的迫切需求。我们介绍了Street Gaussians，一种新的明确的场景表示，它解决了所有这些限制。具体地说，动态城市街道被表示为一组点云，这些点云配备有语义logits和3D Gaussians，每一个都与前景车辆或背景相关联。为了对前景对象车辆的动力学进行建模，使用可优化的跟踪姿态以及动态外观的动态球面谐波模型对每个对象点云进行优化。显式表示允许容易地合成目标车辆和背景，这反过来又允许在半小时的训练内以133 FPS（1066 $\times$ 1600分辨率）进行场景编辑操作和渲染。所提出的方法在多个具有挑战性的基准上进行了评估，包括KITTI和Waymo Open数据集。实验表明，在所有数据集上，所提出的方法始终优于最先进的方法。此外，尽管仅依赖于现成跟踪器的姿态，但所提出的表示提供的性能与使用精确的地面实况姿态所实现的性能不相上下。代码位于https://zju3dv.github.io/street_gaussians/. et.al.|[2401.01339](http://arxiv.org/abs/2401.01339)|null|
|**2024-01-01**|**Deblurring 3D Gaussian Splatting**|最近对辐射场的研究为具有照片级真实感渲染质量的新颖视图合成铺平了坚实的道路。然而，它们通常使用神经网络和体积绘制，这两种方法的训练成本很高，并且由于绘制时间长，阻碍了它们在各种实时应用中的广泛使用。最近，人们提出了一种基于3D高斯散射的方法来对3D场景进行建模，并在实时渲染图像的同时实现了显著的视觉质量。然而，如果训练图像模糊，则渲染质量会严重下降。模糊通常是由于镜头散焦、物体运动和相机抖动而产生的，它不可避免地会干扰干净图像的获取。先前的几项研究试图使用神经场从模糊的输入图像中渲染干净清晰的图像。然而，这些工作中的大多数仅设计用于基于体积渲染的神经辐射场，并不直接适用于基于光栅化的3D高斯散射方法。因此，我们提出了一种新的实时去模糊框架，即去模糊3D高斯散点，使用小型多层感知器（MLP）来操纵每个3D高斯的协方差来对场景模糊度进行建模。虽然去模糊的3D高斯飞溅仍然可以享受实时渲染，但它可以从模糊的图像中重建精细和清晰的细节。在基准上进行了各种实验，结果表明了我们的去模糊方法的有效性。定性结果可在https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/ et.al.|[2401.00834](http://arxiv.org/abs/2401.00834)|null|
|**2024-01-01**|**Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using Sharpness Prior**|神经辐射场（NeRF）在基于神经渲染的新视图合成中表现出了显著的性能。然而，当输入图像是在不完美的条件下拍摄的时，NeRF会遭受严重的视觉质量下降，例如照明不良、散焦模糊和透镜像差。特别是，当通常使用相机拍摄图像时，散焦模糊在图像中非常常见。尽管最近很少有研究提出渲染相当高质量的清晰图像，但它们仍然面临许多关键挑战。特别地，这些方法采用了基于多层感知器（MLP）的NeRF，这需要大量的计算时间。为了克服这些缺点，本文提出了一种新的技术Sharp-NeRF——一种基于网格的NeRF，它可以在半小时的训练内从输入的模糊图像中渲染干净清晰的图像。为此，我们使用了几个基于网格的内核来准确地对场景的清晰度/模糊度进行建模。计算像素的清晰度水平以学习空间变化的模糊核。我们在由模糊图像组成的基准上进行了实验，并评估了完全参考和非参考指标。定性和定量的结果表明，我们的方法以生动的色彩和精细的细节呈现出尖锐的新颖观点，并且它比以前的作品具有更快的训练时间。我们的项目页面位于https://benhenryl.github.io/SharpNeRF/ et.al.|[2401.00825](http://arxiv.org/abs/2401.00825)|**[link](https://github.com/benhenryl/sharpnerf)**|
|**2024-01-02**|**GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields**|在本文中，我们专注于单镜头新颖视图合成（O-NVS）任务，该任务的目标是在每个场景只有一个参考图像的情况下合成照片逼真的新颖视图。先前的一次性可泛化神经辐射场（OG-NeRF）方法以无推理时间微调的方式解决了这一任务，但由于仅编码器的架构高度依赖于有限的参考图像，因此存在模糊问题。另一方面，最近的基于扩散的图像到3d方法通过将预先训练的2D扩散模型提取到3d表示中显示出生动可信的结果，但需要繁琐的逐场景优化。针对这些问题，我们提出了GD $^2$-NeRF，这是一个通过GAN和Diffusion的生成细节补偿框架，既不需要推理时间微调，又具有生动可信的细节。详细地说，遵循从粗到细的策略，GD$^2$-NeRF主要由一级并行流水线（OPP）和3D一致细节增强器（Diff3DE）组成。在粗略阶段，OPP首先将GAN模型有效地插入到现有的OG-NeRF管道中，以主要缓解从训练数据集中捕获的分布内先验的模糊问题，实现清晰度（LPIPS、FID）和保真度（PSNR、SSIM）之间的良好平衡。然后，在精细阶段，Diff3DE进一步利用预先训练的图像扩散模型来补充丰富的分布细节，同时保持良好的3D一致性。在合成数据集和真实世界数据集上进行的大量实验表明，GD$^2$ -NeRF在没有每场景微调的情况下显著改善了细节。 et.al.|[2401.00616](http://arxiv.org/abs/2401.00616)|null|
|**2023-12-28**|**iFusion: Inverting Diffusion for Pose-Free Reconstruction from Sparse Views**|我们提出了iFusion，这是一种新颖的3D对象重建框架，只需要两个具有未知相机姿态的视图。虽然单视图重建会产生视觉上吸引人的结果，但它可能会与实际对象有很大的偏差，尤其是在看不见的一侧。附加视图提高了重建保真度，但需要已知的摄影机姿势。然而，假设姿态的可用性可能是不现实的，并且现有的姿态估计器在稀疏视图场景中失败。为了解决这一问题，我们利用了一个预先训练的新颖视图合成扩散模型，该模型嵌入了关于不同对象的几何形状和外观的隐含知识。我们的策略分为三个步骤：（1）我们反转用于相机姿态估计的扩散模型，而不是合成新的视图。（2） 使用提供的视图和估计的姿态对扩散模型进行微调，使其成为为目标对象量身定制的新型视图合成器。（3） 利用配准的视图和微调的扩散模型，我们重建了3D对象。实验表明，在姿态估计和新视图合成方面都有很强的性能。此外，iFusion与各种重建方法无缝集成，并对其进行了增强。 et.al.|[2312.17250](http://arxiv.org/abs/2312.17250)|**[link](https://github.com/chinhsuanwu/ifusion)**|
|**2023-12-28**|**Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis**|动态场景的新颖视图合成一直是一个有趣但具有挑战性的问题。尽管最近取得了进展，但同时实现高分辨率的真实感、实时渲染和紧凑的存储仍然是一项艰巨的任务。为了应对这些挑战，我们提出了时空高斯特征飞溅作为一种新的动态场景表示，由三个关键组件组成。首先，我们通过增强具有时间不透明度和参数运动/旋转的3D高斯，来形成富有表现力的时空高斯。这使得时空高斯能够捕捉场景中的静态、动态以及瞬态内容。其次，我们介绍了飞溅特征渲染，它用神经特征代替了球面谐波。这些功能有助于在保持小尺寸的同时对视图和与时间相关的外观进行建模。第三，我们利用训练误差和粗略深度的指导，在难以与现有管道融合的区域对新的高斯采样。在几个已建立的真实世界数据集上的实验表明，我们的方法在保持紧凑存储的同时，实现了最先进的渲染质量和速度。在8K分辨率下，我们的lite版本模型可以在Nvidia RTX 4090 GPU上以60 FPS的速度渲染。 et.al.|[2312.16812](http://arxiv.org/abs/2312.16812)|null|
|**2023-12-29**|**DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision**|我们见证了基于深度学习的3D视觉的重大进展，从基于神经辐射场（NeRF）的3D表示学习到在新视图合成（NVS）中的应用。然而，用于基于深度学习的3D视觉的现有场景级数据集，仅限于合成环境或现实世界场景的狭窄选择，是非常不足的。这种不足不仅阻碍了现有方法的全面基准，而且限制了在基于深度学习的3D分析中可以探索的内容。为了解决这一关键差距，我们展示了DL3DV-10K，这是一个大型场景数据集，其特征是从65种类型的兴趣点（POI）位置捕获的10510个视频中的5120万帧，涵盖了有界和无界场景，具有不同的反射、透明度和光照水平。我们在DL3DV-10K上对最近的NVS方法进行了全面的基准测试，为未来的NVS研究提供了宝贵的见解。此外，我们在一项从DL3DV-10K学习可推广NeRF的试点研究中获得了令人鼓舞的结果，这表明了大规模场景级数据集的必要性，以打造学习3D表示的基础模型。我们的DL3DV-10K数据集、基准测试结果和模型将在https://dl3dv-10k.github.io/DL3DV-10K/. et.al.|[2312.16256](http://arxiv.org/abs/2312.16256)|null|
|**2023-12-26**|**fMPI: Fast Novel View Synthesis in the Wild with Layered Scene Representations**|在这项研究中，我们为基于分层场景表示的新视图合成（NVS）方法提出了两种新的输入处理范式，这两种方法在不影响质量的情况下显著提高了它们的运行时间。我们的方法识别并减轻了传统管道最耗时的两个方面：构建和处理所谓的平面扫描体积（PSV），这是输入相机视图的平面重新投影的高维张量。特别是，我们建议在并行组中处理该张量，以提高计算效率，并对相邻输入平面进行超采样，从而生成更密集、更准确的场景表示。所提出的增强提供了显著的灵活性，允许在性能和速度之间取得平衡，从而朝着实时应用迈出了实质性的步伐。此外，它们非常通用，因为任何基于PSV的方法都可以使用它们，包括使用多平面图像、多球体图像和分层深度图像的方法。在一组全面的实验中，我们证明了我们提出的范式能够设计出一种NVS方法，该方法在公共基准上达到最先进的水平，同时比现有的最先进的方法快50倍。它在速度方面也比目前的前辈高出3倍多，同时实现了明显更好的渲染质量。 et.al.|[2312.16109](http://arxiv.org/abs/2312.16109)|null|

<p align=right>(<a href=#updated-on-20240110>back to top</a>)</p>

## MLLM

|Publish Date|Title|Abstract|PDF|Code|
|---|---|-----------------------------------|---|---|
|**2024-01-08**|**Evaluating Brain-Inspired Modular Training in Automated Circuit Discovery for Mechanistic Interpretability**|大型语言模型（LLM）在人工智能领域迅速崛起，凭借其先进的功能改变了广泛的应用程序。随着这些模型越来越成为决策的组成部分，对彻底可解释性的需求从未像现在这样重要。机械解释性通过识别和分析这些复杂系统中的特定子网络或“电路”，提供了一条理解的途径。这种方法的一个关键方面是自动电路发现，它有助于以可行的方式研究GPT4或LLAMA等大型模型。在这种情况下，我们的研究评估了最近的一种方法，即大脑启发模块训练（BIMT），旨在增强神经网络的可解释性。我们展示了BIMT如何显著提高自动电路发现的效率和质量，克服手动方法的局限性。我们的比较分析进一步表明，BIMT在电路质量、发现时间和稀疏性方面优于现有模型。此外，我们还对BIMT进行了全面的计算分析，包括训练持续时间、内存分配要求和推理速度等方面。这项研究推进了创建可信和透明的人工智能系统的更大目标，此外还展示了BIMT在使神经网络更容易理解方面的工作效果。 et.al.|[2401.03646](http://arxiv.org/abs/2401.03646)|null|
|**2024-01-06**|**CaMML: Context-Aware Multimodal Learner for Large Models**|在这项工作中，我们介绍了上下文感知多模式学习器（CaMML），用于调整大型多模式模型（LMM）。CaMML是一个轻量级模块，旨在将多模式上下文样本无缝集成到大型模型中，从而使模型能够从类似的、特定领域的最新信息中获得知识，并做出有根据的推断。重要的是，CaMML具有高度的可扩展性，并且由于其分层设计，可以有效地处理冗长的多模式上下文示例。基于CaMML，我们开发了两个多模式模型，即CaMML-7B和CaMML-13B，它们在多模式任务的基准数据集阵列中表现出了卓越的性能。值得注意的是，在不集成任何外部资源的情况下，CaMML-13B在十多个广泛认可的多模式基准数据集上实现了最先进的性能，以显著的优势超过了LLaVA-1.5（13B）。此外，我们进行了广泛的消融研究，以检查CaMML的内部工作原理，并进行了定性分析，以展示其在处理现实世界中具有挑战性的案例方面的有效性。 et.al.|[2401.03149](http://arxiv.org/abs/2401.03149)|null|
|**2024-01-09**|**Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM**|在对话式人工智能研究中，有一个明显的趋势，即开发具有更多参数的模型，例如ChatGPT等模型。虽然这些扩展的模型往往会产生越来越好的聊天响应，但它们需要大量的计算资源和内存。这项研究探讨了一个相关的问题：相对于单一的大型模型，小型模型的组合能否协同实现可比或增强的性能？我们介绍了一种称为“混合”的方法，这是一种简单而有效的集成多个聊天AI的方法。我们的经验证据表明，当特定的较小模型协同混合时，它们可能会优于或匹配更大模型的能力。例如，仅集成三个中等大小的模型（6B/13B参数）就可以与ChatGPT（175B+参数）等大得多的模型的性能指标相媲美，甚至超过它们。在Chai研究平台上使用A/B测试方法对这一假设进行了为期30天的严格测试，该方法拥有大量用户。这些发现强调了“混合”策略作为一种可行的方法的潜力，可以在不增加相应计算需求的情况下提高聊天人工智能的效率。 et.al.|[2401.02994](http://arxiv.org/abs/2401.02994)|null|
|**2024-01-05**|**LMaaS: Exploring Pricing Strategy of Large Model as a Service for Communication**|下一代通信被设想为智能通信，它可以取代传统的符号通信，在符号通信中，考虑到源和信道的高度浓缩的语义信息将被高效提取和传输。最近流行的GPT4等大型模型和助推学习技术为智能通信奠定了坚实的基础，并促使其在不久的将来得到实际部署。鉴于这些多模式大型语言模型“一次性培训并广泛使用”的特点，我们认为现收现付服务模式将适合这种情况，称为大型服务模型（LMaaS）。然而，交易和定价问题非常复杂，具有异构和动态的客户环境，这使得定价优化问题在寻求现有解决方案方面具有挑战性。在本文中，我们旨在填补这一空白，并将LMaaS市场交易公式化为两步的Stackelberg对策。在第一步中，我们优化了卖家的定价决策，并提出了一种迭代模型定价（IMP）算法，该算法通过推理客户未来的租赁决策来迭代优化大型模型的价格，从而能够实现接近最优的定价解决方案。在第二步中，我们通过设计一个稳健的选择和租赁（RSR）算法来优化客户的选择决策，该算法在严格的理论证明下保证是最优的。大量实验证实了我们算法的有效性和稳健性。 et.al.|[2401.02675](http://arxiv.org/abs/2401.02675)|null|
|**2024-01-05**|**CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal Models with Multiple Image Inputs**|在探索通用人工智能（AGI）的发展时，这些模型的关键任务包括解释和处理来自多个图像输入的信息。然而，大型多模式模型（LMM）在这种情况下遇到了两个问题：（1）缺乏细粒度感知，以及（2）倾向于在多个图像中混合信息。我们首先广泛研究了LMM在处理多个输入图像时感知细粒度视觉细节的能力。研究主要集中在两个方面：第一，图像到图像匹配（评估LMM是否能有效地推理和配对相关图像），第二，多图像到文本匹配（评估LM是否能准确地捕捉和总结详细的图像信息）。我们对一系列开源和闭源大型模型进行了评估，包括GPT-4V、Gemini、OpenFlamingo和MMICL。为了提高模型性能，我们进一步开发了一种基于多输入多模式模型的对比思维链（CoCoT）提示方法。该方法要求LMM比较多个图像输入之间的相似性和差异性，然后根据识别出的相似性与差异性引导模型回答有关多个图像输出的详细问题。我们的实验结果展示了CoCoT在增强大型多模式模型的多图像理解能力方面的熟练程度。 et.al.|[2401.02582](http://arxiv.org/abs/2401.02582)|null|
|**2024-01-03**|**FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers**|近年来，视觉变换器（ViT）模型逐渐成为各种计算机视觉任务的主流，其鲁棒性越来越受到关注。然而，现有的大型模型倾向于在训练过程中优先考虑性能，可能会忽视鲁棒性，这可能会导致严重的安全问题。在本文中，我们提出了一个新的挑战：探索如何使用少量附加参数进行对抗性微调，以快速有效地增强标准训练模型的对抗性鲁棒性。为了应对这一挑战，我们开发了新的LNLoRA模块，在传统的LoRA模块之前加入了可学习的层归一化，这有助于缓解对抗性和标准训练范式之间参数的幅度差异。此外，我们提出了FullLoRA AT框架，通过将可学习的LNLoRA模块集成到基于ViT的模型的所有关键组件中，同时保持预训练的模型冻结，这可以通过以参数有效的方式进行对抗性微调来显著提高模型的鲁棒性。在CIFAR-10、CIFAR-100和Imagenette上进行的大量实验证明了我们提出的FullLoRA AT框架的优越性。它通过完全微调实现了相当的鲁棒性，同时只需要大约5%的可学习参数。这也有效地解决了对抗性微调引起的额外模型存储空间和巨大训练时间的问题。 et.al.|[2401.01752](http://arxiv.org/abs/2401.01752)|null|
|**2024-01-03**|**Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data**|自监督学习（SSL）已成为大型模型事实上的训练范式，在预训练之后，使用特定领域的数据和标签进行监督微调。假设SSL模型将学习更通用的、因此更少偏见的表示，本研究探讨了预训练和微调策略对公平性的影响（即，在不同的人口统计细分中表现相同）。受现实世界时间序列数据上以人为中心的应用程序的激励，我们通过系统地将SSL模型与其监督的对应模型进行比较，来解释模型、层和度量级别上的归纳偏差。我们的研究结果表明，SSL有能力实现与监督方法同等的性能，同时显著提高公平性——通过自我监督，公平性提高了27%，而性能仅下降了1%。最终，这项工作强调了SSL在以人为中心的计算领域的潜力，特别是在医疗保健等高风险、数据稀缺的应用领域。 et.al.|[2401.01640](http://arxiv.org/abs/2401.01640)|null|
|**2024-01-03**|**Glance and Focus: Memory Prompting for Multi-Event Video Question Answering**|视频问答（VideoQA）已成为评估代理人理解人类日常行为能力的重要工具。尽管最近大型视觉语言模型在许多多模式任务中取得了成功，但对涉及多个人机交互事件的视频进行复杂情境推理仍然具有挑战性。相比之下，人类可以通过使用一系列情节记忆作为锚来快速定位与问题相关的关键时刻进行推理，从而轻松应对。为了模仿这种有效的推理策略，我们提出了Glance Focus模型。一种简单的方法是应用动作检测模型来预测作为关键记忆的一组动作。然而，在一个封闭的词汇表中，这些动作很难推广到各种视频领域。相反，我们训练编码器-解码器在扫视阶段生成一组动态事件记忆。除了使用有监督的二分匹配来获得事件记忆外，我们还设计了一种无监督的记忆生成方法来消除对事件注释的依赖。接下来，在聚焦阶段，这些事件记忆充当了一座桥梁，建立了具有高级事件概念的问题与低级冗长视频内容之间的相关性。考虑到这个问题，该模型首先关注生成的关键事件记忆，然后通过我们设计的多级交叉注意力机制关注最相关的推理时刻。我们在四个多事件VideoQA基准上进行了广泛的实验，包括STAR、EgoTaskQA、AGQA和NExT QA。我们提出的模型取得了最先进的结果，在各种具有挑战性的推理任务中超过了当前的大型模型。代码和型号可在https://github.com/ByZ0e/Glance-Focus. et.al.|[2401.01529](http://arxiv.org/abs/2401.01529)|**[link](https://github.com/byz0e/glance-focus)**|
|**2024-01-02**|**Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models**|多模式大语言模型（MLLM）的兴起激发了人们对基于语言的驾驶任务的兴趣。然而，现有的研究通常集中在有限的任务上，往往忽略了关键的多视图和时间信息，这对稳健的自动驾驶至关重要。为了弥补这些差距，我们引入了NuInstruction，这是一个新的数据集，在17个子任务中有91K个多视图视频QA对，其中每个任务都需要整体信息（例如，时间、多视图和空间），大大提高了挑战水平。为了获得NuInstruction，我们提出了一种新的基于SQL的方法来自动生成指令-响应对，这是受人类驱动逻辑进程的启发。我们进一步介绍了BEV InMLLM，这是一种端到端的方法，用于有效地推导感知指令的鸟瞰图（BEV）特征，该特征与大型语言模型的语言对齐。BEV InMLLM集成了多视图、空间感知和时间语义，以增强MLLM在NuInstruction任务上的能力。此外，我们提出的BEV注入模块是现有MLLMs的即插即用方法。我们在NuInstruction上的实验表明，BEV InMLLM显著优于现有的MLLM，例如在各种任务上提高了约9%。我们计划发布我们的NuInstruction，用于未来的研究开发。 et.al.|[2401.00988](http://arxiv.org/abs/2401.00988)|**[link](https://github.com/xmed-lab/nuinstruct)**|
|**2023-12-31**|**Extracting spectra in the shell model Monte Carlo method using imaginary-time correlation matrices**|在非常大的模型空间中，禁止在构型相互作用（CI）壳层模型方法的框架下计算核能级的传统对角化方法。壳层模型蒙特卡罗（SMMC）是一种在非常大的模型空间中计算核的热和基态可观察性的强大技术，但在这种方法中提取核光谱是一项具有挑战性的技术。我们提出了一种新的方法来提取一组好量子数（如自旋和宇称）的给定值的低能级。该方法基于渐近满足广义特征值问题的虚时一体密度相关矩阵。我们在轻核中验证了该方法，该方法允许与CI壳层模型哈密顿量的精确对角化结果进行比较。该方法适用于可以在CI壳模型方法中描述的其他有限尺寸量子多体系统。 et.al.|[2401.00613](http://arxiv.org/abs/2401.00613)|null|

<p align=right>(<a href=#updated-on-20240110>back to top</a>)</p>

## Diffusion

|Publish Date|Title|Abstract|PDF|Code|
|---|---|-----------------------------------|---|---|
|**2024-01-07**|**A Classification of Critical Configurations for any Number of Projective Views**|运动结构是从一组图像中恢复有关相机和3D场景的信息的过程。通常，在无噪声设置中，如果提供了足够的图像和图像点，则可以唯一地恢复所有信息。然而，在某些情况下，即使在理论上，也不可能进行独特的恢复；这些被称为关键配置。我们使用最近开发的代数方法对任意数量的投影相机的所有关键配置进行分类。我们证明了它们形成了众所周知的代数变体，如二次曲面和次数最多为4的曲线。本文还改进了早期的结果，发现了以前未知的关键构型，并表明一些以前认为是关键的构型实际上不是。 et.al.|[2401.03450](http://arxiv.org/abs/2401.03450)|null|
|**2024-01-05**|**Latte: Latent Diffusion Transformer for Video Generation**|我们提出了一种新的潜在扩散转换器，即Latte，用于视频生成。Latte首先从输入视频中提取时空标记，然后采用一系列Transformer块对潜在空间中的视频分布进行建模。为了对从视频中提取的大量令牌进行建模，从分解输入视频的空间和时间维度的角度引入了四种有效的变体。为了提高生成视频的质量，我们通过严格的实验分析确定了Latte的最佳实践，包括视频片段补丁嵌入、模型变体、时间步长类信息注入、时间位置嵌入和学习策略。我们的综合评估表明，Latte在四个标准视频生成数据集（即FaceForensics、SkyTimelapse、UCF101和Taichi HD）中实现了最先进的性能。此外，我们将Latte扩展到文本到视频生成（T2V）任务，其中与最近的T2V模型相比，Latte实现了可比的结果。我们坚信，Latte为未来将变压器纳入视频生成的扩散模型的研究提供了宝贵的见解。 et.al.|[2401.03048](http://arxiv.org/abs/2401.03048)|**[link](https://github.com/maxin-cn/Latte)**|
|**2024-01-05**|**Dataset of turbulent flow over interacting barchan dunes**|Barchans是常见于地球、火星和其他天体沙丘区的沙丘，在那里它们可以相互作用。本文涉及孤立或相互作用的水下条形码流动的实验数据。实验是在矩形截面的透明通道中进行的，在该通道中，湍流被施加在一个或一对杆上。使用低频PIV（粒子图像测速仪）和高频PTV（粒子跟踪测速仪）测量瞬时流场。根据PIV和PTV数据，计算了平均流量、轨迹和二阶矩，这些数据包括在本文描述的数据集中，以及原始数据（图像）、瞬时场和处理它们的脚本。数据集可以重复用于基准测试或处理其他研究小组生成的新图像。 et.al.|[2401.03032](http://arxiv.org/abs/2401.03032)|null|
|**2024-01-04**|**VASE: Object-Centric Appearance and Shape Manipulation of Real Videos**|最近，一些作品解决了大规模文本到图像生成模型的成功所促进的视频编辑任务。然而，这些方法中的大多数都使用文本对帧进行整体编辑，利用基础扩散模型给出的先验知识，并专注于提高帧间的时间一致性。在这项工作中，我们介绍了一个以对象为中心的框架，该框架旨在控制对象的外观，尤其是对对象执行精确而明确的结构修改。我们在预先训练的图像条件扩散模型上构建我们的框架，集成层来处理时间维度，并提出训练策略和架构修改来实现形状控制。我们在图像驱动的视频编辑任务中评估了我们的方法，该方法显示出与最先进技术相似的性能，并展示了新颖的形状编辑功能。更多详细信息、代码和示例可在我们的项目页面上获得：https://helia95.github.io/vase-website/ et.al.|[2401.02473](http://arxiv.org/abs/2401.02473)|null|
|**2024-01-04**|**Linguistic Profiling of Deepfakes: An Open Database for Next-Generation Deepfake Detection**|文本到图像生成模型的出现彻底改变了deepfakes领域，使其能够直接从文本描述中创建逼真且令人信服的视觉内容。然而，这一进步在检测此类内容的真实性方面带来了相当大的挑战。现有的deepfake检测数据集和方法往往无法有效捕获广泛的新兴deepfakes，并为检测提供令人满意的解释信息。为了解决这个重要问题，本文介绍了一个深度伪造数据库（DFLIP-3K），用于开发令人信服和可解释的深度伪造检测。它包括来自大约3K个生成模型的大约300K个不同的深度伪造样本，这是文献中拥有最多深度伪造模型的模型。此外，它还收集了大约190K个这些deepfakes的语言足迹。这两个显著的特征使DFLIP-3K能够开发一个基准，促进deepfakes语言分析的进展，该基准包括三个子任务，即deepfake检测、模型识别和即时预测。深度伪造模型和提示是每个深度伪造的两个重要组成部分，因此从语言上对它们进行剖析，可以在深度伪造检测中对值得信赖和可解释的证据进行宝贵的探索，我们认为这是下一代深度伪造检测的关键。此外，DFLIP-3K被设想为一个开放的数据库，促进透明度并鼓励合作努力进一步促进其增长。我们在开发的基准上进行的大量实验验证了我们的DFLIP-3K数据库能够作为评估和比较基于语言的深度伪造检测、识别和即时预测技术的标准化资源。 et.al.|[2401.02335](http://arxiv.org/abs/2401.02335)|**[link](https://github.com/dflip3k/dflip-3k)**|
|**2024-01-04**|**Bayesian Intrinsic Groupwise Image Registration: Unsupervised Disentanglement of Anatomy and Geometry**|本文提出了一个通用的贝叶斯学习框架，用于医学图像的多模式成组配准。该方法建立在图像生成过程的概率建模基础上，其中观察到的图像的潜在共同解剖结构和几何变化被明确地分解为潜在变量。因此，通过贝叶斯推理的解决方案实现了分组注册。我们提出了一种新的分层变分自动编码架构来实现潜在变量的推理过程，其中配准参数可以以数学上可解释的方式计算。值得注意的是，这种新范式可以在无监督的闭环自重构过程中学习逐组配准，省去了设计复杂的基于强度的相似性度量的负担。计算高效的解纠缠架构也具有固有的可扩展性和灵活性，允许在具有可变大小的大规模图像组上进行逐组配准。此外，从解纠缠学习中推断出的结构表示能够用视觉语义捕捉观察结果的潜在解剖结构。进行了广泛的实验来验证所提出的框架，包括心脏、大脑和腹部医学图像的四个数据集。结果表明，我们的方法在准确性、效率、可扩展性和可解释性方面优于传统的基于相似性的方法。 et.al.|[2401.02141](http://arxiv.org/abs/2401.02141)|null|
|**2024-01-04**|**Improving Diffusion-Based Image Synthesis with Context Prediction**|扩散模型是一类新的生成模型，以前所未有的质量和多样性极大地促进了图像的生成。现有的扩散模型主要试图从沿着空间轴具有逐像素或逐特征约束的损坏图像重建输入图像。然而，这种基于点的重建可能无法使每个预测的像素/特征完全保留其邻域上下文，从而损害基于扩散的图像合成。作为一种强大的自动监控信号源，上下文在学习表征方面已经得到了很好的研究。受此启发，我们首次提出了ConPreDiff来改进基于上下文预测的扩散图像合成。在训练阶段，我们在扩散去噪块结束时用上下文解码器明确地增强每个点，以预测其邻域上下文（即，多步特征/标记/像素），并移除解码器进行推理。通过这种方式，每个点都可以通过保留其与邻域上下文的语义连接来更好地重构自己。这种新的ConPreDiff范式可以推广到任意离散和连续的扩散主链，而不需要在采样过程中引入额外的参数。对无条件图像生成、文本到图像生成和图像修复任务进行了广泛的实验。我们的ConPreDiff始终优于以前的方法，并在MS-COCO上实现了新的SOTA文本到图像生成结果，零样本FID得分为6.21。 et.al.|[2401.02015](http://arxiv.org/abs/2401.02015)|null|
|**2024-01-03**|**Instruct-Imagen: Image Generation with Multi-modal Instruction**|本文提出了指令图像，这是一个处理异构图像生成任务并在看不见的任务中推广的模型。我们为图像生成引入了“多模式指令”，这是一种精确表达一系列生成意图的任务表示。它使用自然语言来合并不同的模式（例如，文本、边缘、风格、主题等），从而可以以统一的格式标准化大量的生成意图。然后，我们通过使用两阶段框架微调预先训练的文本到图像的扩散模型来构建指导图像。首先，我们使用检索增强训练来调整模型，以增强模型基于外部多模式上下文生成的能力。随后，我们对需要视觉语言理解的各种图像生成任务（例如，主题驱动的生成等）的自适应模型进行微调，每个任务都与封装任务本质的多模式指令配对。在各种图像生成数据集上的人工评估表明，指令图像n在领域中匹配或超过了先前的特定任务模型，并证明了对看不见的和更复杂的任务的良好泛化能力。 et.al.|[2401.01952](http://arxiv.org/abs/2401.01952)|null|
|**2024-01-03**|**Can We Generate Realistic Hands Only Using Convolution?**|近十年来，图像生成模型一直无法重建复杂的几何特征，例如人手和手指上的几何特征。尽管在增加模型大小和多样化训练数据集方面取得了进展，但从去噪扩散模型到生成对抗性网络（GAN），这一问题在所有模型中仍然普遍存在，这表明底层架构存在根本缺陷。在本文中，我们展示了如何通过为卷积层提供包含相对 $n$ 维笛卡尔坐标系的单个输入通道来增强卷积层的几何能力来缓解这个问题。我们表明，这大大提高了由GANs和变分自动编码器（VAE）生成的手和面部图像的质量。 et.al.|[2401.01951](http://arxiv.org/abs/2401.01951)|null|
|**2024-01-03**|**A Vision Check-up for Language Models**|学习对字符串之间的关系建模能教会大型语言模型（LLM）关于视觉世界的什么？我们系统地评估了LLM生成和识别各种日益复杂的视觉概念的能力，然后展示了如何使用文本模型训练初步的视觉表示学习系统。由于语言模型缺乏以像素形式消费或输出视觉信息的能力，我们在研究中使用代码来表示图像。尽管LLM生成的图像看起来不像自然图像，但图像生成的结果以及模型校正这些生成图像的能力表明，字符串的精确建模可以向语言模型传授视觉世界的许多方面。此外，利用文本模型生成的图像进行自监督视觉表示学习的实验，突出了训练视觉模型的潜力，该模型能够仅使用LLM对自然图像进行语义评估。 et.al.|[2401.01862](http://arxiv.org/abs/2401.01862)|null|

<p align=right>(<a href=#updated-on-20240110>back to top</a>)</p>

## avatar

|Publish Date|Title|Abstract|PDF|Code|
|---|---|-----------------------------------|---|---|
|**2024-01-01**|**Graph-Convolutional Autoencoder Ensembles for the Humanities, Illustrated with a Study of the American Slave Trade**|我们介绍了一个图形感知的自动编码器集成框架，以及相关的形式主义和工具，旨在促进人文学科学术的深度学习。通过组成子体系结构以生成同构于人文领域的模型，我们保持了可解释性，同时为每个子体系结构选择提供功能签名，允许传统和计算研究人员在不干扰既定实践的情况下进行合作。我们展示了我们的方法在美国后大西洋奴隶贸易历史研究中的实际应用，并做出了一些具体的技术贡献：一种新的混合图卷积自动编码器机制，用于常见图拓扑的批处理策略，以及用于特定用例的掩蔽技术。越来越多的20多项研究证明了该框架扩大不同领域参与的有效性，这些研究既有与人文主义者的合作，也有机器学习文献中的既定任务，涵盖了各种领域和数据模式。我们对几种不同的体系结构选择进行了性能比较，最后列出了这项研究即将采取的下一步行动。 et.al.|[2401.00824](http://arxiv.org/abs/2401.00824)|null|
|**2023-12-23**|**Human101: Training 100+FPS Human Gaussians in 100s from 1 View**|从单视角视频中重构人体在虚拟现实领域发挥着关键作用。一种流行的应用场景需要快速重建高保真3D数字人，同时确保实时渲染和交互。现有的方法往往难以满足这两个要求。在本文中，我们介绍了Human101，这是一种新颖的框架，通过在100秒内训练3D高斯人并以100+FPS进行渲染，能够从单视图视频中生成高保真动态3D人体重建。我们的方法利用了3D高斯飞溅的优势，它提供了3D人类的明确而有效的表示。与之前基于NeRF的管道不同，Human101巧妙地应用了以人为中心的前向高斯动画方法来变形3D高斯的参数，从而提高了渲染速度（即，以令人印象深刻的60+FPS渲染1024个分辨率的图像，以100+FPS渲染512个分辨率的图片）。实验结果表明，我们的方法大大超过了当前的方法，每秒帧数激增了10倍，并提供了相当或卓越的渲染质量。代码和演示将在上发布https://github.com/longxiang-ai/Human101. et.al.|[2312.15258](http://arxiv.org/abs/2312.15258)|**[link](https://github.com/longxiang-ai/human101)**|
|**2023-12-23**|**Prompt-Propose-Verify: A Reliable Hand-Object-Interaction Data Generation Framework using Foundational Models**|扩散模型以文本提示为条件，生成具有复杂细节的逼真图像。但是，当涉及到手、牙齿等人类特征时，大多数预先训练的模型都无法生成准确的图像。我们假设，可以通过注释良好的高质量数据来克服扩散模型的这种无能。在本文中，我们专门研究了使用扩散模型改进手-物体交互图像的生成。我们收集了一个注释良好的手-物交互合成数据集，该数据集使用Prompt Propose-Verify框架进行策划，并在其上微调稳定的扩散模型。我们根据CLIPScore、ImageReward、Fedility和alignment等定性和定量指标对图像-文本数据集进行评估，并显示出比当前最先进的基准测试更好的性能。 et.al.|[2312.15247](http://arxiv.org/abs/2312.15247)|null|
|**2023-12-23**|**TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**|直接语音到语音翻译通过引入自监督学习中获得的离散单元来实现高质量的结果。这种方法避免了与模型级联相关的延迟和级联错误。然而，与音频语音相比，有声头翻译，即将视听语音（即有声头视频）从一种语言转换为另一种语言，仍然面临着几个挑战：（1）现有方法总是依赖于级联，通过音频和文本进行合成，导致延迟和级联错误。（2） 会说话的头翻译有一套有限的参考框架。如果生成的翻译超过了原始语音的长度，则需要通过重复帧来补充视频序列，从而导致不和谐的视频转换。在这项工作中，我们提出了一个有声头翻译模型\textbf{TransFace}，它可以直接将视听语音翻译成其他语言的视听语音。它由一个语音到单元的翻译模型和一个基于单元的视听语音合成器Unit2Lip组成，前者将音频语音转换为离散单元，后者并行地从离散单元重新合成同步的视听语音。此外，我们还引入了一个有界持续时间预测器，确保等轴测头的平移并防止重复的参考帧。实验表明，我们提出的Unit2Lip模型显著提高了同步性（原始和生成的音频语音在LSE-C上分别为1.601和0.982），并将LRS2上的推理速度提高了4.35倍。此外，TransFace在LRS3-T和100%等时翻译上的Es-En和Fr-En的BLEU得分分别为61.93和47.55。 et.al.|[2312.15197](http://arxiv.org/abs/2312.15197)|null|
|**2023-12-22**|**Deformable 3D Gaussian Splatting for Animatable Human Avatars**|神经辐射场的最新进展使得能够在动态设置中对照片真实感图像进行新颖的视图合成，这可以应用于具有人类动画的场景。然而，通常使用的隐式主干来建立准确的模型，需要许多输入视图和额外的注释，如人体遮罩、UV贴图和深度贴图。在这项工作中，我们提出了ParDy Human（参数化动态人类化身），这是一种完全明确的方法，可以从一个单一的单目序列中构建数字化身。ParDy Human在3D高斯飞溅中引入了参数驱动的动力学，其中通过人体姿势模型使3D高斯变形以使化身动画化。我们的方法由两个部分组成：第一个模块根据SMPL顶点使标准3D高斯变形，第二个模块进一步采用其设计的联合编码并预测每高斯变形，以处理SMPL顶点变形之外的动力学。然后通过光栅化器合成图像。ParDy Human构成了逼真动态人类化身的显式模型，其需要显著更少的训练视图和图像。我们的化身学习不需要额外的注释，如掩码，并且可以在可变背景下进行训练，同时即使在消费硬件上也能高效地推断出全分辨率图像。我们提供的实验证据表明，在ZJU MoCap和THUman4.0数据集上，ParDy-Human在数量和视觉上都优于最先进的方法。 et.al.|[2312.15059](http://arxiv.org/abs/2312.15059)|null|
|**2023-12-22**|**Latents2Semantics: Leveraging the Latent Space of Generative Models for Localized Style Manipulation of Face Images**|随着元宇宙慢慢成为现实，以及数字人类的快速发展，对人脸原则风格编辑管道的需求势必会增加。我们通过引入潜在2语义自动编码器（L2SAE）来满足这一需求，这是一种生成式自动编码器模型，有助于对人脸图像中几个感兴趣区域（ROI）的风格属性进行高度本地化编辑。L2SAE学习编码图像的结构和风格信息的单独潜在表示。因此，允许对所选择的ROI进行保留结构的样式编辑。编码的结构表示是具有减少的空间维度的多通道2D张量，其捕获局部和全局结构属性。样式表示是捕获全局样式属性的1D张量。在我们的框架中，我们对结构表示进行切片，以建立与不同ROI的强且不纠缠的对应关系。因此，所选ROI的样式编辑相当于（a）从切片结构表示生成的ROI掩模和（b）具有全局样式变化的解码图像的简单组合，该全局样式变化是从操纵的（使用高斯噪声）全局样式和不变的结构张量生成的。在没有额外人力监督的情况下进行风格编辑是对SOTA风格编辑管道的重大胜利，因为大多数现有作品都需要额外的人力（监督）后期培训，才能将语义归因于风格编辑。我们还取消了基于迭代优化的反演或在训练后确定可控的潜在方向，这需要额外的计算成本高昂的操作。我们在多个应用程序中为相同的应用程序提供定性和定量结果，例如使用从多个数据集采样的测试图像进行选择性风格编辑和交换。 et.al.|[2312.15037](http://arxiv.org/abs/2312.15037)|null|
|**2023-12-21**|**From Past to Future: Digital Methods Towards Artefact Analysis**|在过去的二十年里，数字人文改变了人文和社会科学的格局，实现了对广泛数据集的高级计算分析和解释。值得注意的是，东南亚，特别是新加坡最近的举措侧重于对历史数据进行分类和归档，如艺术品、文学作品，尤其是考古文物。这项研究通过在两个不同的人工制品数据集上应用统计方法，展示了数字人文的巨大潜力。具体而言，我们展示了对公元1千年中期东南亚大陆“旭日”铸币的自动模具研究结果，随后对从新加坡中部殖民前圣安德鲁大教堂遗址挖掘的13-14世纪陶器的2D图像使用了无监督统计方法。这项研究提供了一个比较评估，展示了基于统计的方法对不同考古材料的解释和分析以及数字人文整体的变革影响。 et.al.|[2312.13790](http://arxiv.org/abs/2312.13790)|null|
|**2023-12-18**|**Relightable Neural Actor with Intrinsic Decomposition and Pose Control**|在视觉和图形学中，创建一个可欣赏、可驾驶和逼真的数字人类化身是一个具有挑战性的重要问题。人类是高度立体化的，会产生依赖姿势的外观效果，如自我阴影和皱纹，皮肤和衣服需要复杂且空间变化的BRDF模型。虽然最近的人类重新照明方法可以从多视图视频中恢复看似合理的材料光分解，但它们不能推广到新颖的姿势，并且仍然存在视觉伪影。为了解决这一问题，我们提出了Relightable Neural Actor，这是第一种基于视频的方法，用于学习照片真实感的神经人体模型，该模型可以重新照明，允许外观编辑，并可以由任意骨骼姿势控制。重要的是，为了学习我们的人类化身，我们只需要在已知但静态的照明条件下对人类进行多视图记录。为了实现这一点，我们用可驱动的密度场来表示演员的几何体，该密度场对姿势相关的服装变形进行建模，并提供3D和UV空间之间的映射，其中对法线、可见性和材质进行编码。为了在现实世界场景中评估我们的方法，我们收集了一个新的数据集，其中包括在室内和室外不同光照条件下记录的四个参与者，为人类重新照明提供了第一个此类基准，并展示了最先进的新人类姿势的重新照明结果。 et.al.|[2312.11587](http://arxiv.org/abs/2312.11587)|null|
|**2023-12-18**|**VectorTalker: SVG Talking Face Generation with Progressive Vectorisation**|高保真度和高效的音频驱动谈话头生成一直是计算机图形学和计算机视觉领域的一个关键研究课题。在这项工作中，我们研究了基于矢量图像的音频驱动的谈话头生成。与现有作品中使用最广泛的光栅图像直接动画相比，矢量图像具有良好的可扩展性，可用于多种应用。基于矢量图像的会说话的头生成面临两个主要挑战：高质量的矢量图像重建（相对于源肖像图像）和生动的动画（相对于音频信号）。为了解决这些问题，我们提出了一种新的可扩展矢量图形重建和动画方法，称为VectorTalker。具体而言，对于高保真度重建，VectorTalker以从粗到细的方式分层重建矢量图像。对于生动的音频驱动的面部动画，我们建议使用面部标志作为中间运动表示，并提出一个有效的标志驱动的矢量图像变形模块。我们的方法可以在一个统一的框架内处理各种风格的肖像图像，包括日本漫画、卡通和照片真实感图像。我们进行了广泛的定量和定性评估，实验结果证明了VectorTalker在矢量图形重建和音频驱动动画方面的优势。 et.al.|[2312.11568](http://arxiv.org/abs/2312.11568)|null|
|**2023-12-18**|**AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis**|音频驱动的会说话的头部合成是一个很有前途的课题，在数字人、电影制作和虚拟现实等领域有着广泛的应用。与之前的研究相比，最近基于NeRF的方法在质量和保真度方面显示出优势。然而，当涉及到少镜头会说话的头部生成时，在一个身份只有几秒钟会说话的视频的实际场景中，出现了两个限制：1）它们要么没有基本模型，作为快速收敛的面部先验，要么在构建先验时忽略了音频的重要性；2） 它们大多忽略了不同面部区域与音频之间的相关性，例如，嘴与音频相关，而耳朵与音频无关。在本文中，我们提出了音频增强神经辐射场（AE NeRF）来解决上述问题，它可以用最少的镜头数据集生成新扬声器的逼真肖像。具体来说，我们在参考方案的特征融合阶段引入了音频感知聚合模块，其中权重由参考图像和目标图像之间音频的相似性决定。然后，提出了一种基于双NeRF框架的音频对齐人脸生成策略，分别对音频相关区域和音频无关区域进行建模。大量实验表明，即使在有限的训练集或训练迭代中，AE NeRF在图像保真度、音频嘴唇同步和泛化能力方面也超过了最先进的技术。 et.al.|[2312.10921](http://arxiv.org/abs/2312.10921)|null|

<p align=right>(<a href=#updated-on-20240110>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues

